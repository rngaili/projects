{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Coursework 1\n",
    "## Task 1: Regression Task\n",
    "Your first task deals with a modified dataset that we have prepared based on a collection of household\n",
    "information over various locations across Boston in the US. Each sample in the dataset corresponds to a household\n",
    "characterised by 18 features, from per capita crime rate to proportion of non-retail business acres in the town of the\n",
    "household. We will take one of these features (namely, the median value of owner-occupied homes in US$ 1000s)\n",
    "as the target variable, and we use the other 17 features as predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Linear Regression\n",
    "#### 1.1.1\n",
    "For the modified Boston housing data set, we first obtain a linear regression model to predict the median value of owner-occupied homes in USD 1000's as our target variable using all the other features as predictors. The parameters of this model are obtained by solving the least-squares optimisation problem as we have seen in lectures. We use the regression_train data to train the linear regression model. We also create a function that calculates the mean squared error (MSE) given by $MSE=\\frac{1}{N}(y-\\hat{y})^T(y-\\hat{y})$ where $\\hat{y} = X\\beta$ and $N$ is the number of input samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages that we will need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.413447</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>0.115738</td>\n",
       "      <td>0.115735</td>\n",
       "      <td>0.158124</td>\n",
       "      <td>0.984960</td>\n",
       "      <td>0.797449</td>\n",
       "      <td>-0.773684</td>\n",
       "      <td>0.985161</td>\n",
       "      <td>-0.803212</td>\n",
       "      <td>1.176466</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.983048</td>\n",
       "      <td>0.158124</td>\n",
       "      <td>1.176469</td>\n",
       "      <td>-0.487723</td>\n",
       "      <td>-0.773598</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.412788</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.034027</td>\n",
       "      <td>-1.034035</td>\n",
       "      <td>-0.386091</td>\n",
       "      <td>0.819700</td>\n",
       "      <td>0.207144</td>\n",
       "      <td>-0.418203</td>\n",
       "      <td>0.819617</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-0.857929</td>\n",
       "      <td>0.379323</td>\n",
       "      <td>-0.803625</td>\n",
       "      <td>-0.386091</td>\n",
       "      <td>-0.857939</td>\n",
       "      <td>-0.487723</td>\n",
       "      <td>-0.418305</td>\n",
       "      <td>29.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.387983</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.211099</td>\n",
       "      <td>-0.211084</td>\n",
       "      <td>0.261784</td>\n",
       "      <td>-0.510932</td>\n",
       "      <td>-0.923682</td>\n",
       "      <td>-0.671859</td>\n",
       "      <td>-0.511320</td>\n",
       "      <td>-0.102376</td>\n",
       "      <td>0.344213</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>0.131334</td>\n",
       "      <td>0.261784</td>\n",
       "      <td>0.344218</td>\n",
       "      <td>-0.487727</td>\n",
       "      <td>-0.671863</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.347952</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.720322</td>\n",
       "      <td>-0.720323</td>\n",
       "      <td>-0.412006</td>\n",
       "      <td>0.846768</td>\n",
       "      <td>0.324494</td>\n",
       "      <td>-0.248591</td>\n",
       "      <td>0.846699</td>\n",
       "      <td>-0.601276</td>\n",
       "      <td>-0.488039</td>\n",
       "      <td>0.369674</td>\n",
       "      <td>-0.381702</td>\n",
       "      <td>-0.412006</td>\n",
       "      <td>-0.488023</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.248524</td>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.330562</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.437258</td>\n",
       "      <td>-0.437249</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>0.299699</td>\n",
       "      <td>0.918355</td>\n",
       "      <td>0.313581</td>\n",
       "      <td>0.299802</td>\n",
       "      <td>-0.601276</td>\n",
       "      <td>1.176466</td>\n",
       "      <td>0.342811</td>\n",
       "      <td>0.020597</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>1.176460</td>\n",
       "      <td>-0.487724</td>\n",
       "      <td>0.313542</td>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5         6         7   \\\n",
       "0  1.0 -0.413447 -0.487722  0.115738  0.115735  0.158124  0.984960  0.797449   \n",
       "1  1.0 -0.412788 -0.487722 -1.034027 -1.034035 -0.386091  0.819700  0.207144   \n",
       "2  1.0 -0.387983 -0.487722 -0.211099 -0.211084  0.261784 -0.510932 -0.923682   \n",
       "3  1.0 -0.347952 -0.487722 -0.720322 -0.720323 -0.412006  0.846768  0.324494   \n",
       "4  1.0 -0.330562 -0.487722 -0.437258 -0.437249 -0.144217  0.299699  0.918355   \n",
       "\n",
       "         8         9         10        11        12        13        14  \\\n",
       "0 -0.773684  0.985161 -0.803212  1.176466  0.441052 -0.983048  0.158124   \n",
       "1 -0.418203  0.819617 -0.666608 -0.857929  0.379323 -0.803625 -0.386091   \n",
       "2 -0.671859 -0.511320 -0.102376  0.344213  0.441052  0.131334  0.261784   \n",
       "3 -0.248591  0.846699 -0.601276 -0.488039  0.369674 -0.381702 -0.412006   \n",
       "4  0.313581  0.299802 -0.601276  1.176466  0.342811  0.020597 -0.144217   \n",
       "\n",
       "         15        16        17    18  \n",
       "0  1.176469 -0.487723 -0.773598  23.9  \n",
       "1 -0.857939 -0.487723 -0.418305  29.9  \n",
       "2  0.344218 -0.487727 -0.671863  24.5  \n",
       "3 -0.488023 -0.487722 -0.248524  27.5  \n",
       "4  1.176460 -0.487724  0.313542  18.4  "
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data as pandas data frames\n",
    "regression_train = pd.read_csv('regression_train.csv', header=None)\n",
    "regression_test = pd.read_csv('regression_test.csv', header=None)\n",
    "regression_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating training data\n",
    "X_train_r = regression_train.iloc[:,:-1]\n",
    "y_train_r = regression_train.iloc[:,-1] # target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE function\n",
    "def mse(y_true,y_pred): \n",
    "    mse = np.square(np.subtract(y_true,y_pred)).mean()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Parameters: [ 2.25195148e+01 -6.18293420e-01 -4.49311989e+04 -4.00378419e+04\n",
      "  4.00376826e+04  2.64882498e+04  9.82671278e+02 -1.58321075e-01\n",
      "  1.37927336e+03 -9.79553269e+02 -5.46855295e-02 -5.67508342e+03\n",
      "  7.41991610e-01 -3.71807187e+00 -2.64900423e+04  5.67334155e+03\n",
      "  4.49323309e+04 -1.38276478e+03]\n",
      "In-Sample MSE:  24.36924679123637\n"
     ]
    }
   ],
   "source": [
    "# obtain the parameters of the model using least squares estimate\n",
    "beta_LS = np.linalg.solve(X_train_r.T @ X_train_r, X_train_r.T @ y_train_r) \n",
    "print(\"Linear Regression Parameters:\", beta_LS)\n",
    "\n",
    "#predicted y using LS estimates\n",
    "y_pred_in_sample = X_train_r @ beta_LS\n",
    "\n",
    "# pbtain in-sample MSE\n",
    "MSE_in_sample_lr = mse(y_train_r,y_pred_in_sample)\n",
    "print(\"In-Sample MSE: \", MSE_in_sample_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2\n",
    "We now use the model on the test data to predict the target variable, and compute the out-of-sample\n",
    "MSE on the test set. We also compare the out-of-sample and the in-sample MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-sample MSE: 19.557331360422744\n"
     ]
    }
   ],
   "source": [
    "# seperating test data\n",
    "X_test_r = regression_test.iloc[:,:-1]\n",
    "y_test_r = regression_test.iloc[:,-1]\n",
    "\n",
    "# obtain the pedicted values\n",
    "y_pred_out_of_sample = X_test_r @ beta_LS\n",
    "\n",
    "# obtain out-of-sample MSE on test data\n",
    "MSE_out_of_sample_lr = mse(y_test_r,y_pred_out_of_sample)\n",
    "print(\"Out-of-sample MSE:\", MSE_out_of_sample_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, I would expect the in-sample MSE to be less than the out-of-sample MSE. This is because, for in-sample MSE, it is \"predicting\" the fit of data when we've already constructed the model on that data. The model is well tuned for the training data and hence does well at predicting using it. On the other hand, the data used to test the model is completely different to the one used to build the model. \n",
    "\n",
    "In this task, I obtain 24.37 for the in-sample MSE and 19.56 for the out-of-sample MSE. It is possible for the in-sample MSE to be larger than the out-of-sample MSE by chance. If anything, this shows that the model generalises well because the smaller MSE, the better. Besides, we know that the least squares estimator is unbiased hence the error here only comes from the variance of the estimator which is smaller in the out-of-sample case.\n",
    "\n",
    "The reason this happens is perhaps because the data is not standardised. Now if we try standardising the data, we get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to standardise\n",
    "def standardise(X):\n",
    "  mu = np.mean(X, 0)\n",
    "  sigma = np.std(X, 0)\n",
    "  X_std = (X - mu) / sigma\n",
    "  return X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# changing from dataframes to arrays\n",
    "X_train_r = np.array(regression_train.iloc[:,:-1]) \n",
    "y_train_r = np.array(regression_train.iloc[:,-1])\n",
    "X_test_r = np.array(regression_test.iloc[:,:-1])\n",
    "y_test_r = np.array(regression_test.iloc[:,-1])\n",
    "\n",
    "# standarding the train set besides the first column (as it is the intercept column)\n",
    "X_train_std = np.delete(standardise(X_train_r), obj=0, axis=1)\n",
    "X_test_std = np.delete(standardise(X_test_r), obj=0, axis=1)\n",
    "\n",
    "# append back the 1s onto the standardised X\n",
    "X_train = np.c_[X_train_r[:,0],X_train_std]\n",
    "X_test = np.c_[X_test_r[:,0],X_test_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Parameters: [ 2.30185644e+01 -6.39777746e-01 -4.79088723e+04 -4.01191585e+04\n",
      "  4.01189993e+04  2.01718375e+05  9.84774878e+02 -1.58802973e-01\n",
      "  1.40337093e+03 -9.81652963e+02 -5.39798122e-02 -5.80542910e+03\n",
      "  7.05181773e-01 -3.69131572e+00 -2.01720144e+05  5.80364570e+03\n",
      "  4.79100761e+04 -1.40692168e+03]\n",
      "In-Sample MSE:  24.36924678783743\n",
      "Out-of-sample MSE: 26.62483454590197\n"
     ]
    }
   ],
   "source": [
    "# obtain the parameters of the model using least squares estimate\n",
    "beta_LS = np.linalg.solve(X_train.T @ X_train, X_train.T @ y_train_r) \n",
    "print(\"Linear Regression Parameters:\", beta_LS)\n",
    "\n",
    "# predicted y using LS estimates\n",
    "y_pred_in_sample = X_train @ beta_LS\n",
    "\n",
    "# obtain in-sample MSE\n",
    "MSE_in_sample_lr = mse(y_train_r,y_pred_in_sample)\n",
    "print(\"In-Sample MSE: \", MSE_in_sample_lr)\n",
    "\n",
    "# obtain the pedicted values using LS estimates\n",
    "y_pred_out_of_sample = X_test @ beta_LS\n",
    "\n",
    "# obtain out-of-sample MSE on test data\n",
    "MSE_out_of_sample_lr = mse(y_test_r,y_pred_out_of_sample)\n",
    "print(\"Out-of-sample MSE:\", MSE_out_of_sample_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With standardised data, it appears more normal in that the in-sample MSE is smaller than the out-of sample MSE. I will therefore compare the standardised data results in the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Ridge Regression\n",
    "#### 1.2.1\n",
    "In this task, I repeat task 1.1.1 employing ridge regression using a 5-fold cross validation algorithm to tune\n",
    "the ridge model on the set regression_train.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_r = regression_train.iloc[:,:-1]\n",
    "y_train_r = regression_train.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the beta ridge parameters\n",
    "def ridge_estimate(X, y, penalty):\n",
    "    \n",
    "    # X: N x D matrix of training inputs\n",
    "    # y: N x 1 vector of training targets/observations\n",
    "    # returns: beta ridge parameters (D x 1)\n",
    "    \n",
    "    N, D = X.shape\n",
    "    I = np.identity(D)\n",
    "    beta_ridge = np.linalg.solve(X.T @ X + I*penalty, X.T @ y)\n",
    "    return beta_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into k (num_folds) folds\n",
    "def cross_val_split(data, num_folds):\n",
    "  fold_size = int(len(data) / num_folds)\n",
    "  np.random.seed(14) # set seed so that we get the same folds each time\n",
    "  data_perm = np.random.permutation(data)\n",
    "  folds = []\n",
    "  for k in range(num_folds):\n",
    "    folds.append(data_perm[k*fold_size:(k+1)*fold_size, :])\n",
    "\n",
    "  return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the folds into training and validation data \n",
    "def cross_val_evaluate(data, num_folds):\n",
    "  folds = cross_val_split(data, num_folds) # split the data\n",
    "  \n",
    "  fold_split = [] # [(xtrain, ytrain, xval, yval), (xtrain, ytrain, xval, yval), ...]\n",
    "  for i in range(len(folds)):\n",
    "    # define the training set\n",
    "    train_set = np.delete(np.asarray(folds).reshape(len(folds), folds[0].shape[0], folds[0].shape[1]), i, axis=0)\n",
    "    train_folds = train_set.reshape(len(train_set)*train_set[0].shape[0], train_set[0].shape[1])\n",
    "    X_train_subset = train_folds[:,:-1]\n",
    "    y_train_subset = train_folds[:, -1]\n",
    "    \n",
    "    # define the validation set\n",
    "    val_fold = folds[i]\n",
    "    X_val = val_fold[:,:-1]\n",
    "    y_val = val_fold[:, -1]\n",
    "\n",
    "    fold_split.append((X_train_subset, y_train_subset, X_val, y_val))\n",
    "  return fold_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I use one of the folds to demonstrate with plots how you scan the penalty parameter of the ridge model to find the optimal value of the penalty by examining the MSE on the corresponding validation subset. I do this for the second fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the minimum MSE and corresponding penalty for each fold\n",
    "def min_mse_rr(data, num_folds, fold):\n",
    "    \n",
    "    folds = cross_val_evaluate(data, num_folds)\n",
    "    X_train_subset, y_train_subset, X_val, y_val = folds[fold]\n",
    "    val_scores = [] \n",
    "    penalty = np.linspace(0, 20, 10**3)\n",
    "    \n",
    "    # for each of the folds, scan the hyperparameter lamnda\n",
    "    for i in penalty:\n",
    "        beta_ridge = ridge_estimate(X_train_subset, y_train_subset, i)\n",
    "        y_pred_ridge = X_val @ beta_ridge  \n",
    "        val_score = mse(y_val,y_pred_ridge)\n",
    "        val_scores.append(val_score)\n",
    "      \n",
    "    plt.plot(penalty,val_scores)\n",
    "    plt.xlabel(\"Penalty\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"MSE against penalty term in ridge regression\")\n",
    "        \n",
    "    location_min_mse = np.argmin(val_scores)\n",
    "    print(\"Optimal penalty:\", min(val_scores)), print(\"Optimal penalty:\", penalty[location_min_mse])\n",
    "    return min(val_scores), penalty[location_min_mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal penalty: 48.00568954645486\n",
      "Optimal penalty: 0.02002002002002002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48.00568954645486, 0.02002002002002002)"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XeYFeXZx/HvDUuv0kFYkN5BWRE0Kgo2rCi2xFiiwfJaiLHGrrHFXhIVjV0UG4aoIBh7QaWD9N77AgvLsu1+/5jZeLJuhT3n7O75fa5rr50z9T7T7jPPzDyPuTsiIpK4qsQ7ABERiS8lAhGRBKdEICKS4JQIREQSnBKBiEiCUyIQEUlwSgQJxsySzWynmVWNdyzRYmZuZh3jHUdZKOvtZWaHm9mCIoa/bGZ/LYtlVXTFravKRImgBMxsuZllmlmTfP1nhCedduHn1mb2npltNrPtZjbbzC4Mh7ULx92Z7+/sWH4Xd1/p7nXdPWdf5mNmX5jZJWUVV7Ts64kt3PZDyjKm0iir7RUxv6/dvUtZzKuyS6R1lRTvACqQZcC5wFMAZtYLqJVvnNeAmUBbYA/QC2iRb5yG7p4d3VClPDAzA8zdc+MdC4CZJZWXfa+sYylP361Ccnf9FfMHLAduBX6K6PcwcAvgQLuw306gbyHzaBeOm1TCZV4EzAPSgKXApfmG3wCsA9YCl4Tz7hgOOxGYDuwAVgF3FhYH8AVwD/BtuKyJQJNwWE3gdWALsA34CWgO3AvkABnhd366iO87IoxxHfDniOFVgJuAJeH83wYa5Zv2AmAlsBm4JWLa/sD3YUzrgKeB6hHDHegYLjsLyAzj/DdwPfBevlifAh4v4Du8BuQCu8Ppbwj7DwC+C5c/ExgUMc0X4fr5NpyuY9jvr+E0eXE0Bt4It9FPeftQcftNUdurgGkHAauBG4H14fcZBKyOGOdAYFo4rzHAW8BfS7if1SA4DlYCG4BngVqFxHJhGPNjwNa8ZQB/INjPU4FPgLYR0xwLLAC2A/8AvgQu2Zv5ARaOuzGc3yygZzhsKDA3XAdrgOsi119EPN3C9b8N+Bk4JWLYy8DfgY/C+fwAdIj3uavE57h4B1AR/ggSwZBwp+wGVCU4wbblfxPBp+HOeQ6QnG8e/3NAl2CZJwIdwh34SCAdOCgcdnx4YPcAaocHeOQBOojgaqQK0Ds8SE8rKI5wx14CdCa4wvkCeCAcdinBSat2+J37AfUjprukiPjzlvMmUCeMZxMwJBw+EpgMtCY4oTwHvJlv2ufDmPoQXGF1C4f3IzgZJ4XjzgNGRiw7cl28zP+e2FoCuwiuzAjnsRHoV9S2j/i8P0HiGhqu32PCz00j1svKcNskAdXCfovD7dmA4KSzkGCfSgJeBV4qZj0Wu70KmHYQkA08GK7jWkSc3IDqwArgT2GcwwkSZ95Jtbj97HFgHNAIqEewr9xfSCwXhrFcFX7nWsBp4XrpFva7FfguHL8JQZI8PRx2TRjbJXs5v+OAqUBDgmOqG9AyHLYOODzs3o9fjrPIdVUtnPdfwvV2NMEJv0vEfraV4EdKEkGSfyve564Sn+PiHUBF+OOXRHArcH94gEwKN3hkItgPeIDg10IOMAM4OBzWLhx3W76/biWM4QPgmrD7xcgDjuBX538P0AKmfRx4LF8ckSeWWyPGvQKYEHb/geBXbO8C5vkFJUsEXSP6/Q34Z9g9DxgcMaxleKAnRUzbOmL4j8A5hSxrJDA24nOhiSDsNx74Y9h9EjC3uG0f8flG4LV843wCXBCxXu4uYF1FXtE8AoyP+HwyMKOY9Vjs9ipg2kEEV0M18/XLO7kdQfBL3yKGf8cviaDQ/YzgZLqLiF+9wEBgWSGxXAisLGA7XBzxuQrBD562wPnA9xHDjODH1yV7Ob+jCZLvAKBKvulWEvzoqV/A+stbV4cTJMUqEcPfJLzaDvezFyKGDQXml+TYLg9/ullcOq8BvyXYCV/NP9DdU939JnfvQVCEMgP4ICwrztPE3RtG/M0raEFmdoKZTTazrWa2jWDHyrtZ3YrgoMizKt+0h5jZ52a2ycy2A5dFTFuQ9RHd6UDdiO/7CfCWma01s7+ZWbUi5lOQyNhWhLFDcHCONbNt4febR5A8mxcXl5l1NrMPzWy9me0A7qPo75ffK8B5Yfd5BN+zpNoCZ+bFHcb+G4JElmdVAdNtiOjeXcDnupRcYdurIJvcPaOQYa2ANR6euUIr8g0vbD9rSnCVMDViPUwI+xcm/3ppCzwRMf1WghP+/vmXHca4em/n5+6fERQh/h3YYGajzKx+ON0ZBMfXCjP70swGFhB7K2CV/+/9nhVhrHlKs13KFSWCUnD3FQQ3jYcC7xcz7maC8tNWBJfOJWZmNYD3wumbu3tD4GOCnRqCS9nWEZO0yTeL0QSX7G3cvQFB2a1RSu6e5e53uXt34FCCX8/n5w0u4WwiY0sm+AUKwUF8Qr6kWNPd15Rgns8A84FO7l6f4HK9sO9XUJwfAL3NrCfBd3qjiGXln34VwRVBZNx13P2BYpYZL0XFsg7YP98PleR8wwvbzzYTJLAeEeuhgbsXdfIraF1emm9d1nL37/IvO4yxdb7pSzM/3P1Jd+9HUNTVmeB+Ee7+k7ufCjQj2DfeLiD2tUAbM4s8ZyYT3FOo8JQISu9i4Gh335V/gJk9aGY9zSzJzOoBlwOL3X1LKZdRnaBMdxOQbWYnENw4y/M2cJGZdTOz2sDt+aavB2x19wwz609wFVNqZnaUmfUKn2HfQVB0k/cY4wagfQlmc5uZ1TazHgQ3wMeE/Z8F7jWztuGymprZqSUMrV4Yz04z60qwngvzqzjDX8jvEiTMH919ZSmmfx042cyOM7OqZlbTzAaZWf6TVEXwPUE5+9XhPns6QRl3nkL3s/CX8fPAY2bWDMDM9jez40qx/GeBm8N9AzNrYGZnhsM+AnqZ2WlmlgT8H79+Aq/E8zOzg8Mr5WoERVoZQI6ZVTez35lZA3fPItivCnpU94dwuhvMrJqZDSIo0nurFN+33FIiKCV3X+LuUwoZXBsYS1D2v5TgUvWUfONsy/cewbUFLCMNuJrgQEwlOJGPixg+HngS+JzgBtb34aA94f8rgLvNLI3g4C3oF05JtCA4Ye4gKLr5kuBECPAEMNzMUs3sySLm8WUY43+Ah919YsT044CJYZyTgUNKGNd1BOskjeBkNKaIcf8JdA+LCz6I6P8KwQ3s4oqF7gduDae/zt1XAacSXIVsIvgVej0V8Fhy90yCm7EXEuxnZxNxpVuC/ezGsP/ksIjuU6DEz927+1iCG9lvhdPPAU4Ih20GziS4r7QF6A5MiVh2qeYH1CfYV1IJinS2EFxxA/weWB5Ocxm/FBtGzjuT4Fg+geBq6B/A+e4+v6Tftzyz/y0elIrIzLoR7PQ1vJw8S23BS3bLgGrlJaZIZpZMULzUwt13xDueiiCe+1lYJLMa+J27fx7LZSeCCvcrRgJmNiy8rN2P4FfQv8vjCbc8Ck8q1xI83qckUIR47mdh8VvD8J5Z3n2gybFYdqJRIqi4LiUomlhCUKZZVDm5hMysDkFR1zHAHXEOpyKI5342MFzuZoLy+NPcfXcMl58wVDQkIpLgdEUgIpLgKkSlc02aNPF27drFOwwRkQpl6tSpm929qJf8gAqSCNq1a8eUKYU9sSkiIgUxsxXFj6WiIRGRhBfVRGBBox6zLWjAZUq+YddZ0FBLaeqIERGRMhaLoqGjwrcE/8vM2hA8vlfUq/0iIhID8SoaeoygwQs9uyoiEmfRTgROUJfMVDMbAWBmpxBUfTuzqAnNbISZTTGzKZs2bYpymCIiiSvaRUOHufvasHbCSWY2n6B5x2OLmQ53HwWMAkhJSdGVg4hIlET1isDd14b/NxLUynkkcAAw08yWE9QvPs3MiqteVkREoiRqicDM6oR18ufV73IsQePvzdy9nbu3I6hN8CB3X1/ErEREEk5aRhZ3jvuZHRlZUV9WNIuGmhM0RZi3nNHuPiGKyxMRqRQWb0xjxGtTWbElncM7NWFwt+bFT7QPopYI3H0p0KeYcdpFa/kiIhXR+NnruO6dmdSqXpU3LjmEAe0bR32ZFaKKCRGRyi47J5eHPlnAc18tpW+bhjxz3kG0bFArJstWIhARibNNaXu46s1pTF66ld8PaMutJ3WjRlLVmC1fiUBEJI6mrUzlitenkZqeySNn9uGMfq1jHoMSgYhIHLg7r09ewd0fzqVlg1q8f8Wh9GjVIC6xKBGIiMTY7swcbhk7m/enr+Hors147Ky+NKhdLW7xKBGIiMTQii27uPS1qSzYkMafhnTmqqM7UqWKxTUmJQIRkRj5z7wNjBwzgypmvHjhwRzVpVm8QwKUCEREoi4n13ni04U8+dlierSqz7Pn9aNNo9rxDuu/lAhERKIodVcm14yZwVcLNzG8X2v+elpPalaL3aOhJaFEICISJXPWbOey16eyccce7hvWi3P7tyGsdqdcUSIQEYmCt6es4tYP5tCkTnXevmwgfds0jHdIhVIiEBEpQ3uyc7hz3Fze/HElh3VszJPnHEjjujXiHVaRlAhERMrImm27ueL1qcxcvZ3LB3Xgz8d0JqlqvFoELjklAhGRMvDNos1c9eY0snKcZ8/rx/E9K057W0oEIiL7IDfXeeqzxTz+n4V0alaXZ8/rR/umdeMdVqkoEYiI7KWtuzIZGT4aelrfVtx3ei9qV694p9WKF7GISDkwfWUq//fGNDbvzOTeYT35bf/kcvloaEkoEYiIlIK788p3y7n343k0r1+T9y4/lF6t41NraFlRIhARKaGde7K56b1ZfDhrHYO7NuORs/rQsHb1eIe1z5QIRERKYOGGNC57fSrLN+/ihuO7cNkRHeJea2hZUSIQESnG2Omr+cv7c6hTI4k3LhnAwA7Rb1A+lpQIREQKkZGVw90fzmX0Dyvpf0Ajnj73QJrVrxnvsMqcEoGISAFWbU3nijemMXvNdi47sgPXHVsx3hLeG0oEIiL5fDp3A9e+PQMHnj8/hWO6N493SFGlRCAiEsrOyeWRSQt55osl9Ny/Pv/4bT+SG5efBmSiJaqJwMyWA2lADpDt7ilmdg9wKpALbAQudPe10YxDRKQ4G9MyuPrN6UxeupVz+ydzx8ndy10DMtESiyuCo9x9c8Tnh9z9NgAzuxq4HbgsBnGIiBToh6VbuPLN6aRlZPHImX04o1/reIcUUzEvGnL3HREf6wAe6xhERCCoMG7U10t56JMFtG1Um9cu7k/XFvXjHVbMRTsRODDRzBx4zt1HAZjZvcD5wHbgqIImNLMRwAiA5OTkKIcpIokmdVcmf35nJp/N38iJvVrywBm9qFezWrzDigtzj94PcjNr5e5rzawZMAm4yt2/ihh+M1DT3e8oaj4pKSk+ZcqUqMUpIoll6oqtXDl6Olt2ZnLrSd34/YC2FbbCuKKY2VR3TyluvKg+FJt3E9jdNwJjgf75RhkNnBHNGERE8uTmOs99uYSznptMtapVeO/yQzl/YLtKmQRKI2pFQ2ZWB6ji7mlh97HA3WbWyd0XhaOdAsyPVgwiInkii4JO6NmCB4f3pn6CFgXlF817BM2BsWGmTQJGu/sEM3vPzLoQPD66Aj0xJCJRNnVFKleNDtoOuOuUHpw/sHIWBe2tqCUCd18K9Cmgv4qCRCQm3J0Xvl7GgxPm07JhTd69fCC9WzeMd1jljt4sFpFKaVt6Jte9M5NP5wVFQQ+c0ZsGtVQUVBAlAhGpdKatTOWq0dPZmJbBnSd354JDdUO4KEoEIlJp/Koo6LJD6dNGRUHFUSIQkUohsijouB7N+dvwPioKKiElAhGp8KavTOXKsCjojpO7c6GKgkpFiUBEKix355/fLOOB8fNp0UBFQXtLiUBEKqTt6Vn8+Z2ZfDpvg4qC9pESgYhUOFNXpHL1m0FR0O0ndeeiw1QUtC+UCESkwsjNdZ79agmPTFxIq4Y1eeeyQ+mroqB9pkQgIhXCxrQM/vz2TL5etJkTe7fk/tN7qa6gMqJEICLl3lcLN3Ht2zPYuSebB07vxdkHt1FRUBlSIhCRcisrJ5dHw8bkOzevy+g/DqBz83rxDqvSUSIQkXJp1dZ0rn5rOtNXbuPc/sncflJ3alVPjMbkY02JQETKnfGz13HDe7PA4enfHshJvVvFO6RKTYlARMqNjKwc7vlwLm/8sJI+bRry9LkH0qZR7XiHVekpEYhIubB4YxpXjp7O/PVpXHpEe/58bBeqJ0W1NV0JKRGISFy5O+9MWc0d436mdvWqvHzRwQzq0izeYSUUJQIRiZu0jCxuGTuHcTPXcmiHxjx+dl+a1a8Z77ASjhKBiMTFrNXbuOrN6axO3c11x3bm8kEdqVpF7wbEgxKBiMRUXo2hD06YT9O6NXhrxAAObtco3mElNCUCEYmZzTv3cP07M/l8wSaO7d6cvw3vTcPa1eMdVsJTIhCRmPhy4Sb+/PZMdmRkcfepPfj9gLaqJqKcUCIQkajak53DQxMW8MI3y+jcvC5vXHIIXVqomojyRIlARKJm8cadXP3mdOau28H5A9vyl6HdqFlN1USUN1FNBGa2HEgDcoBsd08xs4eAk4FMYAlwkbtvi2YcIhJb7s6Yn1Zx17/nUrNaFV44P4Uh3ZvHOywpRCyuCI5y980RnycBN7t7tpk9CNwM3BiDOEQkBralZ3Lz+7MZP2c9v+nYhEfO6kNzvRtQrsW8aMjdJ0Z8nAwMj3UMIhIdk5du4U9jZrApbQ83n9CVPx7enip6N6Dci3YicGCimTnwnLuPyjf8D8CYgiY0sxHACIDk5OSoBiki+yYrJ5cn/7OIpz9fTLvGdXj/ikPp3VpNSFYU0U4Eh7n7WjNrBkwys/nu/hWAmd0CZANvFDRhmDRGAaSkpHiU4xSRvbRySzrXjAnaDTizX2vuPKUHdWroOZSKJKpby93Xhv83mtlYoD/wlZldAJwEDHZ3neRFKqgPpq/h1g/mYAZPnXsgJ/dRuwEVUdQSgZnVAaq4e1rYfSxwt5kdT3Bz+Eh3T4/W8kUketIysrj9Xz8zdvoaUtrux+Pn9KX1fmo3oKKK5hVBc2Bs+OZgEjDa3SeY2WKgBkFREcBkd78sinGISBmavjKVa96awerUdEYO6cSVR3UkqaraDajIopYI3H0p0KeA/h2jtUwRiZ6cXOfZL5fw6KSFtKhfk7cvHUiKKourFHRHR0SKtWprOte+PYOflqdyUu+W3DusFw1qVYt3WFJGlAhEpFDuztjpa7j9Xz9jwGNn9+G0vvursrhKRolARAq0LT2TWz6Yw0ez1nFwu/149Ky+aki+klIiEJFf+W7xZq59e2bQfsBxXbjsyA5qPawSUyIQkf/ak53Dw58s4Pmvl9G+id4QThRKBCICwIL1aVzz1nTmr0/jvAHJ/GVoN2pX1ykiEWgriyS43Fzn5e+W88CE+dSvmcSLF6ZwdFdVGZ1IlAhEEtiGHRlc985Mvl60mcFdm/Hg8N40qVsj3mFJjCkRiCSo8bPXcfPY2WRk5XDvsJ78tn+yHgtNUEoEIglm555s7hr3M+9MXU3v1g147Oy+dGhaN95hSRwpEYgkkKkrtvKnMTNZnZrOlUd15JohnaimeoISnhKBSALIysnlqbDhmFYNazHm0oEcrHqCJKREIFLJLd6Yxp/GzGT2mu2cftD+3HVKD+rVVD1B8gslApFKKjfXeem75Tw4YT51ayTx7HkHcXzPlvEOS8ohJQKRSmh1ajrXvTOTyUu3MqRbM+47vRfN6tWMd1hSTikRiFQi7s67U1dz17/n4u787YzenJnSWo+FSpGUCEQqic079/CX92czce4G+h/QiEfO7KPaQqVElAhEKoGJP6/n5vdnk5aRzS1Du3Hxbw6gimoLlRJSIhCpwHZkZHH3v+fy7tTV9GhVn9F/7EuXFvXiHZZUMEoEIhXUd0s2c/07s1i3fTdXHd2Rq47uRPUkvRwmpadEIFLBZGTl8NAnC/jnN8s4oEkd3r38UA5K3i/eYUkFpkQgUoHMXr2dP709g8Ubd3L+wLbcdEJXtRkg+6zI60gzOy+i+7B8w66MVlAi8r+ycnJ54tNFDPvHt+zMyObVP/Tn7lN7KglImSiuQPHaiO6n8g37QxnHIiIFWLQhjeHPfMdjny7kpN4t+WTkERzRuWm8w5JKpLifE1ZId0GfRaQM5eQ6z3+9lEcnLqRuzST+8buDGNpLVURI2SsuEXgh3QV9/hUzWw6kATlAtrunmNmZwJ1AN6C/u08pcbQiCWLJpp1c985Mpq/cxvE9WvDXYT3VcphETXGJoKuZzSL49d8h7Cb83L6EyzjK3TdHfJ4DnA48V6pIRRJATq7z0rfLeOiTBdSqXpUnzz2Qk3u3VBURElXFJYJuZb1Ad58HaMcWyWfZ5l1c/85MpqxIZUi35tx3ek9VFCcxUWQicPcVkZ/NrDFwBLDS3aeWYP4OTDQzB55z91ElDczMRgAjAJKTk0s6mUiFk5vrvPr9ch6YMJ/qVavw6Fl9GHbg/vqxJDFTZCIwsw+Bm9x9jpm1BKYBUwiKiUa5++PFzP8wd19rZs2ASWY2392/KklgYdIYBZCSklLs/QiRimjllnSuf3cmPyzbylFdmnL/6b1p0UBXARJbxRUNHeDuc8Lui4BJ7n6+mdUDvgWKTATuvjb8v9HMxgL9gRIlApHKLDfXeePHldz/8TyqmvG34b05s5+qi5b4KC4RZEV0DwaeB3D3NDPLLWpCM6sDVAnHrQMcC9y9L8GKVAarU9O58b1ZfLt4C4d3asKDZ/SmVcNa8Q5LElhxiWCVmV0FrAYOAiYAmFktoLhGT5sDY8NfOEnAaHefYGbDCF5Oawp8ZGYz3P24ffgOIhWCu/PWT6v464dzAbhvWC/O7d9GVwESd8UlgosJfsUPAc52921h/wHAS0VN6O5LgT4F9B8LjC19qCIV19ptu7nxvVl8vWgzh3ZozINn9FajMVJuFPfU0EbgsgL6fw58Hq2gRCoLd+edKau558O5ZOc695zag98d0laNxki5UtxTQ+OKGu7up5RtOCKVx+rUdG5+fzZfL9pM/wMa8fDwPiQ31lWAlD/FFQ0NBFYBbwI/oPqFRIqVm+u8/sMKHhw/H4B7TuvJ7/on6ypAyq3iEkEL4BjgXOC3wEfAm+7+c7QDE6mIlm3exY3vzuLH5Vs5vFMT7j+9F63301WAlG/F3SPIIXhSaIKZ1SBICF+Y2d3unr9aapGElZPrvPjNMh6euIDqSVX0XoBUKMW2ahEmgBMJkkA74Eng/eiGJVJxLNqQxvXvzmLGqm0M6daMe4f1onl9vR0sFUdxN4tfAXoC44G7It4yFkl4WTm5PPflEp78z2Lq1KjKE+f05ZQ+rXQVIBVOcVcEvwd2AZ2BqyN2cAPc3etHMTaRcuvntdu5/p1ZzF23gxN7t+SuU3qovQCpsIq7R1BcU5YiCWVPdg5Pf7aYZ75YQsPa1Xn2vH4c37NFvMMS2Sdq+VqkhKavTOWGd2exaONOzjioNbed1I2GtavHOyyRfaZEIFKMjKwcHp20kBe+Xkrz+jV56aKDOapLs3iHJVJmlAhEivDdks385f3ZLN+Szm8PSebmE7pSr2Zx9S2KVCxKBCIF2J6exf3j5/HWT6tIblSb0ZccwqEdm8Q7LJGoUCIQieDujJ+zntv/9TOp6ZlcemR7Rg7uTK3qVeMdmkjUKBGIhNZvz+C2f81h0twN9GhVn5cvOpie+zeId1giUadEIAkvN9cZ/eNKHhw/n8ycXG4+oSsX/+YAkqrq6WlJDEoEktAWb9zJze/P4qflqRzWsTH3DetF28Z14h2WSEwpEUhCyszO5dkvl/D0Z4upVb0qDw3vzXBVEicJSolAEs60lanc9N4sFm7YyUm9W3LHyT1oWk/VQ0jiUiKQhLFzTzYPf7KAV75fTov6NXnh/BSGdG8e77BE4k6JQBLC5/M3cusHc1i7fTfnD2jLdcd10YthIiElAqnUNqXt4Z4P5zJu5lo6NqvLu5cNpF/bRvEOS6RcUSKQSik31xkzZRX3fzyP3Vk5XDO4E1cc1YEaSXoxTCQ/JQKpdBasT+OWsbOZsiKV/gc04r5hvejYrG68wxIpt5QIpNLYnZnDU58tYtRXS6lXM0mPhIqUUFQTgZktB9KAHCDb3VPMrBEwhqD94+XAWe6eGs04pPL7cuEmbvtgDiu3pjO8X2v+MrQbjeqorQCRkojFFcFR7r454vNNwH/c/QEzuyn8fGMM4pBKaGNaBvd8OI9/z1xL+yZ1GP3HQzi0g2oJFSmNeBQNnQoMCrtfAb5AiUBK6b/1A02Yz56sXEYO6cTlg3QzWGRvRDsRODDRzBx4zt1HAc3dfR2Au68zswKbejKzEcAIgOTk5CiHKRXJ/PU7+Mv7s5m2chsD2zfmr8N60qGpbgaL7K1oJ4LD3H1teLKfZGbzSzphmDRGAaSkpHi0ApSKY3dmDk/8ZxEvfL2U+rWq8ciZfTj9oP11M1hkH0U1Ebj72vD/RjMbC/QHNphZy/BqoCWwMZoxSOXw+YKN3PbBHFan7uaslNbcfEI39tPNYJEyEbVEYGZ1gCrunhZ2HwvcDYwDLgAeCP//K1oxSMW3fnsG93w0l49mraND0zqMGTGAQ9o3jndYIpVKNK8ImgNjw8v2JGC0u08ws5+At83sYmAlcGYUY5AKKisnl1e+W85jkxaSnetce0xnLj2yvW4Gi0RB1BKBuy8F+hTQfwswOFrLlYrvp+Vbue2DOcxfn8ZRXZpy1yk9SW5cO95hiVRaerNYyo3NO/fwwPj5vDt1Na0a1OS53/fj2O7NdTNYJMqUCCTucnKdN39cyd8mzGd3Vg6XD+rAVUd3pHZ17Z4isaAjTeJq9urt3PrBbGau3s7A9o2557QedGxWL95hiSQUJQKJi+3pWTw8cQGv/7CCJnVr8MQ5fTmlTysVA4nEgRKBxJS78/60Ndz38TxS0zO5YGA7rj22M/XVWphI3CgRSMwsWJ/GbR/M4cflWzkwuSGv/KE/PfdvEO+wRBKeEoFE3c492Tzx6UJe/HY59Wom8eAZvTizXxuqVFExkEh5oEQgUePablEAAAAQOElEQVTujJu5lvs+nseGHXs45+A23HB8V7UTIFLOKBFIVMxbt4M7xv3Mj8u20nP/+vzjd/3o13a/eIclIgVQIpAytT09i0cnLeC1yStoUKsa9w3rxdkHt6GqioFEyi0lAikTObnO21NW8dAnC9iWnsl5A9py7TGdaVhbxUAi5Z0Sgeyz6StTuWPcz8xavZ2D2+3HXaccQvdW9eMdloiUkBKB7LVNaXt4cEJQN1CzenopTKSiUiKQUsurIvqJTxeRkZ3DpUe256qjO1G3hnYnkYpIR66UyneLN3PHuJ9ZtHEnR3Zuyu0nd1d7wSIVnBKBlMiabbu596O5fDx7PW0a1eL581MY0q2ZioFEKgElAinS7swcRn21lGe+XIw7XHtMZ0Yc0Z6a1dRSmEhloUQgBXJ3/j1rHQ98PI+12zMY2qsFfxnajdb7qaUwkcpGiUB+Zdbqbdz977lMWZFK95b1eezsvmowXqQSUyKQ/9qYlsFDExbw7rTVNK5TnQdO78WZKXorWKSyUyIQMrJyePHbZfz9s8Vk5uQy4vD2XHl0R+qpjQCRhKBEkMDcnU9+Xs+9H89j1dbdHNO9ObcM7Ua7JnXiHZqIxJASQYKau3YHd3/4M5OXbqVz87q8fvEh/KZTk3iHJSJxoESQYLbs3MPDExcy5qeVNKhVjXtO7cG5/ZNJqlol3qGJSJxEPRGYWVVgCrDG3U8ys6OBh4HqwFTgYnfPjnYciS4zO5dXvw+qhdidlcMFh7Zj5ODONKit+wAiiS4WVwTXAPOA+mZWBXgFGOzuC83sbuAC4J8xiCMh5d0HeGD8fJZvSWdQl6bcemJ3OjZTtRAiEohqIjCz1sCJwL3AtUBjYI+7LwxHmQTcjBJBVMxctY17P5rHj8u30qlZXV666GCO6tIs3mGJSDkT7SuCx4EbgHrh581ANTNLcfcpwHCgTUETmtkIYARAcnJylMOsXFanpvPQJwv414y1NKlbnXuH9eTslDa6DyAiBYpaIjCzk4CN7j7VzAYBuLub2TnAY2ZWA5gIFHh/wN1HAaMAUlJSPFpxViY7MrL4x+dLePHbZRjwf0d14LIjO+h9ABEpUjSvCA4DTjGzoUBNgnsEr7v7ecDhAGZ2LNA5ijEkhOycXN78cSWPf7qILbsyGXbg/lx/XBdaNawV79BEpAKIWiJw95sJyv8Jrwiuc/fzzKyZu28MrwhuJLh/IHvB3fls/kbu+3geSzbtov8BjXjpxG70bt0w3qGJSAUSj/cIrg+LjaoAz7j7Z3GIocL7ee127v1oHt8t2UL7JnUY9ft+HNO9udoHEJFSi0kicPcvgC/C7uuB62Ox3Mpo/fYMHp64gPemraZhrWrceXJ3fjegLdV0I1hE9pLeLK4g0jKyeP6rpYz6eim5ufDHw9vzf0d1pEEt3QgWkX2jRFDOZWbnMvqHFTz12WK27MrkxN4tuen4rrRppAZiRKRsKBGUU7m5zkez1/HwxAWs2JLOgPaNePGEbvRpoxvBIlK2lAjKoe8Wb+b+8fOZvWY7XVvU46WLDmZQ56a6ESwiUaFEUI7MXbuDByfM58uFm9i/YS0eObMPpx24v1oIE5GoUiIoB1anpvPoxIWMnbGG+jWrccvQbvx+YFtqVqsa79BEJAEoEcRR6q5M/v75Yl79fgVmcOkRHbj8yA6qGlpEYkqJIA4ysnJ46dvl/OOLxezak83wfq0ZOaSzqoQQkbhQIoih7Jxc3pu2mscmLWL9jgwGd23GDcd3pUuLesVPLCISJUoEMZCb63w8Zx2PTlzI0s276NumIU+c05dD2jeOd2giIkoE0eTufLFgEw99soC563bQuXld1QkkIuWOEkGU/LhsKw99Mp+flqeS3Kg2j5/dl5P7tNKjoCJS7igRlLE5a7bz0CcL+HLhJprVq8FfT+vJWSltqJ6kSuFEpHxSIigjizfu5NFJC/h49noa1q7GzSd05fyB7ahVXe8CiEj5pkSwj1anpvPEp4t4b9pqalWrytWDO3HJ4QdQX81DikgFoUSwlzal7eHvny9m9A8rweCiww7gikEdaFy3RrxDExEplUqfCNy9TJ/QSd2VyfNfL+Wlb5eTmZPLmf1ac/XgTnoZTEQqrEqdCG77YA7j56xnyq1D9nle29KDBPDyt8tJz8rhxF4tufaYzrRvWrcMIhURiZ9KnQiSqhp7snP2aR7b07N44ZvgCmDnnmxO7NWSqwd30tvAIlJpVOpEUD2pCnuyc/dq2u27s3jxm2W8+M0y0vZkc0LPFlwzpBNdW9Qv4yhFROKrUieCGklVyczOLdV9gh0ZWbz0zXJe+GYpaRnZHNejOdcM7kz3VkoAIlI5VfJEELzElZmTS42kop/nT8vI4uVvl/P810vZkZHNMd2bM3JIJ3q0ahCLUEVE4iYhEsGe7MITwc492bzyXZAAtqVnMaRbM0YO6UzP/ZUARCQxJEYiyMqFmv87bNeebF75fjnPf7WU1PQsju7ajJFDOtG7tRqHF5HEUskTQXAVEPnkUHpmNq9+v4JRXy1l665MBnVpysghnenbRglARBJT1BOBmVUFpgBr3P0kMxsMPARUAXYCF7r74mgsu0a18B5Bdi7pmdm8PnkFz325lC27Mjmic1NGDunEQcn7RWPRIiIVRiyuCK4B5gF5j908A5zq7vPM7ArgVuDCaCy4etUgEbz83XI+nr2OzTszObxTE0YO6Uy/tkoAIiIQ5URgZq2BE4F7gWvD3s4vSaEBsDZay8+7Inj1+xX8pmMTRg7pREq7RtFanIhIhRTtK4LHgRuAyNdwLwE+NrPdwA5gQEETmtkIYARAcnLyXi28/wGNueiwdpzQsyX9D1ACEBEpSNRaSzGzk4CN7j4136A/AUPdvTXwEvBoQdO7+yh3T3H3lKZNm+5VDHVrJHHHyT2UBEREihDNK4LDgFPMbCjBw5v1zewjoKu7/xCOMwaYEMUYRESkGFG7InD3m929tbu3A84BPgNOBRqYWedwtGMIbiSLiEicxPQ9AnfPNrM/Au+ZWS6QCvwhljGIiMj/ikkicPcvgC/C7rHA2FgsV0REihe1oiEREakYlAhERBKcEoGISIJTIhARSXDm7vGOoVhmtglYsZeTNwE2l2E4ZUVxlY7iKh3FVTrlNS7Yt9jaunuxb+RWiESwL8xsirunxDuO/BRX6Siu0lFcpVNe44LYxKaiIRGRBKdEICKS4BIhEYyKdwCFUFylo7hKR3GVTnmNC2IQW6W/RyAiIkVLhCsCEREpghKBiEiCqzSJwMyON7MFZrbYzG4qYHgNMxsTDv/BzNrFIKY2Zva5mc0zs5/N7JoCxhlkZtvNbEb4d3u04wqXu9zMZofLnFLAcDOzJ8P1NcvMDopBTF0i1sMMM9thZiPzjROT9WVmL5rZRjObE9GvkZlNMrNF4f8CG742swvCcRaZ2QUxiOshM5sfbqexZtawkGmL3OZRiOtOM1sTsa2GFjJtkcduFOIaExHTcjObUci00VxfBZ4b4raPuXuF/wOqAkuA9kB1YCbQPd84VwDPht3nAGNiEFdL4KCwux6wsIC4BgEfxmGdLQeaFDF8KDAeMILmRH+IwzZdT/BCTMzXF3AEcBAwJ6Lf34Cbwu6bgAcLmK4RsDT8v1/YvV+U4zoWSAq7HyworpJs8yjEdSdwXQm2c5HHblnHlW/4I8DtcVhfBZ4b4rWPVZYrgv7AYndf6u6ZwFsEjeBEOhV4Jex+FxhsZhbNoNx9nbtPC7vTCBrh2T+ayyxDpwKvemAy0NDMWsZw+YOBJe6+t2+U7xN3/wrYmq935D70CnBaAZMeB0xy963ungpMAo6PZlzuPtHds8OPk4HWZbW8fYmrhEpy7EYlrvD4Pwt4s6yWV1JFnBviso9VlkSwP7Aq4vNqfn3C/e844UGzHWgck+iAsCjqQOCHAgYPNLOZZjbezHrEKCQHJprZVDMbUcDwkqzTaDqHwg/QeKwvgObuvg6CAxloVsA48V5vfyC4kitIcds8Gq4Mi6xeLKSYI57r63Bgg7svKmR4TNZXvnNDXPaxypIICvpln/+52JKMExVmVhd4Dxjp7jvyDZ5GUPzRB3gK+CAWMQGHuftBwAnA/5nZEfmGx3N9VQdOAd4pYHC81ldJxXO93QJkA28UMkpx27ysPQN0APoC6wiKYfKL2/oCzqXoq4Gor69izg2FTlZAv31aZ5UlEawG2kR8bg2sLWwcM0sCGrB3l7KlYmbVCDb0G+7+fv7h7r7D3XeG3R8D1cysSbTjcve14f+NBC3G9c83SknWabScAExz9w35B8RrfYU25BWPhf83FjBOXNZbeMPwJOB3HhYk51eCbV6m3H2Du+e4ey7wfCHLi9f6SgJOB8YUNk6011ch54a47GOVJRH8BHQyswPCX5PnAOPyjTMOyLu7Phz4rLADpqyEZZD/BOa5+6OFjNMi716FmfUn2CZbohxXHTOrl9dNcLNxTr7RxgHnW2AAsD3vkjUGCv2lFo/1FSFyH7oA+FcB43wCHGtm+4VFIceG/aLGzI4HbgROcff0QsYpyTYv67gi7ykNK2R5JTl2o2EIMN/dVxc0MNrrq4hzQ3z2sWjcEY/HH8FTLgsJnkC4Jex3N8HBAVCToKhhMfAj0D4GMf2G4JJtFjAj/BsKXAZcFo5zJfAzwdMSk4FDYxBX+3B5M8Nl562vyLgM+Hu4PmcDKTHajrUJTuwNIvrFfH0RJKJ1QBbBL7CLCe4p/QdYFP5vFI6bArwQMe0fwv1sMXBRDOJaTFBmnLeP5T0d1wr4uKhtHuW4Xgv3nVkEJ7iW+eMKP//q2I1mXGH/l/P2qYhxY7m+Cjs3xGUfUxUTIiIJrrIUDYmIyF5SIhARSXBKBCIiCU6JQEQkwSkRiIgkOCUCSVhmlhPWLDnHzN4xs9plPP8LzezpsPs0M+telvMXKStKBJLIdrt7X3fvCWQSvK8QLacR1C4pUu4oEYgEvgY6ApjZeWb2Y3i18JyZVQ377zSze8MK7yabWfOw/8kWtHEx3cw+zeufx8wOJag76aFwnh3MbFrE8E5mNjVm31QkHyUCSXhhvTMnALPNrBtwNkGFY32BHOB34ah1gMkeVHj3FfDHsP83wAB3P5CgGuUbIufv7t8RvFl7fXgFsgTYbmZ9w1EuInjTVSQukuIdgEgc1bJfWqf6mqDulxFAP+CnsEqjWvxS8Vcm8GHYPRU4JuxuDYwJ69apDiwrwbJfAC4ys2sJEk9UK4ATKYoSgSSy3eGv/v8KKwN7xd1vLmD8LP+lTpYcfjl+ngIedfdxZjaIoGWu4rwH3AF8Bkx191hVnCfyKyoaEvlf/wGGm1kz+G8bsm2LmaYBsCbsLqz92DSCJgkBcPcMghojnwFe2qeIRfaREoFIBHefC9xK0DLVLIJmAItrovNO4B0z+xrYXMg4bwHXhzeUO4T93iBsBWufAxfZB6p9VCROzOw6guq2b4t3LJLYdI9AJA7MbCxBM45HxzsWEV0RiIgkON0jEBFJcEoEIiIJTolARCTBKRGIiCQ4JQIRkQT3/03UixzVZD06AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 2nd fold\n",
    "min_mse_rr(regression_train, 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now report the values of the penalty parameter obtained for the five folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalty: 10.21102110211021\n",
      "Penalty: 0.002000200020002\n",
      "Penalty: 9.894989498949895\n",
      "Penalty: 0.002000200020002\n",
      "Penalty: 4.934493449344934\n",
      "The optimal penalty is: 1.45014501450145\n"
     ]
    }
   ],
   "source": [
    "folds = cross_val_evaluate(regression_train, 5)\n",
    "penalty = np.linspace(0, 20, 10**4)\n",
    "penalty = penalty[1:]\n",
    "    \n",
    "def min_mse_rr_Fold(data):\n",
    "    \n",
    "    X_train_subset, y_train_subset, X_val, y_val = data \n",
    "    val_scores = [] \n",
    "\n",
    "    for i in penalty: # loop over penalty values\n",
    "        beta_ridge = ridge_estimate(X_train_subset, y_train_subset, i)\n",
    "        y_pred_ridge = X_val @ beta_ridge  \n",
    "        val_score = mse(y_val,y_pred_ridge)\n",
    "        val_scores.append(val_score)\n",
    "    location_min_mse = np.argmin(val_scores)\n",
    "   # print(\"Fold \", folds)\n",
    "    print(\"Penalty:\", penalty[location_min_mse])\n",
    "    return val_scores\n",
    "results = map(min_mse_rr_Fold, folds) # use map to apply min_mse_rr_Fold function to each fold\n",
    "m = np.array(list(results))\n",
    "mse_mean = np.mean(m, axis=0)\n",
    "optimal_penalty_loc = mse_mean.argmin() # find the location of the optimal penalty\n",
    "optimal_penalty = penalty[optimal_penalty_loc] # find the corresponding optimal penalty\n",
    "print(\"The optimal penalty is:\", optimal_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal penalty parameter, I first took the average MSE over the 5 folds, then found the minimum MSE and then took the corresponding penalty parameter. We will need the optimal penalty in the next part of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2\n",
    "In this task, I obtain the in-sample MSE by retraining the model using the optimal penalty parameter on the\n",
    "entire training set (found in the task above), and I also compute its out-of-sample MSE (on the test set regression_test.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample MSE: 24.561211818092605\n",
      "Out-of-sample MSE: 19.361771106788023\n"
     ]
    }
   ],
   "source": [
    "# MSE in-sample\n",
    "X_train_r = regression_train.iloc[:,:-1]\n",
    "y_train_r = regression_train.iloc[:,-1]\n",
    "beta_ridge_opt = ridge_estimate(X_train_r, y_train_r, optimal_penalty)\n",
    "y_pred_train = X_train_r @ beta_ridge_opt\n",
    "MSE_in_sample_rr = mse(y_train_r, y_pred_train)\n",
    "print(\"In-sample MSE:\", MSE_in_sample_rr)\n",
    "\n",
    "# MSE out-of-sample\n",
    "X_test_r = regression_test.iloc[:,:-1]\n",
    "y_test_r = regression_test.iloc[:,-1] \n",
    "y_pred_test = X_test_r @ beta_ridge_opt\n",
    "MSE_out_of_sample_rr = mse(y_test_r, y_pred_test)\n",
    "print(\"Out-of-sample MSE:\", MSE_out_of_sample_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In task 1.1.2 where we used linear regression, I obtain 24.37 for the in-sample MSE and 19.56 for the out-of-sample MSE whereas in this task, using ridge regression, I obtain 24.56 or the in-sample MSE and 19.36. These are extremely similar which is quite surprising for 2 reasons:\n",
    "1. In task 1.2.1, we used cross validation. Cross validation is a method used to deal with overfitting hence we expect to get better results.\n",
    "2. Ridge regression is a technique that is mean to reduce the overall MSE. So althought there is some bias, the overall MSE should decrease.\n",
    "\n",
    "Similarly to above, I stadardise the data to see if I get more \"normal\" results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# changing from dataframes to arrays\n",
    "X_train_r = np.array(regression_train.iloc[:,:-1]) \n",
    "y_train_r = np.array(regression_train.iloc[:,-1])\n",
    "X_test_r = np.array(regression_test.iloc[:,:-1])\n",
    "y_test_r = np.array(regression_test.iloc[:,-1])\n",
    "\n",
    "# standarding the train set besides the first column (as it is the intercept column)\n",
    "X_train_std = np.delete(standardise(X_train_r), obj=0, axis=1)\n",
    "X_test_std = np.delete(standardise(X_test_r), obj=0, axis=1)\n",
    "\n",
    "# append back the 1s onto the standardised X\n",
    "X_train = np.c_[X_train_r[:,0],X_train_std]\n",
    "X_test = np.c_[X_test_r[:,0],X_test_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample MSE: 24.561498465984066\n",
      "Out-of-sample MSE: 25.99491592074952\n"
     ]
    }
   ],
   "source": [
    "# MSE in-sample\n",
    "beta_ridge_opt = ridge_estimate(X_train, y_train_r, optimal_penalty)\n",
    "y_pred_train = X_train @ beta_ridge_opt\n",
    "MSE_in_sample_rr = mse(y_train_r, y_pred_train)\n",
    "print(\"In-sample MSE:\", MSE_in_sample_rr)\n",
    "\n",
    "# MSE out-of-sample\n",
    "y_pred_test = X_test @ beta_ridge_opt\n",
    "MSE_out_of_sample_rr = mse(y_test_r, y_pred_test)\n",
    "print(\"Out-of-sample MSE:\", MSE_out_of_sample_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the out-of-sample MSE is bigger than the in-sample MSE. A difference we can note compared with linear regression is that the difference between the in-sample and out-of-sample MSE for ridge regression (24.56 and 25.99) is slightly smaller then the same difference in linear regression (24.34 and 26.62). This suggests that the ridge model perfoms ever so slightly better which is expected as we have used cross-validation and have added the penalty term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Regression with K nearest neighbours (KNN)\n",
    "#### 1.3.1\n",
    "I now repeat task 1.2.1 employing the kNN algorithm as a regression model. Using one of the folds: I will first demonstrate the process by which you scan over a range of k to find your optimal value within this\n",
    "range; and secondly, examine both the MSE and the distribution of the errors obtained on the corresponding\n",
    "validation subset.\n",
    "\n",
    "The error is defined as $e = y - \\hat{y}$ where $\\hat{y} = X\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Changing from dataframes to arrays\n",
    "X_train_r = np.array(regression_train.iloc[:,:-1]) \n",
    "y_train_r = np.array(regression_train.iloc[:,-1])\n",
    "X_test_r = np.array(regression_test.iloc[:,:-1])\n",
    "y_test_r = np.array(regression_test.iloc[:,-1])\n",
    "\n",
    "# Standarding the train set besides the first column (as it is the intercept column)\n",
    "X_train_std = np.delete(standardise(X_train_r), obj=0, axis=1)\n",
    "X_test_std = np.delete(standardise(X_test_r), obj=0, axis=1)\n",
    "\n",
    "# Append back the 1s onto the standardised X\n",
    "X_train = np.c_[X_train_r[:,0],X_train_std]\n",
    "X_test = np.c_[X_test_r[:,0],X_test_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p, q):\n",
    "    return np.sqrt(np.sum((p-q)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_neighbours(X_test, X_train, k, return_distance=False):\n",
    "  n_neighbours = k\n",
    "  dist = []\n",
    "  neigh_ind = []\n",
    "  \n",
    "  # compute distance from each point x_text in X_test to all points in X_train\n",
    "  point_dist = [euclidian_distance(x_test, X_train) for x_test in X_test]\n",
    "\n",
    "  # determine which k training points are closest to each test point\n",
    "  for row in point_dist:\n",
    "      enum_neigh = enumerate(row)\n",
    "      sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:k]\n",
    "\n",
    "      ind_list = [tup[0] for tup in sorted_neigh]\n",
    "      dist_list = [tup[1] for tup in sorted_neigh]\n",
    "\n",
    "      dist.append(dist_list)\n",
    "      neigh_ind.append(ind_list)\n",
    "  \n",
    "  # return distances together with indices of k nearest neighbouts\n",
    "  if return_distance:\n",
    "      return np.array(dist), np.array(neigh_ind)\n",
    "  \n",
    "  return np.array(neigh_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a predicting function called reg_predict\n",
    "def reg_predict(X_test, X_train, y_train, k):\n",
    "  # each of the k neighbours contributes equally to the classification of any data point in X_test  \n",
    "  neighbours = k_neighbours(X_test, X_train, k=k)\n",
    "  # compute mean over neighbours labels\n",
    "  y_pred_kNN = np.array([np.mean(y_train[neighbour]) for neighbour in neighbours])\n",
    "    \n",
    "  return y_pred_kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the folds into training and validation data \n",
    "def cross_val_evaluate(data, num_folds):\n",
    "  folds = cross_val_split(data, num_folds) # split the data\n",
    "  \n",
    "  fold_split = [] # [(xtrain, ytrain, xval, yval), (xtrain, ytrain, xtest, ytest)]\n",
    "  for i in range(len(folds)):\n",
    "    # define the training set\n",
    "    train_set = np.delete(np.asarray(folds).reshape(len(folds), folds[0].shape[0], folds[0].shape[1]), i, axis=0)\n",
    "    train_folds = train_set.reshape(len(train_set)*train_set[0].shape[0], train_set[0].shape[1])\n",
    "    X_train_subset = train_folds[:,:-1]\n",
    "    y_train_subset = train_folds[:, -1]\n",
    "    \n",
    "    # define the validation set\n",
    "    val_fold = folds[i]\n",
    "    X_val = val_fold[:,:-1]\n",
    "    y_val = val_fold[:, -1]\n",
    "\n",
    "    fold_split.append((X_train_subset, y_train_subset, X_val, y_val))\n",
    "  return fold_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I demonstrate the process by which you scan over a range of k to find the optimal value within this range; and secondly, I examine both the MSE and the distribution of the errors obtained on the corresponding validation subset. I do this for fold 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_error_kNN(data, num_folds):\n",
    "    \n",
    "    X_train_subset, y_train_subset, X_val, y_val = cross_val_evaluate(data, num_folds)[2] # fold 3 \n",
    "    empty_mse = np.zeros(20)\n",
    "    empty_errors = []\n",
    "\n",
    "    for k in range(1,21):\n",
    "            y_pred_kNN = reg_predict(X_val, X_train_subset, y_train_subset, k)\n",
    "            mse_kNN = mse(y_val, y_pred_kNN) # calculating mse\n",
    "            empty_mse[k-1] = mse_kNN\n",
    "            error_kNN = y_val - y_pred_kNN # calculating errors\n",
    "            empty_errors.append(error_kNN)\n",
    "    \n",
    "    plt.figure(figsize = [10, 15])\n",
    "    \n",
    "    plt.subplot(2, 1, 1) \n",
    "    plt.plot(range(1,21), empty_mse)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"MSE against k parameter in kNN for 5 folds\")\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.hist(empty_errors,bins=25)\n",
    "    plt.xlabel(\"Distribution\")\n",
    "    plt.ylabel(\"Size of error\")\n",
    "    plt.title(\"MSE against k parameter in kNN for 5 folds\")\n",
    "    \n",
    "    return empty_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0666858 , 0.04536486, 0.05610901, 0.06338753, 0.06888438,\n",
       "       0.07594045, 0.08368271, 0.08207064, 0.08855804, 0.09303032,\n",
       "       0.10709379, 0.10371979, 0.11333862, 0.11542015, 0.11839117,\n",
       "       0.12303717, 0.1254932 , 0.12988213, 0.13396991, 0.1364631 ])"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAANsCAYAAAAEPu/oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd81dX9x/HXJwlJGCEQkkACYS8ZQSAMFy6coNY6cSsVtbW2+qurra3V1i5nra0bV111IqA4wYksIRA2YYUkkJBFIDvn98e90BgDCZCb703yfj4ePMy93/X53oz79pxzzzHnHCIiIiISnEK8LkBERERE9k9hTURERCSIKayJiIiIBDGFNREREZEgprAmIiIiEsQU1kRERESCmMKaSCthZj3NrNjMQgN8nd5m5swsLJDXaYnM7Ndm9vQhHvucmf2xsWuq55rHmNk6/8/Vj5ry2v7r32Bm2/3X71LPvnPN7Cf72aafWQlqCmsiB2Bmm8ys3Mxiaz2/1P/Hvbf/cQ8ze9PMcs2s0MyWm9lV/m173wiKa/27qCnvxTm3xTnXwTlXdTjnOdCbXmtiZneb2UuNeU7n3H3OuUZ/bf1BzpnZ2BrP9TczV+PxXDMrNbOkGs9NNLNNBzj1PcA//T9X7zRCnXebWUWt35O++9m3DfAgcKr/+jsP9/oiwUphTaR+G4Epex+Y2XCgba19XgS2Ar2ALsAVwPZa+3Tyv6ns/fdaAGtudZpbq4gH9eYB9bW87QbuOohz9gLSDqWYA9z/a7V+T9L3s19XIPJQry/SnCisidTvRXzha68rgRdq7TMGeM45t9s5V+mc+8459/6hXMzMrjazVWa2y8zSzey6WttvM7MsM8s0s5/4W0z6+7dNMrPvzKzIzLaa2d01jvteV4+/JeVeM/vKf60P97Ygmlmkmb1kZjvNrMDMFppZVzP7E3Ac8E9/q8c/G3A/5/lbKIfVse0EM8vwd//l+ve7tMb2htzPVDPbAnzqf/6/Zpbtb+H83MyG1jjmOTP7l5m976//KzPrZmYPm1m+ma02s5E19k/0t5jmmNlGM7vJ//zpwK+Bi/znWeZ/PtrMnvF/f7aZ2R/N3+1sZlf5r/eQmeUB++6lxvX2tdbVuL8rzWyL//X5TX2vt//YKDP7zMz+YWbmf/p5INnMjj/Aof8Apuz9earnGhuAvsB7/tcgwv96zTCzPDNbb2bX1rq3N/w/V0XAVQ25l/1ceyCwxv+wwMz2fu+P9v+sFvr/e/R+jg81s/v9r2k6MKnW9qv8v3u7/N/3S+s6j0hTUVgTqd98oKOZHeF/470IqN39NR94zMwuNrOeh3m9HcBkoCNwNfCQmY2CfSHhFmAi0B+o/ca7G1+w7ITvDegGO/BYokv814gHwoFf+Z+/EogGkvC1FF4PlDjnfgN8Adzob/W48UA3YmZXA38FJjrnVuxnt25ALNDdf90nzWzQQdzP8cARwGn+x+8DA/z3tAT4T639LwR+679mGfCNf79Y4A18XWuYWQjwHrDMX9vJwC/N7DTn3AfAffyvFWiE/9zPA5X4vjcjgVOBmt2a44B0f21/2s/rUduxwCD/9X9nZkccaGfzjd36BPjKOXeT+9+agnv8NR/outuAp6gjSNbmnOsHbAHO8r8GZcArQAaQCJwP3GdmJ9c47Bx8r3Enfvh92essf9hLM7Mb9nPttcDeEN7JOXeSmcUAs/AFzi74vo+zrO6xbNfi+x0bCaT4awXAzNr7z3GGcy4KOBpYeuBXQySwFNZEGmZv69opwGp8b2o1XYAvxNwFbDTfmLYxtfbJ9bdS7f1X55uuc26Wc26D85kHfIivNQt8QWO6cy7NObcH+EOtY+c655Y756qdc6n43jwP1JIy3Tm31jlXArwOHOl/vgLfG15/51yVc26xc67oAOepyy+BW4ETnHPr69n3Ludcmf9+Z/nvs6H3c7e/RbPEf8yzzrld/vBwNzDCzKJr7P+2/35KgbeBUufcC/6xfK/hewMHX2tpnHPuHudcub877ing4rpuwMy6AmcAv/TXswN4qNb+mc65R/2tryX1vCZ7/cE5V+KcW4YvOI44wL6JwDzgv86539ax/Qmgp5mdcYBz/BlfYBp6gH1+wHxj3Y4FbnfOlTrnlgJPA5fX2O0b59w7/u9nXff/Or7gHYcvUP3OzKbUsV9dJgHrnHMv+l/fV/D9rp5Vx74XAg8757Y65/Lw3XNN1cAwM2vrnMtyzqmrVTylsCbSMC/ia4W6ih92geKcy3fO3eGcG4pvLM1S4J0aXVAAsc65TjX+rarrQmZ2hpnN97cuFABn4mv1Ad+b8dYau2+tdew4f/dXjpkV4msR+96HI2rJrvH1HqBDjfudA7xqvu7Wv5lvQPfBuBV4zDmXUc9++c653TUeb8Z3nw29n32vgb976y9mtsHf1bbJv6nmMTXHEpbU8Xjva9ALSKwZsPF1fXbdz330AtoAWTX2fwJfK9oPaj0I+/se1WUSvvGUj9e10R9g7/X/s/3skwP8E9+HBw5GIpDnnNtV47nN+Fol9zrg/TvnVjrnMv3/g/A18Ag1Wr0acP3NtZ6rff2a+26ttd/eGnbjaz2/Ht/3cpaZDW5gDSIBobAm0gDOuc34PmhwJvBWPfvmAvfje0OIOZjrmFkE8Kb/+K7OuU7AbP73xpoF9KhxSNL3z8DLwAwgyTkXje9Nu8435XruocI59wfn3BB83UCT+d+4Pbf/I7/nVOC3ZnZePft19nc97dUTyPR/3ZD7qVnPJfi62ibi68bt7X/+oF8DfG/mG2sF7Cjn3Jl1XHfv/mV8P5R39Af4umoNhKeAD4DZtV7Tmqbje23OPcB5/g6cCIw+iGtnAjFmFlXjuZ58vxX6YO/f0fDvXSa+wFxT7evvlcX3f3e+N3TBOTfHOXcKkICvde6pBtYgEhAKayINNxU4qVYrEABm9lczG2ZmYf43qxuA9YcwnUA4EAHkAJX+7qpTa2x/HbjaP36uHfC7WsdH4WvdKDXfNA2XHOT1997PiWY23D9Grwhft+jeKT+24xtYXp804HR8Y/nOrmffP5hZuJkdhy8Y/tf//MHeTxS+wLQTaIdvjNahWgAUmdntZtbW32o3rEb39nagt39sG865LHxd1g+YWUczCzGzfvUM6A+EG/ENvp9pZrU/tYxzrhJf9/Dt+zuBc64AeAC4raEXdc5tBb4G/my+D6gk4/ud2d/YtB8ws3PMrLP5jAVuAt5t4OGzgYFmdon/9/AiYAgws459XwduMt+UO52BO2rU0NXMzvaH3TKgmP/97It4QmFNpIH848gW7WdzO3zjnwrwDSDvBdQOKAX2/fmjbqnjGrvwvUG9DuTjCyczamx/H9/g58+A9fgGx4PvTQXgp8A9ZrYLX5B7/aBv1KcbvoHgRcAqfOOg9n6o4hHgfPN9evIfBzqJf5zVZOCpA4yTysZ3r5n43tivd86tPsT7eQFfl9Y2YCW+D34cEv8YtrPwjePbCOTiG4O1d/zb3kC508yW+L++Al/gXum/pzfwtc40Gf8HCqbha+l718wi69jtFXytSwfyCAcfUqbga83MxPf78Hvn3EcHcfzF+H6ud+H7Xv7VOfd8Qw70/4/RZOD/8IX124DJ/pbu2p7C182/DN+HS2q2lof4z5GJb7qT4/H9HIp4xv73QSERaW78H1JYAUT4W0yaFTM7AXjJOdejvn1FRFortayJNDNmdq6/y7Azvmkx3muOQU1ERBpGYU2k+bkO35i2Dfi6qeqci0pERFoGdYOKiIiIBDG1rImIiIgEsWa18HF9YmNjXe/evb0uQ0RERKReixcvznXOxdW3X0DDmn8dw0eAUOBp59xfam2fADwMJAMXO+feqLW9I75pA9529axBCNC7d28WLdrfzAoiIiIiwcPMaq+6UaeAdYP6J9N8DN9aeUOAKWY2pNZuW/At3/Pyfk5zL775nURERERapUCOWRuLbwb3dOdcOfAqvmVg9nHObfIvzlxd+2AzG41vDb4PA1ijiIiISFALZFjrzvcXys2g7gV1f8C/fMsD+BaCrm/faWa2yMwW5eTkHFKhIiIiIsEqkGGtrsV3GzpPyE+B2f615g7IOfekcy7FOZcSF1fvGD0RERGRZiWQHzDIAJJqPO6Bb621hjgKOM7Mfgp0AMLNrNg5d0c9x4mIiIi0KIEMawuBAWbWB9+iyhfjW5S6Xs65S/d+bWZXASkKaiIiItIaBawb1L9W4Y3AHHzTb7zunEszs3vM7GwAMxtjZhnABcATZpYWqHpEREREmqMWtdxUSkqK0zxrIiIi0hyY2WLnXEp9+2m5KREREZEgprAmIiIiEsQU1kRERESCmMKaiIiISBBTWBMREREJYgprIiIiIkFMYU1EREQEKKus4qOV2/nHJ+u8LuV7ArmCgYiIiEhQq6yq5pv0nby3LJMPVmRTVFpJl/bhTD22D+0jgiMmBUcVIiIiIk2kutqxZEs+M5ZlMnt5FrnF5XSICOPUoV05a0Qix/aPpU1o8HQ+KqyJiIhIi+ecIy2ziBnLMpm5LJPMwlIiwkKYeERXzhqRwAmD4olsE+p1mXVSWBMREZEWa/2OXcxYlsXMZZmk5+4mLMSYMDCO204fzMQhXekQJF2dBxL8FYqIiIgchK15e3gvNZMZSzNZnb0LMziqbxeundCXM4Z1o1O7cK9LPCgKayIiItLs7SgqZWZqFu+lZvLdlgIARvXsxO/PGsKk4QnEd4z0uMJDp7AmIiIizVL+7nLeX5HNe8symb9xJ87BEQkduf30wUxOTiAppp3XJTYKhTURERFpNorLKvloZTYzlmbyxbpcKqsdfWLb8/OTBnD2iAT6x0d5XWKjU1gTERGRoFZV7fhoZTbvLs3k09U7KKusJjE6kqnH9uGsEYkMTeyImXldZsAorImIiEjQ+np9LvfOWsWqrCJiO4Rz8ZgkzhqRyKienQkJabkBrSaFNREREQk66TnF3Dd7NR+v2k73Tm35x5SRnDmsG2FBNFltU1FYExERkaBRuKeCRz5ZxwvfbCIiLIRbTxvE1GP7BO2EtU1BYU1EREQ8V1FVzX/mb+bhT9ZRWFLBRSlJ3HLqQOKjmu+UG41FYU1EREQ845zjszU7+NOsVWzI2c3R/brw20lDGJLY0evSgobCmoiIiHhiTfYu/jhrJV+sy6VPbHueuiKFiUfEt+hPdh4KhTURERFpUrnFZTz40VpeXbCFqMg2/G7yEC4b34vwsNb34YGGUFgTERGRJlFWWcX0rzbx2KfrKamo4oqjevPLiQOa3VqdTU1hTURERALKOcf7K7L58/ur2JpXwsmD4/n1pCPoF9fB69KaBYU1ERERCZjUjAL+OHMVCzblMbhbFC9NHcexA2K9LqtZUVgTERGRRpdVWMLf56zhrSXbiO0Qzn3nDueiMUmEtpJVBxqTwpqIiIg0mj3llTwxL50nPt9AdTVcf3w/fnZiP6Ii23hdWrOlsCYiIiKHrbra8fZ32/j7nDVkF5UyaXgCd5wxmKSYdl6X1uwprImIiMhhWbgpj3tnriQ1o5DkHtE8eslIxvSO8bqsFkNhTURERA7J1rw9/Pn9Vcxenk23jpE8eOEIfnRkd0I0Lq1RKayJiIjIQSmtqOLxeRv419wNhJpx88SBXDuhD+3CFSsCQa+qiIiINNjcNTv4/Yw0Nu/cw+TkBH47aQjdorXYeiAprImIiEi9MgtKuHfmSt5fkU3f2PaaL60JKayJiIjIflVUVTP9q408/PE6qqodt542iJ8c14eIsFCvS2s1FNZERESkTt+m7+Sud1ewdnsxE4+I5/dnDdVUHB5QWBMREZHvydlVxp/fX8VbS7bRvVNbnroihVOGdPW6rFZLYU1EREQAqKp2vPztZv42Zw2lFVX87MR+3HjiANqGq8vTSwprIiIiwtKtBdz1zgqWbyvkmP5d+MPZw+gf38HrsgSFNRERkVatYE85f5+zhpcXbCGuQwSPThnJ5OQEzDSxbbBQWBMREWmFqqsdby7J4M/vr6awpIKrj+7DzacM0ILrQUhhTUREpJVZnV3EXe+sYOGmfEb36sy95wxjSGJHr8uS/VBYExERaSWKyyp5+KO1TP96Ex0jw/jbecmcP7qH1vIMcgprIiIiLZxzjlnLs7h35kp27Crj4jE9ue20QXRuH+51adIACmsiIiItWHpOMb+fkcYX63IZmtiRxy8bzcienb0uSw6CwpqIiEgLVFpRxWOfreeJeelEhIXwh7OHctn4XoSqy7PZUVgTERFpYT5ZtZ2730tja14J547szp1nDiY+KtLrsuQQKayJiIi0ADuKSpm9PIv3UrNYvDmf/vEdeOXa8RzVr4vXpclhUlgTERFppnYWl/H+imxmpmby7cY8nIPB3aK4a/IQLh/fi/CwEK9LlEagsCYiItKMFOwpZ05aNjNTs/h6w06qqh1949rz85MGcFZyAgO6RnldojQyhTUREZEgt6u0go9WbmdmahZfrMuhosrRM6Yd103oy+TkRI5IiNLyUC2YwpqIiEgQ2lNeycerdjBzWSZz1+ZQXllN905tufqYPkxOTmB492gFtFZCYU1ERCRIlFZU8dnqHcxMzeKT1dspragmPiqCS8b25KwRiYxM6qTVBlohhTUREREPlVVW8cXaXGamZvLRyu3sLq+iS/twzh/dg8nJiYzpHaO50Vo5hTUREZEmVlFVzVfrc5mZmsWctGx2lVbSqV0bzhqRyOTkRMb3jSEsVJ/kFB+FNRERkSZQVe2Yn76TmamZfLAim/w9FURFhHHK0K6cNSKRY/vH0kYBTeqgsCYiIhJA1dWOVxZu4aGP1pFbXEa78FAmHtGVyckJTBgYR2SbUK9LlCCnsCYiIhIg6TnF3PnWcr7dmMfYPjHcc85QThwUT9twBTRpOIU1ERGRRlZRVc1TX6Tz8MfriAgL4a/nDefClCRNtSGHRGFNRESkES3PKOT2N1NZmVXEGcO68YezhxLfUYuoy6FTWBMREWkEJeVVPPzxWp76Ip3YDhE8ftloTh/WzeuypAVQWBMRETlMX6/P5c63l7N55x4uHpPEnWceQXTbNl6XJS2EwpqIiMghKtxTwX2zV/Haoq306tKOl68dx9H9Yr0uS1oYhTUREZFD8MGKLO56N4283eVcd3xfbp44UNNwSEAorImIiByEHUWl/O7dND5Iy2ZIQkemXzWGYd2jvS5LWrCATpVsZqeb2RozW29md9SxfYKZLTGzSjM7v8bzR5rZN2aWZmapZnZRIOsUERGpj3OOVxds4eQH5/HZmh3cfvpg3r3xGAU1CbiAtayZWSjwGHAKkAEsNLMZzrmVNXbbAlwF/KrW4XuAK5xz68wsEVhsZnOccwWBqldERGR/NuXu5s63lvNN+k7G9YnhL+cl0ye2vddlSSsRyG7QscB651w6gJm9CpwD7AtrzrlN/m3VNQ90zq2t8XWmme0A4gCFNRERaTKVVdU88+VGHvxoLeGhIfz5x8O5KCWJkBBNbitNJ5BhrTuwtcbjDGDcwZ7EzMYC4cCG/WyfBkwD6Nmz58FXKSIiUoe0TN/ktiu2FXHqkK7c+6NhdNXktuKBQIa1uv63wx3UCcwSgBeBK51z1XXt45x7EngSICUl5aDOLyIiUltpRRWPfLKOJz9Pp3O7cP596ShOH9ZNS0WJZwIZ1jKApBqPewCZDT3YzDoCs4DfOufmN3JtIiIiPzA/fSd3vrWcjbm7uTClB785cwjR7TS5rXgrkGFtITDAzPoA24CLgUsacqCZhQNvAy845/4buBJFRESgqLSCP89ezSsLttAzph3/+ck4jumvyW0lOAQsrDnnKs3sRmAOEAo865xLM7N7gEXOuRlmNgZfKOsMnGVmf3DODQUuBCYAXczsKv8pr3LOLQ1UvSIi0jp9mJbNXe+uIGdXGdMm+Ca3bRuuyW0leJhzLWeYV0pKilu0aJHXZYiISJBzzrFkSwFPfr6BOWnbGdwtir+dn0xyj05elyatiJktds6l1LefVjAQEZFWY3dZJe8uzeTF+ZtZlVVEh4gwbj1tENMm9KVNaEDniRc5ZAprIiLS4q3dvouX5m/mrSXbKC6r5IiEjtx37nDOOTKR9hF6K5Tgpp9QERFpkcorq/kgLZuX5m9mwcY8wkNDmJycwKXjezGqZydNxSHNhsKaiIi0KBn5e3hlwRZeW7iV3OJyesa0484zBnNBShIx7cO9Lk/koCmsiYhIs1dd7Zi3LoeXvtnMZ2t2AHDS4K5cNr4nEwbEaXkoadYU1kREpNnaWVzG64syeHnBZrbmlRDbIYKfntCfKeN60r1TW6/LE2kUCmsiItKs+KbdyOfFbzYze3k25VXVjOsTw22nDea0od0ID9OnOqVlUVgTEZFmobiskne+28ZL8zezOnsXHSLCmDI2iUvH92Jg1yivyxMJGIU1EREJamuyfdNuvP2db9qNIZp2Q1oZ/ZSLiEjQKaus4oMV2fxn/hYWbMojPCyEycM17Ya0TgprIiISFMoqq1iwMY/PVucwY9k2Tbsh4qewJiIintlWUMLcNTv4bHUOX2/IZU95FeFhIRw/MI7LxvfiuP6xmnZDWj2FNRERaTIVVdUs2pTvC2hrdrB2ezEAPTq35bxRPThxcBxH9Y2lbXiox5WKBA+FNRERCajtRaXMXbODuWty+HJdLrvKKmkTaoztE8OFKUmcMCiOfnEdNA5NZD8U1kREpFFVVlWzdGsBn/m7N1dmFQHQrWMkk0ckcMKgeI7pH0sHfZJTpEH0myIiIoctt7iMeWty+GzNDj5fm0NRaSWhIcboXp25/fTBnDg4jkFdo9R6JnIIFNZEROSgVVU7UjMKmLsmh7lrdpC6rRDnILZDBKcO7caJg+I5dkAs0W3beF2qSLOnsCYiIg2Sv7ucz9flMHdNDvPW5pC3uxwzGJnUiVsmDuTEwfEMSeioT2+KNDKFNRERqddrC7fw67dXUFXt6NyuDccPjOPEwfEcNyBO85+JBJjCmoiIHNCKbYXc9U4a4/rE8KvTBjGiRydC1Xom0mQU1kREZL92lVbws5eXENM+nH9eMkqtaCIeUFgTEZE6Oee4463lZOSX8Oq08QpqIh4J8boAEREJTi99u4VZqVn86tRBjOkd43U5Iq2WwpqIiPzAim2F3PveSk4cFMd1E/p6XY5Iq6awJiIi37N3nFqXDuE8cOGRmopDxGMasyYiIvs457jjTd84tdc0Tk0kKKhlTURE9nlp/mZmLc/i1tMGkaJxaiJBQWFNREQA/zi1mas4cVAc047TODWRYKGwJiIiFGmcmkjQ0pg1EZFWzjnHnRqnJhK01LImItLKaZyaSHBTWBMRacU0Tk0k+CmsiYi0UhqnJtI8aMyaiEgr5JtPLZWM/BJev07j1ESCmVrWRERaoRfnb2b28mxuO20Qo3tpnJpIMFNYExFpZZZnFPLHmas4aXA812qcmkjQU1gTEWlFvjdO7YIRGqcm0gxozJqISCuxd5zatgLfOLXOGqcm0iyoZU1EpJXQODWR5klhTUSkFdA4NZHmS2FNRKSF0zg1keZNY9ZERBrZnW8tJ393OdOO78uonp09rcU5x+1vpJJZUMJrGqcm0iwprImINKL56Tt5ZcEW2oQaH6RlM65PDNef0I8TBsZh1vQtWi98s5n3V2Tz6zMHa5yaSDOlblARkUbinOPBD9cSHxXB/DtP5q7JQ9iSt4erpy/kzH98ybtLt1FZVd1k9aRmFPCnWas4eXA8PzlW49REmiuFNRGRRvLl+lwWbMrjxpP606VDBFOP7cO8W0/k/gtGUFFVzS9eXcqJD8zlxW82UVpRFdBaCkt849RiO4Rzv8apiTRrCmsiIo3AOccDH64lMTqSi8Yk7Xs+PCyE80f34MNfTuDJy0cT2yGCu95N45i/fMpjn62nsKQiILXc8WYqWQWlPHrJKI1TE2nmNGZNRKQRfLp6B0u3FvCXHw8nIiz0B9tDQoxTh3bjlCFdWbAxj3/P28Df56zhX5+t59LxvZh6bB+6doxslFq+P07N2w84iMjhU1gTETlM1dWOBz9aS8+Ydpw3uscB9zUzxvXtwri+XViZWcQTn2/g6S/See6rTZw7sjvTju9Lv7gOh1xLakYBf5y1UuPURFoQdYOKiBymOWnZpGUW8YuTB9AmtOF/VockduSRi0cy79YTuXhsEu8s3cbEB+dxw0uLWba14KDr2DtOLa5DhMapibQgalkTETkMVdWOhz5eS7+49vxoZPdDOkdSTDvuOWcYN508gOe/3sTzX2/i/RXZHN2vCzec0I9j+8fWO+3H3vnUsgpKee26ozROTaQFUcuaiMhhmJmaydrtxfxy4kBCD7MlK7ZDBP936iC+vvNkfnPmEWzIKebyZxYw+dEvmZmaSVW12++xz3+9iQ/Ssrnt9EEapybSwiisiYgcosqqah7+eB2Du0UxaXhCo523Q0QY107oy+e3ncjfzkumpKKKG1/+jpMfmMt/vt38g2k/UjMK+NPsVUw8Qut+irRECmsiIofo7e+2sTF3NzefMjAg48MiwkK5cEwSH998PI9fNproduH85u0VHPvXz/j33A0UlVbsG6cWHxXJ/ReM8GSVBBEJLI1ZExE5BOWV1TzyyTqGd4/m1CFdA3qtkBDj9GHdOG1oV75J38nj89L56wer+ddn60mKabdvnFqndhqnJtISKayJiByC/y7eSkZ+Cff+aFiTtWaZGUf3i+XofrGs2FbI4/M2MHt5Fr+ZNETj1ERaMIU1EZGDVFpRxT8/Xc+onp04YWCcJzUM6x7NPy8ZRWlFFZFtfjgJr4i0HBqzJiJykF5ZsIWswlL+79RBno8RU1ATafkU1kREDkJJeRWPfbaB8X1jOLpfF6/LEZFWQGFNROQgvPDNJnKLy4KiVU1EWgeFNRGRBiouq+TxeRuYMDCOMb1jvC5HRFoJhTURkQZ67quN5O+p4JZTBnpdioi0IgprIiINUFhSwZOfpzPxiHiOTOrkdTki0ooorImINMAzX6RTVFrJzWpVE5EmprAmIlKPvN3lPPPlRs4c3o2hidFelyMirYzCmohIPZ74fAN7Kqr45US1qolI0wtoWDOz083kTsg6AAAgAElEQVRsjZmtN7M76tg+wcyWmFmlmZ1fa9uVZrbO/+/KQNYpIrI/O3aV8vzXmzhnRCIDu0Z5XY6ItEIBC2tmFgo8BpwBDAGmmNmQWrttAa4CXq51bAzwe2AcMBb4vZlp4TsRaXL/nruBiirHL9SqJiIeCWTL2lhgvXMu3TlXDrwKnFNzB+fcJudcKlBd69jTgI+cc3nOuXzgI+D0ANYqIvIDWYUl/OfbLZw3qjt9Ytt7XY6ItFKBDGvdga01Hmf4n2vUY81smpktMrNFOTk5h1SoiEhd/vnpepxz/PykAV6XIiKtWCDDWl3rsLjGPtY596RzLsU5lxIXF9fg4kREDmRr3h5eX7SVC1OSSIpp53U5ItKKBTKsZQBJNR73ADKb4FgRkcP26KfrMDNuPKm/16WISCsXyLC2EBhgZn3MLBy4GJjRwGPnAKeaWWf/BwtO9T8nIhJwG3N38+aSbVw6ricJ0W29LkdEWrmAhTXnXCVwI76QtQp43TmXZmb3mNnZAGY2xswygAuAJ8wszX9sHnAvvsC3ELjH/5yISMA98vFawkNDuOGEfl6XIiJCWCBP7pybDcyu9dzvany9EF8XZ13HPgs8G8j6RERqW7t9F+8uy2TahL7ER0V6XY6IiFYwEBGp6eGP19KuTSjXTVCrmogEB4U1ERG/tMxCZi/PZuqxfYhpH+51OSIigMKaiMg+D320lo6RYUw9rq/XpYiI7KOwJiICLN1awMerdjBtQl+i27bxuhwRkX0U1kREgAc+XEPndm246pg+XpciIvI9Cmsi0uot3JTHF+tyuf74fnSICOiH5EVEDprCmoi0as457p+zhtgOEVxxVG+vyxER+QGFNRFp1b7esJNvN+bxsxP70TY81OtyRER+QGFNRFot5xwPfLiGhOhIpozt6XU5IiJ1UlgTkVZr7poclmwp4MaT+hPZRq1qIhKcFNZEpFVyzvHAR2vo0bktF4xO8rocEZH9UlgTkVZpTtp2Vmwr4hcnDyA8TH8KRSR46S+UiLQ61dWOhz5aS9/Y9pw7srvX5YiIHJDCmoi0OrOWZ7Fm+y5+MXEAYaH6MygiwU1/pUSkVamsquahj9cysGsHJicnel2OiEi9FNZEpFV5d2km6Tm7uXniQEJDzOtyRETqpbAmIq1GRVU1j3yyjqGJHTltaDevyxERaRCFNRFpNd5YnMGWvD3ccspAQtSqJiLNhMKaiLQKZZVVPPrJOo5M6sRJg+O9LkdEpMEU1kSkxdtdVsndM9LILCzl/04diJla1USk+ThgWDOzy2p8fUytbTcGqigRkcbyYVo2pzw4j1cWbOWqo3tzbP9Yr0sSETko9bWs3VLj60drbbumkWsREWk02wpKuPaFRUx7cTFRkW1484ajuPvsoWpVE5FmJ6ye7bafr+t6LCLiucqqaqZ/tYmHPl5LtXPcccZgph7bhzaa/FZEmqn6wprbz9d1PRYR8dSSLfn85u0VrMoq4uTB8dx99lCSYtp5XZaIyGGpL6wNNrNUfK1o/fxf43/cN6CViYg0UOGeCv42ZzUvL9hC16hIHr9sFKcN7aYuTxFpEeoLa0c0SRUiIofAOceMZZncO3MlebvLufroPtxy6kA6RNT3p01EpPk44F8059zmmo/NrAswAdjinFscyMJERA5kY+5u7npnBV+uz2VEj2ieu3osw7pHe12WiEijO2BYM7OZwB3OuRVmlgAsARbh6xJ90jn3cFMUKSKyV1llFY/PTeexueuJCA3h3nOGcsm4XlrnU0RarPr6Cvo451b4v74a+Mg5d4WZRQFfAQprItJkvl6fy2/fWUF67m4mJyfwu8lDiO8Y6XVZIiIBVV9Yq6jx9cnAUwDOuV1mVh2wqkREasgtLuNPs1bx9nfb6BnTjuevGcvxA+O8LktEpEnUF9a2mtnPgQxgFPABgJm1BdoEuDYRaeWqqx2vLdrKX95fzZ7ySm48sT83ntSfyDahXpcmItJk6gtrU4F7gInARc65Av/z44HpgSxMRFq31dlF/ObtFSzenM/YPjHcd+4w+sdHeV2WiEiTq+/ToDuA6+t4/jPgs0AVJSKt157ySh75ZB3PfLGRqMgw/n5+MueP7qE500Sk1arv06AzDrTdOXd245YjIq3ZJ6u287t309hWUMKFKT2484wj6Nw+3OuyREQ8VV836FHAVuAV4Fu0HqiIBEBWYQl3z0hjTtp2BsR34PXrjmJsnxivyxIRCQr1hbVuwCnAFOASYBbwinMuLdCFiUjLV13tmP71Jh78cA2V1Y5bTxvEtcf1JTxMi66LiOxV35i1KnyfAP3AzCLwhba5ZnaPc+7RpihQRFqmssoqbv1vKjOWZXLCoDjuOXsYPbto0XURkdrqXUDPH9Im4QtqvYF/AG8FtiwRacl2lVZw/UuL+Wr9Tm47fRA3HN9PHyAQEdmP+j5g8DwwDHgf+EON1QxERA7Jjl2lXPXsQtZs38X9F4zg/NE9vC5JRCSo1deydjmwGxgI3FTj/3wNcM65jgGsTURamPScYq6cvoDcXeU8fWUKJw6K97okEZGgV9+YNY3yFZFGsXRrAdc8txCAV6aN58ikTh5XJCLSPNQ7Zk1E5HB9tmYHP31pCbFR4bxwzTj6xLb3uiQRkWZDYU1EAuqNxRnc/mYqg7tFMf3qMcRHRXpdkohIs6KwJiIB4Zzj8Xnp/PWD1RzTvwuPXzaaqMg2XpclItLsKKyJSKOrrnbcM3Mlz329ibNHJHL/BSM00a2IyCFSWBORRlVWWcUtry9jVmoWPzm2D78+8whCQjSHmojIoVJYE5FGU1RawbQXFjE/PY/fnHkE107o63VJIiLNnsKaiDSK7UWlXPnsAtbvKObhi47kRyO7e12SiEiLoLAmIodtQ04xVzyzgII95Tx71RgmDIzzuiQRkRZDYU1EDsuSLflMfW4hoSHGq9OOYniPaK9LEhFpURTWROSQfbJqOz97eQldO0bywjVj6dVFk92KiDQ2hTUROSSvL9zKnW8vZ0hCR6ZfPYbYDhFelyQi0iIprInIQXHO8dhn67n/w7UcNyCWxy8bTfsI/SkREQkU/YUVaSaKyypJzylmYNcoItuEelJDVbXj7hlpvDh/M+eO7M5fz0vWZLciIgGmsCbSDOQWl3Hh49+Qnrub8NAQhiR2ZHSvzozq2ZlRvTqREN024DWUVlTxy1eX8kFaNtcd35fbTxusyW5FRJqAwppIkCsqreDKZxeQWVjCvecMJSO/hCVb8nlp/mae+XIjAAnRkYzq2ZmRPTsxuldnhiZGN2qLV2FJBde+sIgFG/O4a/IQph7bp9HOLSIiB6awJhLESsqrmPrcQtZu38VTV6RwwqD4fdvKK6tZlVXEki35LN6cz3dbCpi1PAuA8LAQhnePZlTPTv7Wt8507Rh5SDVkFZZw1bMLSc8t5h9TRnL2iMRGuTcREWkYc855XUOjSUlJcYsWLfK6DJFGUV5ZzbQXFzFvbQ6PThnJ5OT6Q9L2olKWbM7fF+BWbCuivKoagO6d2jKqV+d9AW5IYkfahB649W3d9l1c+ewCikoreeLy0RzTP7ZR7k1ERMDMFjvnUurbTy1rIkGoqtpxy+tLmbsmhz//eHiDghpA146RnDE8gTOGJwC+RdXTMotY4m95W7gxj/eWZQIQERbCiB6dGNnL3/rWszNxUf+bfmPRpjymPr+INqEhvDptPMO6a7JbEREvKKyJBBnnHHe9u4KZqVncccZgpoztecjniggL3RfE9sos8I15W7K5gMVb8nn2y408UZUOQFJMW0b17EyvLu15Yt4GEju15YVrxpIU0+6w70tERA6NwppIkPnbnDW8/O0WbjihH9cf36/Rz5/YqS2Jndrua60rrahixbbCfQHu6w07eXdpJiN6RPPsVWPoosluRUQ8pbAmEkQen7eBf8/dwCXjenLbaYOa5JqRbUJJ6R1DSu8YwNeyl7OrjC4dIgjV1BwiIp5TWBMJEq8s2MJf3l/N5OQE7j1nGGbeBCUzI/4QPzkqIiKNT1OPiwSBmamZ/Prt5ZwwKI4HLzxSLVoiIrKPwpqIx+au2cHNry0lpVdn/n3paC3fJCIi3xPQdwUzO93M1pjZejO7o47tEWb2mn/7t2bW2/98GzN73syWm9kqM7szkHWKeGXRpjyuf2kxA+KjePrKMbQN92bNTxERCV4BC2tmFgo8BpwBDAGmmNmQWrtNBfKdc/2Bh4C/+p+/AIhwzg0HRgPX7Q1yIi1FWmYhVz+3kMTotrwwdSzRbdt4XZKIiAShQLasjQXWO+fSnXPlwKvAObX2OQd43v/1G8DJ5htV7YD2ZhYGtAXKgaIA1irSpNJzirny2QV0iAjjxZ+MI1bTY4iIyH4EMqx1B7bWeJzhf67OfZxzlUAh0AVfcNsNZAFbgPudc3l1XcTMppnZIjNblJOT07h3IBIAWYUlXP7MAqodvDh1HN07tfW6JBERCWKBDGt1fZyt9kKk+9tnLFAFJAJ9gP8zs751XcQ596RzLsU5lxIXF3c49YoE3M7iMi57+luKSip44Zqx9I/v4HVJIiIS5AIZ1jKApBqPewCZ+9vH3+UZDeQBlwAfOOcqnHM7gK+Aehc6FQlmu0oruGr6QjLyS3j6yhSttSkiIg0SyLC2EBhgZn3MLBy4GJhRa58ZwJX+r88HPnXOOXxdnyeZT3tgPLA6gLWKBFRpRRU/eX4Rq7KK+PdloxjXt4vXJYmISDMRsLDmH4N2IzAHWAW87pxLM7N7zOxs/27PAF3MbD1wC7B3eo/HgA7ACnyhb7pzLjVQtYoEUkVVNT/7zxIWbMrjgQtHcNLgrl6XJCIizYj5GrJahpSUFLdo0SKvyxDZp7racfPrS3l3aSb3/mgYl4/v5XVJIiISJMxssXOu3mFemipdJECcc9z9XhrvLs3k1tMGKaiJiMghUVgTCZAHP1rLC99sZtqEvvz0hH5elyMiIs2UwppIADz9RTqPfrqei1KSuPOMwfjmehYRETl4Cmsijez1hVv546xVnDm8G/f9eLiCmoiIHBaFNZFG9P7yLO54K5XjBsTy0EVHEhqioCYiIodHYU2kkXyxLodfvLqUI5M68cTlo4kIC/W6JBERaQEU1kQaweLN+Ux7YTF949oz/aqxtAsP87okERFpIRTWRA7Tqqwirp6+gK4dI3hh6lii27XxuiQREWlBFNZEDsPG3N1c8ewC2oaH8uLUccRHRXpdkoiItDAKayKHaP2OXVz0xDdUVTtemjqOpJh2XpckIiItkAbWiByCNdm7uPTp+YDx6rTxDOga5XVJIiLSQqllTeQgpWUWcvGT3xAaYrx23XgGKqiJiEgAqWVN5CAszyjksme+pV14KK9cO57ese29LklERFo4hTWRBvpuSz5XPLuAjpFteHXaeI1RExGRJqFuUJEGWLQpj8ufWUDnduG8dp2CmoiINB21rInU49v0nVz93EK6dozk5WvHkRDd1uuSRESkFVHLmsgBfLU+lyunLyAhOpLXpo1XUBMRkSansCayH/PW5nDNcwvpFdOeV6cdRXxHTXgrIiJNT92gInX4ZNV2bnhpCf3jO/DST8YR0z7c65JERKSVUsuaSC1z0rK5/qXFDOoWxcvXKqiJiIi31LImUsPs5Vnc9Mp3DOsezfPXjCW6rRZlFxERbymsifi9u3Qbt7y+jJFJnZh+9RiiIhXURETEewprIsCbizO49Y1lpPSOYfpVY2gfoV8NEREJDhqzJq3eawu38Ks3lnFUvy48d7WCmoiIBBe9K0mr9tL8zfz2nRVMGBjHk5ePJrJNqNcliYiIfI9a1qTVmv7VRn77zgpOHhyvoCYiIkFLLWvSKj31eTp/mr2K04Z25dEpowgP0/+3iIhIcFJYk1bnsc/W8/c5a5g0PIGHLz6SNqEKaiIiErwU1qRVeeTjdTz08VrOOTKRBy4YQZiCmoiIBDmFNWkVnHM8+NFaHv10PeeN6sHfzk8mNMS8LktERKReCmvS4jnn+MsHq3liXjoXj0nivnOHE6KgJiIizYTCmrRozjnunbmKZ7/ayGXje3LP2cMU1EREpFlRWJMWq7ra8fsZabw4fzNXH9Ob300egpmCmoiINC8Ka9IiVVc7fvPOcl5ZsJVpE/py5xmDFdRERKRZUliTFqe8spo731rOm0sy+NmJ/fjVqYMU1EREpNlSWJMWJSN/Dz97+TuWbS3g5okDuenk/gpqIiLSrCmsSYvx6ert3PzaMqqrHf++dBRnDE/wuiQREZHDprAmzV5lVTUPfLSWf8/dwBEJHfn3paPoHdve67JEREQahcKaNGs7ikr5+Svf8e3GPC4ek8TdZw/VguwiItKiKKxJs/X1hlxuemUpu8sqeeCCEZw3uofXJYmIiDQ6hTVpdqqrHf+au54HP1pLn9j2vHztOAZ2jfK6LBERkYBQWJNmJX93OTe/vpS5a3I4e0Qif/7xcNpH6MdYRERaLr3LNZBzjjlp2wkPM04a3NXrclqlJVvyufE/S8gtLufeHw3jsnE9NS2HiIi0eAprDWRmPPbZekIMhbUm5pxj+lebuG/2KrpFR/LGDUeR3KOT12WJiIg0iRCvC2hOJiUnsCyjkC0793hdSqtRVFrBT/+zhHtmruSEQfHM+vlxCmoiItKqKKwdhEn+SVZnLc/yuJLWYWVmEWc/+iUfrtzOnWcM5qkrRhPdro3XZYmIiDQphbWDkBTTjiOTOjEzNdPrUlo05xyvLdzCuf/6ipKKKl65djzXHd9P49NERKRVUlg7SJOTE0jLLGJj7m6vS2mR9pRX8qv/pnL7m8tJ6d2ZWTcdx9g+MV6XJSIi4hmFtYN05t6uULWuNbr1O4r50WNf8dZ3Gfzi5AG8cM04YjtEeF2WiIiIpxTWDlJip7ak9OrMzFSNW2tMM5Zlcs4/vyS3uJznrx7LzacMJDRE3Z4iIiIKa4dgUnICq7N3sX7HLq9LafbKKqu4650V3PTKdwxO6Mism45lwsA4r8sSEREJGgprh+DM4QmYoda1w7Q1bw8XPP4NL87fzLXH9eHVaeNJiG7rdVkiIiJBRWHtEHTtGMnY3jHMTM3COed1Oc3Sxyu3M+kfX7AxdzdPXD6a30waQptQ/TiKiIjUpnfHQzQ5OYH1O4pZs11doQejsqqaP7+/ip+8sIikmHbM/PmxnDa0m9dliYiIBC2FtUN0+rAEQgxmqSu0wbYXlXLJU9/yxLx0LhnXkzdvOJpeXdp7XZaIiEhQU1g7RHFRERzVr4u6Qhtoa94efvyvr1m+rZCHLzqS+84dTmSbUK/LEhERCXoKa4dh0vBENubuJi2zyOtSglpG/h6mPDWfXaUVvH7dUfxoZHevSxIREWk2FNYOw+nDuhEaYlor9AC2FZQw5an5FJVU8J+fjGd4j2ivSxIREWlWFNYOQ0z7cI7pH8vM1Ex1hdYhs6CEKU/Op2BPBS9OHaegJiIicggU1g7T5OEJbM0rITWj0OtSgkpWoa9FLX93OS9cM5YRSZ28LklERKRZUlg7TKcN7UabUHWF1pRdWMqUJ+ezs7ic56eOZWTPzl6XJCIi0mwprB2m6HZtOG5AHLP0qVDANz3HlKfmk7OrjOevGcsoBTUREZHDorDWCCYNT2BbQQlLthR4XYqndviD2o6iUp6/ZiyjeymoiYiIHC6FtUZwytCuhIeGtOoJcnfs8gW17MJSnrtmLCm9Y7wuSUREpEVQWGsEHSPbcPygOGYvz6K6uvV1hebsKuOSp74lq7CU564eyxgFNRERkUYT0LBmZqeb2RozW29md9SxPcLMXvNv/9bMetfYlmxm35hZmpktN7PIQNZ6uCYnJ5BdVMqizflel9KkcovLuOSp+WzLL+HZq8Ywto+CmoiISGMKWFgzs1DgMeAMYAgwxcyG1NptKpDvnOsPPAT81X9sGPAScL1zbihwAlARqFobw8lHdCUiLIRZqZlel9JkdvqD2tb8PTx71RjG9+3idUkiIiItTiBb1sYC651z6c65cuBV4Jxa+5wDPO//+g3gZDMz4FQg1Tm3DMA5t9M5VxXAWg9bh4gwThocz+wV2VS1gq7QvN3lXPr0t2zJ28OzV47hqH4KaiIiIoEQyLDWHdha43GG/7k693HOVQKFQBdgIODMbI6ZLTGz2/Z3ETObZmaLzGxRTk5Oo97AwZqUnEDOrjIWbMzztI5Ay99dziVPzWdj7m6euXIMR/eP9bokERGRFiuQYc3qeK52k9P+9gkDjgUu9f/3XDM7ua6LOOeedM6lOOdS4uLiDqfew3bS4HjatgllZgvuCs33t6il5+7m6StTOEZBTUREJKACGdYygKQaj3sAtVPMvn3849SigTz/8/Occ7nOuT3AbGBUAGttFO3Cwzj5iHg+WJFNZVW11+U0uoI95Vz2zLeszynmqStSOG6At+FYRESkNQhkWFsIDDCzPmYWDlwMzKi1zwzgSv/X5wOfOt8yAHOAZDNr5w9xxwMrA1hro5mcnMDO3eXMT29ZXaGFeyq47JlvWbe9mCcvH83xAxXUREREmkLAwpp/DNqN+ILXKuB151yamd1jZmf7d3sG6GJm64FbgDv8x+YDD+ILfEuBJc65WYGqtTGdMCie9uEtqyu0sMQX1NZmF/PE5aM5YVC81yWJiIi0GtaS1rNMSUlxixYt8roMfvnqd8xdm8PC30ykTWjznne4sKSCK575lpVZRTxx+WhOGtzV65JERERaBDNb7JxLqW+/5p0kgtSk5EQK9lTw1fpcr0s5LEWlFVzx7AJWZhXx70sV1ERERLygsBYAEwbGEhUZxsxmvFbortIKrnx2ASszC/nXpaOZOERBTURExAsKawEQERbKqUO6MSctm7LKoJ7Lt07FZZVc+ewClmcU8s9LRnGKgpqIiIhnFNYCZHJyArtKK/lyXfPqCi0uq+SqZxeQ6g9qpw3t5nVJIiIirZrCWoAc0z+W6LZtmlVX6O6ySq6evoDvthbw6JSRnD5MQU1ERMRrCmsBEh4WwulDu/HRyu2UVgR/V6gvqC1kyZYC/nHxSM4YnuB1SSIiIoLCWkBNSk6guKySeWu9XbO0PnvKK7nmuYUs3pLPIxcfyaRkBTUREZFgobAWQEf360JM+/Cg7gotKa9i6nOLWLgpj4cuOpLJyYlelyQiIiI1KKwFUFhoCKcP68Ynq7ZTUh6cXaH3zExj/sadPHTRkZw9QkFNREQk2CisBdjk4QnsKa/iszU7vC7lB+atzeGVBVu5bkI/zjmyu9fliIiISB0U1gJsXN8uxHaICLq1QgtLKrj9jVQGxHfglxMHeF2OiIiI7IfCWoCFhhhnDu/Gp6t3sLus0uty9vnjzJXkFJdx/wUjiGwT6nU5IiIish8Ka01g0vAESiuq+WR1cHSFfrp6O/9dnMENx/djRFInr8sRERGRA1BYawJjesfQtWMEM5d53xVauKeCO95czuBuUfz85P5elyMiIiL1UFhrAiEhxpnDE5i7NoddpRWe1vKH99LI213O/ReMICJM3Z8iIiLBTmGtiUxOTqC8spqPV233rIYP07J567tt/OzE/gzrHu1ZHSIiItJwCmtNZGRSZxKjI5m5zJsJcvN3l/Prt1cwJKEjPztR3Z8iIiLNhcJaEwkJMSYlJ/D5uhwK9zR9V+jvZ6RRWOLr/gwP07ddRESkudC7dhOalJxIRZXjw5XZTXrd95dnMWNZJjedNIAhiR2b9NoiIiJyeBTWmtCIHtEkxbRt0rVCdxaX8dt3VjC8ezTXn9Cvya4rIiIijUNhrQmZGZOGJ/LV+lzyd5c3yTV/924au0oruf+CEbQJ1bdbRESkudG7dxObnJxAZbVjTlrgu0JnpmYya3kWvzxlAIO6RQX8eiIiItL4FNaa2NDEjvTu0i7gXaE5u8q4650VjEjqxLTj+gb0WiIiIhI4CmtNzMyYnJzI1xtyyS0uC8g1nHP89p3l7C6v4oELkglT96eIiEizpXdxD0xKTqDawQcrAtMVOmNZJnPStvOrUwfSP17dnyIiIs2ZwpoHBneLol9ce2amNv5aoTuKSvndu2mM6tmJqceq+1NERKS5U1jzwN6u0G835rGjqLTRzuuc49dvL6e0oor7LxhBaIg12rlFRETEGwprHpmcnIBz8H4jdoW+tWQbH6/awW2nD6ZvXIdGO6+IiIh4R2HNIwO6RjGoa1SjdYVmF5Zy93tpjO0dw9VH926Uc4qIiIj3FNY8NDk5gYWb8skqLDms8zjnuOOtVCqrHH87P5kQdX+KiIi0GAprHpqUnADA7OWH1xX630UZzF2Twx1nDKZ3bPvGKE1ERESChMKah/rGdWBIQsfD6grdVlDCvTNXMr5vDJeP79WI1YmIiEgwUFjz2OQRCXy3pYCM/D0HfaxzjjveTKXKOf5+/gh1f8r/t3fvcXLV9f3H328SuRpDICEXbpFgSZAFpBGtqFxEBIyAlZqgVkBsGiv1x8/auhXFldZW2kZ+peRXCghEVOSiKBIQ+GVjTRXUgAm3cAk3WROTQLiDSODz++N8J5lMZmZnszsz3919PR+PeezMOd9zzud75szMe885MwcAMAQR1tpsRsckSdINd/X98lNX/PJxLX7wCX3h2GnafaftB7o0AACQAcJam+2x8/baf7fRfb5W6OPrXtRXF9yrd+49Vh992x5Nqg4AALQbYS0DM/afqDt7ntFjT77QUPvXXgt9/nt3yra+9qEO2Rz+BABgqCKsZeDYjuJboQsaPBT67V88pp8/9KS++P5p2m0Mhz8BABjKCGsZ2G3M9nrLHjvq+mW9h7XfPPmi/umG+/TuPxqnmW/dvQXVAQCAdiKsZWLG/pN076pn9fDa52u2ee210OeuWaaRI6xzOPwJAMCwQFjLxLEdEyRJC+p80WD+rY/ql4+s01kz9tXE0du1qDIAANBOhLVMTBy9nd46eUzNb4U+8sQLOtuu+6IAACAASURBVOfH9+mIqbvoxD/ercXVAQCAdiGsZWTG/pN0/+rn9ODq5zYZ/uprob+9epm2HrGV/vlPOfwJAMBwQljLyDH7TZCtzfauXfqzR7Tksaf0lePfrPFv2LZN1QEAgHYgrGVklzdsq7e9cSddf+dKRYQkacWa5/WvN92v9+47XiccuGubKwQAAK1GWMvMjP0n6aG1L+j+1c/p1ddCn7t6mbbbeoS++sH9OPwJAMAwRFjLzNH7TdBWlq5ftkoXLX5YSx9/Wmcfv592GcXhTwAAhqOR7S4Amxr7+m30jiljddWSx/X0i6/omP0m6AP7T2x3WQAAoE3Ys5ahGftP1JrnXtbrtx2pfziBw58AAAxnhLUMHb3fBO0zfpTO+dD+Gvv6bdpdDgAAaCMOg2Zox+231k3/+93tLgMAAGSAPWsAAAAZI6wBAABkjLAGAACQMcIaAABAxghrAAAAGSOsAQAAZIywBgAAkDHCGgAAQMYIawAAABkjrAEAAGSMsAYAAJAxwhoAAEDGCGsAAAAZI6wBAABkjLAGAACQMUdEu2sYMLbXSnqs3XW02VhJT7S7iEywLgqsh41YFxuxLjZiXRRYDxu1al3sGRHjems0pMIaJNtLImJ6u+vIAeuiwHrYiHWxEetiI9ZFgfWwUW7rgsOgAAAAGSOsAQAAZIywNvRc2O4CMsK6KLAeNmJdbMS62Ih1UWA9bJTVuuCcNQAAgIyxZw0AACBjhDUAAICMEdYGIdu7215ke7nte2z/ryptDrP9jO2l6XZWO2ptBduP2r4r9XNJlfG2fZ7tFbbvtH1QO+psJtv7lD3XS20/a/uMijZDdpuwfYntNbbvLhu2k+1bbD+Y/o6pMe3Jqc2Dtk9uXdXNUWNd/Kvt+9L2f63tHWtMW/e1NNjUWBddtn9b9jo4tsa0R9u+P71vdLau6oFXYz1cWbYOHrW9tMa0Q22bqPr5mf37RURwG2Q3SRMlHZTuj5L0gKR9K9ocJun6dtfaovXxqKSxdcYfK+lGSZb0dkm/aHfNTV4fIyT9TsWPLQ6LbULSuyUdJOnusmH/Iqkz3e+UdE6V6XaS9HD6OybdH9Pu/jRhXRwlaWS6f061dZHG1X0tDbZbjXXRJelzvUw3QtJDkvaStLWkZZXvsYPpVm09VIyfK+msYbJNVP38zP39gj1rg1BErIqIO9L95yQtl7Rre6vK2vGSvhmF2yTtaHtiu4tqovdIeigihs3VPCLip5LWVQw+XtL8dH++pBOqTPo+SbdExLqIeErSLZKOblqhLVBtXUTEzRGxPj28TdJuLS+sDWpsF404WNKKiHg4Iv4g6bsqtqdBqd56sG1JH5Z0RUuLapM6n59Zv18Q1gY525MlvUXSL6qM/hPby2zfaPvNLS2stULSzbZvtz27yvhdJT1e9rhHQzvczlLtN97hsk1I0viIWCUVb9CSdqnSZrhtG5L0CRV7mqvp7bU0VJyeDglfUuNw13DaLt4laXVEPFhj/JDdJio+P7N+vyCsDWK2Xy/pe5LOiIhnK0bfoeIw2AGS/kPSD1pdXwsdEhEHSTpG0qdtv7tivKtMMyR/s8b21pKOk3R1ldHDaZto1LDZNiTJ9pmS1kv6do0mvb2WhoL/lDRF0oGSVqk4BFhpOG0XJ6n+XrUhuU308vlZc7Iqw1qyXRDWBinbr1OxoX07Ir5fOT4ino2I59P9GyS9zvbYFpfZEhGxMv1dI+laFYcwyvVI2r3s8W6SVramupY7RtIdEbG6csRw2iaS1aXD3envmipths22kU6GniHpo5FOwKnUwGtp0IuI1RHxakS8JukiVe/jsNgubI+U9KeSrqzVZihuEzU+P7N+vyCsDULpHINvSFoeEV+v0WZCaifbB6t4rp9sXZWtYXsH26NK91WcSH13RbPrJH08fSv07ZKeKe3uHoJq/pc8XLaJMtdJKn1b62RJP6zS5iZJR9kekw6HHZWGDSm2j5b0eUnHRcSLNdo08loa9CrOV/2gqvfxV5LeZPuNaW/1LBXb01BzpKT7IqKn2sihuE3U+fzM+/2i3d/M4Nb3m6R3qtj1eqekpel2rKQ5kuakNqdLukfFt5huk/SOdtfdpHWxV+rjstTfM9Pw8nVhSfNUfLvrLknT2113k9bF9irC1+iyYcNim1ARUFdJekXFf7+nSdpZ0kJJD6a/O6W20yVdXDbtJyStSLdT292XJq2LFSrOtSm9X1yQ2k6SdEO6X/W1NJhvNdbF5el94E4VH9ATK9dFenysim8KPjTY10W19ZCGX1Z6fyhrO9S3iVqfn1m/X3C5KQAAgIxxGBQAACBjhDUAAICMEdYAAAAyRlgDAADIGGENAAAgY4Q1AKjB9mTbg/p3pQAMfoQ1AACAjBHWAKABtvey/Wvbb213LQCGF8IaAPTC9j4qriV4akT8qt31ABheRra7AADI3DgV1wn8UETc0+5iAAw/7FkDgPqeUXFdzUPaXQiA4Yk9awBQ3x8knSDpJtvPR8R32l0QgOGFsAYAvYiIF2zPkHSL7Rci4oftrgnA8OGIaHcNAAAAqIFz1gAAADJGWAMAAMgYYQ0AACBjhDUAAICMEdYAAAAyRlgDAADIGGENAAAgY4Q1AACAjBHWAAAAMkZYAwAAyBhhDQAAIGOENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjhDUAAICMEdYAAAAyRlgDAADIGGENAAAgY4Q1AACAjBHWAAAAMkZYAwAAyBhhDQAAIGOENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjhDUAAICMEdaAYcL2Hraftz2iycuZbDtsj2zmcoYi21+wffEWTnuZ7X8c6Jp6WeYhth9M29UJrVx2Wv6nbK9Oy9+5l7Y/sf3JGuPYZpE1whpQh+1Hbf/B9tiK4UvTm/vk9Hg329+z/YTtZ2zfZfuUNK70QfB8xW1mK/sSEb+JiNdHxKv9mU+9D73hxHaX7W8N5Dwj4p8iYsDXbQpyYfvgsmF7246yxz+x/Xvbu5cNO9L2o3Vmfbak89N29YMBqLPL9isVr5O9arR9naSvSzoqLf/J/i4fyBVhDejdI5JOKj2w3SFpu4o2l0t6XNKeknaW9HFJqyva7Jg+VEq3K5tY87Az2PaKtKHedZJ62/P2gqQv9WGee0q6Z0uKqdP/KyteJw/XaDde0rZbunxgMCGsAb27XEX4KjlZ0jcr2rxV0mUR8UJErI+IX0fEjVuyMNun2l5u+znbD9v+y4rxf2d7le2Vtj+Z9pjsnca93/avbT9r+3HbXWXTbXKoJ+1J+QfbP0vLurm0B9H2tra/ZftJ20/b/pXt8ba/Kuldks5Pez3Ob6A/H0p7KPerMu4w2z3p8N8Tqd1Hy8Y30p/TbP9GUncafrXt36U9nD+1/eayaS6z/X9t35jq/5ntCbb/j+2nbN9n+y1l7SelPaZrbT9i+zNp+NGSviBpZprPsjR8tO1vpOfnt7b/0emws+1T0vLOtb1O0oa+lC1vw966sv6dbPs3af2c2dv6TtOOsr3I9nm2nQbPl7S/7UPrTHqepJNK21Mvy3hI0l6SfpTWwTZpfV1ne53tFbb/oqJv16Tt6llJpzTSlxrL/iNJ96eHT9suPffvSNvqM+nvO2pMP8L2v6V1+rCk91eMPyW99p5Lz/tHq80HaBXCGtC72yS9wfa09ME7U1Ll4a/bJM2zPcv2Hv1c3hpJMyS9QdKpks61fZC0ISR8VtKRkvaWVPnB+4KKYLmjig+gT7n+uUQfScvYRdLWkj6Xhp8sabSk3VXsKZwj6aWIOFPSYkmnp70ep9friO1TJZ0j6ciIuLtGswmSxkraNS33Qtv79KE/h0qaJul96fGNkt6U+nSHpG9XtP+wpC+mZb4s6dbUbqyka1QcWpPtrST9SNKyVNt7JJ1h+30R8WNJ/6SNe4EOSPOeL2m9iufmLZKOklR+WPNtkh5OtX21xvqo9E5J+6Tln2V7Wr3GLs7dWijpZxHxmYgoHep8MdVcb7m/lXSRqgTJShExRdJvJH0grYOXJV0hqUfSJEknSvon2+8pm+x4Fet4R23+vJR8IIW9e2x/qsayH5BUCuE7RsQRtneStEBF4NxZxfO4wNXPZfsLFa+xt0ianmqVJNneIc3jmIgYJekdkpbWXxtAcxHWgMaU9q69V9J9Kj7Uyv2ZihDzJUmPuDin7a0VbZ5Ie6lKt6ofuhGxICIeisJ/S7pZxd4sqQgal0bEPRHxoqSvVEz7k4i4KyJei4g7VXx41tuTcmlEPBARL0m6StKBafgrKj7w9o6IVyPi9oh4ts58qjlD0t9KOiwiVvTS9ksR8XLq74LUz0b705X2aL6UprkkIp5L4aFL0gG2R5e1vzb15/eSrpX0+4j4ZjqX70oVH+BSsbd0XEScHRF/SIfjLpI0q1oHbI+XdIykM1I9aySdW9F+ZUT8R9r7+lIv66TkKxHxUkQsUxEcD6jTdpKk/5Z0dUR8scr4/5K0h+1j6szjn1UEpjfXabMZF+e6vVPS5yPi9xGxVNLFkv68rNmtEfGD9HxW6/9VKoL3OBWB6izbJ1VpV837JT0YEZen9XuFitfqB6q0/bCk/xMRj0fEOhV9LveapP1sbxcRqyKCQ61oK8Ia0JjLVeyFOkWbHwJVRDwVEZ0R8WYV59IslfSDskNQkjQ2InYsuy2vtiDbx9i+Le1deFrSsSr2+kjFh/HjZc0fr5j2benw11rbz6jYI7bJlyMq/K7s/ouSXl/W35skfdfF4dZ/cXFCd1/8raR5EdHTS7unIuKFssePqehno/3ZsA7S4a2v2X4oHWp7NI0qn6b8XMKXqjwurYM9JU0qD9gqDn2Or9GPPSW9TtKqsvb/pWIv2ma19kGt56ia96s4n/KCaiNTgP2HdHONNmslna/iywN9MUnSuoh4rmzYYyr2SpbU7X9E3BsRK9M/CD+X9O8q2+vVwPIfqxhWufzyto9XtCvV8IKKvedzVDyXC2xPbbAGoCkIa0ADIuIxFV80OFbS93tp+4Skf1PxgbBTX5ZjextJ30vTj4+IHSXdoI0frKsk7VY2ye6bzkHfkXSdpN0jYrSKD+2qH8q99OGViPhKROyr4jDQDG08by9qT7mJoyR90faHemk3Jh16KtlD0sp0v5H+lNfzERWH2o5UcRh3chre53Wg4sP8kYqAPSoijq2y3FL7l7VpKH9DCvDVam2GiyT9WNINFeu03KUq1s0H68znXyUdLumP+7DslZJ2sj2qbNge2nQvdF/7H2r8uVupIjCXq1x+ySpt+trZ5NSFiLgpIt4raaKKvXMXNVgD0BSENaBxp0k6omIvkCTJ9jm297M9Mn1YfUrSii34OYGtJW0jaa2k9elw1VFl46+SdGo6f257SWdVTD9Kxd6N37v4mYaP9HH5pf4cbrsjnaP3rIrDoqWf/Fit4sTy3twj6WgV5/Id10vbr9je2va7VATDq9PwvvZnlIrA9KSk7VWco7WlfinpWduft71d2mu3X9nh7dWSJqdz2xQRq1Qcsp5r+w22t7I9pZcT+pvhdBUn319vu/Jby4qI9SoOD3++1gwi4mlJcyX9XaMLjYjHJf1c0j+7+ILK/ipeM7XOTduM7eNtj3HhYEmfkfTDBie/QdIf2f5Ieh3OlLSvpOurtL1K0mdc/OTOGEmdZTWMt31cCrsvS3peG7d9oC0Ia0CD0nlkS2qM3l7F+U9PqziBfE9JlQHlaW/6+1GfrbKM51R8QF0l6SkV4eS6svE3qjj5eZGkFSpOjpeKDxVJ+itJZ9t+TkWQu6rPHS1MUHEi+LOSlqs4D6r0pYp/l3Sii29PnldvJuk8qxmSLqpzntTvVPR1pYoP9jkRcd8W9uebKg5p/VbSvSq++LFF0jlsH1BxHt8jkp5QcQ5W6fy3UqB80vYd6f7HVQTue1OfrlGxd6Zl0hcKZqvY0/dD29tWaXaFir1L9fy7+h5STlKxN3OlitfDlyPilj5MP0vFdv2ciufynIiY38iE6R+jGZL+RkVY/ztJM9Ke7koXqTjMv0zFl0vK95ZvleaxUsXPnRyqYjsE2sYbvygEYLBJX1K4W9I2aY/JoGL7MEnfiojdemsLAMMVe9aAQcb2B9MhwzEqfhbjR4MxqAEAGkNYAwafv1RxTttDKg5TVf0tKgDA0MBhUAAAgIyxZw0AACBjg+rCx70ZO3ZsTJ48ud1lAAAA9Or2229/IiLG9dZuSIW1yZMna8mSWr+sAAAAkA/blVfdqIrDoAAAABkjrAEAAGSMsAYAAJAxwhoAAEDGCGsAAAAZI6wBAABkjLAGAACQMcIaAABAxghrAAAAGSOsAQAAZIywBgAAkDHCGgAAQMYIawAAABkjrAEAAGSMsAYAAJAxwhoAAEDGCGsAmmZy54J2lwAAgx5hDQAAIGOENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjI5s1Y9uXSJohaU1E7JeGXSlpn9RkR0lPR8SBVaZ9VNJzkl6VtD4ipjerTgAAgJw1LaxJukzS+ZK+WRoQETNL923PlfRMnekPj4gnmlYdAADAINC0sBYRP7U9udo425b0YUlHNGv5AAAAQ0G7zll7l6TVEfFgjfEh6Wbbt9ue3cK6AAAAstLMw6D1nCTpijrjD4mIlbZ3kXSL7fsi4qfVGqYwN1uS9thjj4GvFAAAoI1avmfN9khJfyrpylptImJl+rtG0rWSDq7T9sKImB4R08eNGzfQ5QIAALRVOw6DHinpvojoqTbS9g62R5XuSzpK0t0trA8AACAbTQtrtq+QdKukfWz32D4tjZqlikOgtifZviE9HC/pf2wvk/RLSQsi4sfNqhMAACBnzfw26Ek1hp9SZdhKScem+w9LOqBZdQEAAAwmXMEAAAAgY4Q1AACAjBHWAAAAMkZYAwAAyBhhDUBNc2fO6P9Mukb3fx4AMIwR1gAAADJGWAMAAMgYYQ0AACBjhDUAAICMEdYAAAAyRlgDAADIGGENAAAgY4Q1AACAjBHWAAAAMkZYAwAAyBhhDQAAIGOENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjhDUAAICMEdYAAAAyRlgDAADIGGENAAAgY4Q1AACAjBHWAAAAMkZYAwAAyBhhDQAAIGOENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjhDUAAICMEdYAAAAyRlgDAADIGGENAAAgY4Q1AACAjBHWAAAAMkZYAwAAyBhhDQAAIGNNC2u2L7G9xvbdZcO6bP/W9tJ0O7bGtEfbvt/2CtudzaoRAAAgd83cs3aZpKOrDD83Ig5MtxsqR9oeIWmepGMk7SvpJNv7NrFOAACAbDUtrEXETyWt24JJD5a0IiIejog/SPqupOMHtDgAAIBBoh3nrJ1u+850mHRMlfG7Snq87HFPGgYAADDstDqs/aekKZIOlLRK0twqbVxlWNSaoe3ZtpfYXrJ27dqBqRJAU8yb073h/sLuKZqwaKnUNbqNFQFA/loa1iJidUS8GhGvSbpIxSHPSj2Sdi97vJuklXXmeWFETI+I6ePGjRvYggEAANqspWHN9sSyhx+UdHeVZr+S9Cbbb7S9taRZkq5rRX0AAAC5GdmsGdu+QtJhksba7pH0ZUmH2T5QxWHNRyX9ZWo7SdLFEXFsRKy3fbqkmySNkHRJRNzTrDoBAABy1rSwFhEnVRn8jRptV0o6tuzxDZI2+1kPAACA4YYrGAAAAGSMsAYAAJAxwhoAAEDGCGsAAAAZI6wBAABkjLAGAACQMcIaAABAxghrAAAAGSOsAQAAZIywBgAAkDHCGgAAQMYIawAAABkjrAEAAGSMsAYAAJAxwhowwBZ2T2l3Cf3TNbrdFQAAyhDWAAAAMkZYAwAAyBhhDQAAIGOENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjhDUAAICMEdYAAAAyRlgDAADIGGENAAAgY4Q1AACAjBHWAAAAMkZYAwAAyBhhDQAAIGOENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjhDVgkOuY39HU+fd0Lu73PJpdIwAMZYQ1AACAjBHWAAAAMkZYAwAAyBhhDQAAIGOENQAAgIw1LazZvsT2Gtt3lw37V9v32b7T9rW2d6wx7aO277K91PaSZtUIAACQu2buWbtM0tEVw26RtF9E7C/pAUl/X2f6wyPiwIiY3qT6AAAAste0sBYRP5W0rmLYzRGxPj28TdJuzVo+AADAUNDOc9Y+IenGGuNC0s22b7c9u4U1AQAAZGVkOxZq+0xJ6yV9u0aTQyJipe1dJN1i+760p67avGZLmi1Je+yxR1PqBQAAaJeW71mzfbKkGZI+GhFRrU1ErEx/10i6VtLBteYXERdGxPSImD5u3LhmlAwAANA2LQ1rto+W9HlJx0XEizXa7GB7VOm+pKMk3V2tLQAAwFDXzJ/uuELSrZL2sd1j+zRJ50sapeLQ5lLbF6S2k2zfkCYdL+l/bC+T9EtJCyLix82qEwAAIGdNO2ctIk6qMvgbNdqulHRsuv+wpAOaVRcAAMBgwhUMAAAAMkZYAwAAyBhhDQAAIGOENQAAgIwR1oA2mDenW3Nnzhjw+fZ0Lh6Q+SyfOk3z5nQPyLzK51lLx/yOhuczuXPBJo+bsR4BICeENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjhDUAAICMEdYAAAAyRlgDAADIGGENAAAgY4Q1AACAjBHWAAAAMkZYAwAAyBhhDQAAIGOENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjhDUAAICMEdYAAAAyRlgDAADIGGENaLOurq5+z2P51GmbDZuwaGmf5zO5c0HV4QNRYyPzmzenW3NnzmhgBqO1fOo0zZvTPaB1AUCOCGsAAAAZqxvWbI+w/a1WFQMAAIBN1Q1rEfGqpHG2t25RPQAAACgzsoE2j0r6me3rJL1QGhgRX29WUQAAACg0EtZWpttWkkY1txwAAACU6zWsRcRXJMn2qOJhPN/0qgAAACCpgW+D2t7P9q8l3S3pHtu3235z80sDAABAIz/dcaGkz0bEnhGxp6S/kXRRc8sCAACA1FhY2yEiFpUeRMRPJO3QtIoAAACwQSNfMHjY9pckXZ4ef0zSI80rCQAAACWN7Fn7hKRxkr6fbmMlndrMogAAAFCou2fN9ghJX4iIz7SoHgAAAJRp5AoGf9yiWgAAAFChkXPWfp2uXnC1Nr2CwfebVhUAAAAkNRbWdpL0pKQjyoaFivPXAAAA0ESNnLN2Z0ScuyUzt32JpBmS1kTEfmnYTpKulDRZxXVHPxwRT1WZ9mRJX0wP/zEi5m9JDQAAAINZI+esHdeP+V8m6eiKYZ2SFkbEmyQtTI83kQLdlyW9TdLBkr5se0w/6gAAABiUGvnpjp/bPt/2u2wfVLo1MvOI+KmkdRWDj5dU2ks2X9IJVSZ9n6RbImJd2ut2izYPfQAAAENeI+esvSP9PbtsWGjTc9j6YnxErJKkiFhle5cqbXaV9HjZ4540DAAAYFjpdc9aRBxe5balQa1RrlZK1Yb2bNtLbC9Zu3Ztk8vCYDW5c4EkqWN+x4DMb8KipZs8njene0Dm22ddo6sOXtg9pSmLK63HZixz+dRpWzxtT+fifi0bAHLWa1izPd72N2zfmB7va/u0fixzte2JaV4TJa2p0qZH0u5lj3eTtLLazCLiwoiYHhHTx40b14+yAAAA8tPIOWuXSbpJ0qT0+AFJZ/RjmddJOjndP1nSD6u0uUnSUbbHpC8WHJWGAQAADCuNhLWxEXGVpNckKSLWS3q1kZnbvkLSrZL2sd2T9sh9TdJ7bT8o6b3psWxPt31xWsY6Sf8g6VfpdnYaBgAAMKw08gWDF2zvrHTOmO23S3qmkZlHxEk1Rr2nStslkj5Z9vgSSZc0shwAAIChqpGw9lkVhy6n2P6ZpHGSTmxqVQAAAJDUQFiLiDtsHyppHxXf0rw/Il5pemUAAABoaM9a6Ty1e5pcCwAAACo08gUDAAAAtEnNsGb7kPR3m9aVAwAAgHL19qydl/7e2opCAAAAsLl656y9YvtSSbvaPq9yZER8pnllAQAAQKof1mZIOlLFBdtvb005AAAAKFczrEXEE5K+a3t5RCxrYU0AAABIGvk26JO2r7W9xvZq29+zvVvTKwMAAEBDYe1SFVcwmCRpV0k/SsMAAADQZI2EtV0i4tKIWJ9ul6m45BQAAACarJGwttb2x2yPSLePSXqy2YUBAACgsbD2CUkflvQ7SatUXMT9E80sCgAAAIVew1pE/CYijouIcRGxS0ScEBGPtaI4oFnmzpzR1PlPWLS0qfOvZt6cbklSV1dXy5fdrGW2Yz02oqdzcbtLADCMcG1QAACAjBHWAAAAMkZYAwAAyFivYc32eNvfsH1jeryv7dOaXxoAAAAa2bN2maSbVPworiQ9IOmMZhUEAACAjRoJa2Mj4ipJr0lSRKyX9GpTqwIAAICkxsLaC7Z3lhSSZPvtkp5palUAAACQJI1soM3fqLg26BTbP1Nxqak/a2pVAAAAkNRAWIuI220fKmkfSZZ0f0S80vTKAAAA0NC3QR+S9MmIuCci7o6IV2xf34LaAAAAhr1Gzll7RdLhti+1vXUatmsTawIAAEDSSFh7MSJmSlouabHtPZW+bAAAAIDmauQLBpakiPgX27er+M21nZpaFQAAACQ1FtbOKt2JiIW23yfp5OaVBAAAgJKaYc321Ii4T9JvbR9UMZovGAAAALRAvT1rn5U0W9LcKuNC0hFNqQgAAAAb1AxrETE7/T28deUAAACgXM1vg9p+q+0JZY8/bvuHts+zzRcMAAAAWqDeT3f8l6Q/SJLtd0v6mqRvqrgu6IXNLw0AAAD1wtqIiFiX7s+UdGFEfC8iviRp7+aXBgywrtGbPOzpXLxFs1nYPaXmuC2dZzVzZ84YsHlJ0rw53QM2r475HZo3p3uLamxkUI3sSgAAFFpJREFUHXV1dW1BVdUN9HoEgFarG9Zsl85pe4+k8nf6Rn7yAwAAAP1UL3RdIem/bT8h6SVJiyXJ9t4qDoUCAACgyep9G/SrthdKmijp5ogoXWJqK0l/3YriAAAAhru6hzMj4rYqwx5oXjkAAAAo18iF3AEAANAmhDUAAICMtTys2d7H9tKy27O2z6hoc5jtZ8ranFVrfgAAAENZy3+CIyLul3SgJNkeIem3kq6t0nRxRPADSQAAYFhr92HQ90h6KCIea3MdAAAAWWp3WJul4vfcqvkT28ts32j7zbVmYHu27SW2l6xdu7Y5VQIAALRJ28Ka7a0lHSfp6iqj75C0Z0QcIOk/JP2g1nwi4sKImB4R08eNG9ecYgEAANqknXvWjpF0R0SsrhwREc9GxPPp/g2SXmd7bKsLBAAAaLd2hrWTVOMQqO0Jtp3uH6yizidbWBsAAEAW2nJBdtvbS3qvpL8sGzZHkiLiAkknSvqU7fUqrks6q+xyVwAAAMNGW8JaRLwoaeeKYReU3T9f0vmtrgsAACA37f42KAAAAOogrAEAAGSMsAYAAJAxwhoAAEDGCGsYdpZPndbW5XfM76g6fHLnAqlrtDrmd2yosadzsbq6urSwe0q/ljkQfZ6waGlxp2v0JsN7Ohf3e97V9LfPHfM7NG9O94bHXV1d/ayoug3rpYkmdy5o+jIA5IuwBgAAkDHCGgAAQMYIawAAABkjrAEAAGSMsAYAAJAxwhoAAEDGCGsAAAAZI6wBAABkjLAGAACQMcIaAABAxghrAAAAGSOsAQAAZIywBgAAkDHCGgAAQMYIawAAABkjrAEAAGSMsAYAAJAxwhoAAEDGCGuApAmLltYct3zqtJrjOuZ31B3fm57OxVs8bSMmdy5Qx/yOpi4jFwu7p2z5xF2ja46asGhp3fHt1q9+AxgUCGsAAAAZI6wBAABkjLAGAACQMcIaAABAxghrAAAAGSOsAQAAZIywBgAAkDHCGgAAQMYIawAAABkjrAEAAGSMsAYAAJAxwhoAAEDGCGsAAAAZI6wBAABkjLAGAACQMcIaAABAxtoW1mw/avsu20ttL6ky3rbPs73C9p22D2pHnQAAAO00ss3LPzwinqgx7hhJb0q3t0n6z/QXAABg2Mj5MOjxkr4Zhdsk7Wh7YruLAgAAaKV2hrWQdLPt223PrjJ+V0mPlz3uScM2YXu27SW2l6xdu7ZJpaI/JixausXTLp86rV/L7urq6tf4vpg7c8aAzatZ5s6csaHP/XlehpLJnQs2eTx35gz1dC7ebNvomN+xyfbY1dWlhd1TBmSZW6Knc3G/51FPeY3z5nQ3PF3H/I5mlAMMa+0Ma4dExEEqDnd+2va7K8a7yjSx2YCICyNiekRMHzduXDPqBAAAaJu2hbWIWJn+rpF0raSDK5r0SNq97PFukla2pjoAAIA8tCWs2d7B9qjSfUlHSbq7otl1kj6evhX6dknPRMSqFpcKAADQVu36Nuh4SdfaLtXwnYj4se05khQRF0i6QdKxklZIelHSqW2qFQAAoG3aEtYi4mFJB1QZfkHZ/ZD06VbWBQAAkJucf7oDAABg2COsAQAAZIywBgAAkDHCGgAAQMYIawAAABkjrAEAAGSMsAYAAJAxwhoAAEDGCGsAAAAZI6wBAABkjLAGAACQMcIaAABAxghrAAAAGSOsoS0mdy7oU/u5M2eop3Nxk6rp3bw53ZsN62sf6rWfsGhpn2vqj4XdU1q6vFbZsB67Rtcf32STOxeoY36Hlk+dtsnwhd1TtryGGn0qH98xv2OLZl3avru6uiRVX0+l1+BQ3XaAnBHWAAAAMkZYAwAAyBhhDQAAIGOENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjhDUAAICMEdYAAAAyRlgDAADIGGENAAAgY4Q1AACAjBHWAAAAMkZYAwAAyBhhDQAAIGOENQAAgIwR1gAAADJGWAMAAMgYYQ0AACBjhDW0Tcf8jj5Ps7B7SsNtezoXD/g8u7q6Gm5bfQaj+zc9ejW5c8FmwxrdFpZPnbbJ42rbxrw53VtWWI3lzZvTrbkzZ0hqfPsqta81z0ZUrqda86w2vwmLlja0jGo65nc0XGN/NFJjvfUI5ISwBgAAkDHCGgAAQMYIawAAABkjrAEAAGSMsAYAAJAxwhoAAEDGWh7WbO9ue5Ht5bbvsf2/qrQ5zPYztpem21mtrhMAACAHI9uwzPWS/iYi7rA9StLttm+JiHsr2i2OCH4EBwAADGst37MWEasi4o50/zlJyyXt2uo6AAAABoO2nrNme7Kkt0j6RZXRf2J7me0bbb+5zjxm215ie8natWubVCkAAEB7tC2s2X69pO9JOiMinq0YfYekPSPiAEn/IekHteYTERdGxPSImD5u3LjmFQwAANAGbQlrtl+nIqh9OyK+Xzk+Ip6NiOfT/Rskvc722BaXCQAA0Hbt+DaoJX1D0vKI+HqNNhNSO9k+WEWdT7auSgAAgDy049ugh0j6c0l32V6ahn1B0h6SFBEXSDpR0qdsr5f0kqRZERFtqBUAAKCtWh7WIuJ/JLmXNudLOr81FQEAAOSLKxgAAABkjLAGAACQMcIaAABAxghrAAAAGSOsNdG8Od19nmbuzBnq6VxcddzyqdO2uJaezsVa2D1FExYtrdmmq6ur9gy6RtdvXza+vN8Lu6f0pcyqNVaux5p96BqtjvkdVee5RSr6PHcml6odyqptO01VsX2VXvf1XqOVNdZ6zTb8mtHm7yubvF6qvO631OTOBZvMr/R66urq6vW9qdp734RFS4t5JvPmdG94/+zLa76nc3H9974+qvX+XU29Pg9E+4YN4PNcruWvqSGMsAYAAJAxwhoAAEDGCGsAAAAZI6wBAABkjLAGAACQMcIaAABAxghrAAAAGSOsAQAAZIywBgAAkDHCGgAAQMYIawAAABkjrAEAAGSMsAYAAJAxwhoAAEDGCGsAAAAZI6wBAABkjLAGAACQMcIaAABAxghrTTZ35gz1dC5WV1dXn6dd2D1FExYtlbpGq2N+R9U2ExYt3WzY5M4FG+7Pm9OtuTNnbD6+a/RmNfamY36Hlk+dVnN55eMb7fPyqdMaqrE35X2WtNn8tlSt9Q40qrR9N/Iaa0S912C50mtwYfeUmvMayO27/DU7b073JuNq1dCXdVKaZ+X7ypb0oWN+x2Y1bpD6sHzqtMbevxt4n6pWY73npbT8TRbTy/tpb891SXm/G2nfX+Xb/5Z8DlYqfWaUtp0Nn5MDpKura0DnN1AIawAAABkjrAEAAGSMsAYAAJAxwhoAAEDGCGsAAAAZI6wBAABkjLAGAACQMcIaAABAxghrAAAAGSOsAQAAZIywBgAAkDHCGgAAQMYIawAAABkjrAEAAGSMsAYAAJAxwhoAAEDG2hLWbB9t+37bK2x3Vhm/je0r0/hf2J7c+ioBAADar+VhzfYISfMkHSNpX0kn2d63otlpkp6KiL0lnSvpnNZWCQAAkId27Fk7WNKKiHg4Iv4g6buSjq9oc7yk+en+NZLeY9strBEAACALjojWLtA+UdLREfHJ9PjPJb0tIk4va3N3atOTHj+U2jxRZX6zJc1OD/eRdH+TuzAQxkrarC/DyHDu/3DuuzS8+0/fh6/h3P/h3Hep9/7vGRHjepvJyIGrp2HV9pBVJsZG2hQDIy6UdGF/i2ol20siYnq762iX4dz/4dx3aXj3n74Pz75Lw7v/w7nv0sD1vx2HQXsk7V72eDdJK2u1sT1S0mhJ61pSHQAAQEbaEdZ+JelNtt9oe2tJsyRdV9HmOkknp/snSuqOVh+vBQAAyEDLD4NGxHrbp0u6SdIISZdExD22z5a0JCKuk/QNSZfbXqFij9qsVtfZZIPqsG0TDOf+D+e+S8O7//R9+BrO/R/OfZcGqP8t/4IBAAAAGscVDAAAADJGWAMAAMgYYa1JbO9k+xbbD6a/Y6q0Odz20rLb722fkMZdZvuRsnEHtr4XW66R/qd2r5b18bqy4W9Mlxp7MF16bOvWVd8/DT73B9q+1fY9tu+0PbNs3KB77vtzCTnbf5+G32/7fa2seyA00PfP2r43Pc8Lbe9ZNq7q9j+YNND/U2yvLevnJ8vGnZxeJw/aPrly2tw10Pdzy/r9gO2ny8YN6ufe9iW216TfRa023rbPS+vmTtsHlY0b7M97b33/aOrznbZ/bvuAsnGP2r4rPe9LGl5oRHBrwk3Sv0jqTPc7JZ3TS/udVHyZYvv0+DJJJ7a7H83uv6Tnawy/StKsdP8CSZ9qd58Gsu+S/kjSm9L9SZJWSdpxMD73Kr4o9JCkvSRtLWmZpH0r2vyVpAvS/VmSrkz3903tt5H0xjSfEe3u0wD3/fCy1/WnSn1Pj6tu/4Pl1mD/T5F0fpVpd5L0cPo7Jt0f0+4+DWTfK9r/tYov1A2V5/7dkg6SdHeN8cdKulHF76a+XdIvhsLz3mDf31Hqk4pLa/6ibNyjksb2dZnsWWue8ktmzZd0Qi/tT5R0Y0S82NSqWqev/d/AtiUdoeJSY32ePgO99j0iHoiIB9P9lZLWSOr1V6wz1Z9LyB0v6bsR8XJEPCJpRZrfYNFr3yNiUdnr+jYVvy05VDTy3NfyPkm3RMS6iHhK0i2Sjm5Snc3Q176fJOmKllTWAhHxU9X//dPjJX0zCrdJ2tH2RA3+573XvkfEz1PfpAF6zRPWmmd8RKySpPR3l17az9LmL+Svpt2o59rephlFNlGj/d/W9hLbt5UOAUvaWdLTEbE+Pe6RtGtzyx1QfXrubR+s4j/zh8oGD6bnfldJj5c9rvZ8bWiTntdnVDzPjUybs77Wf5qKvQ0l1bb/waTR/n8obc/X2C79KPqwee7Toe83SuouGzzYn/ve1Fo/g/1576vK13xIutn27S4ul9mQdlxuasiw/f8kTagy6sw+zmeipA4Vvz1X8veSfqfiQ/xCSZ+XdPaWVdocA9T/PSJipe29JHXbvkvSs1XaZfUbMwP83F8u6eSIeC0Nzv65r9CfS8g1fGm5TDVcv+2PSZou6dCywZtt/xHxULXpM9VI/38k6YqIeNn2HBV7WI9ocNqc9aX+WZKuiYhXy4YN9ue+N0P1Nd8w24erCGvvLBt8SHred5F0i+370p66ughr/RARR9YaZ3u17YkRsSp9IK+pM6sPS7o2Il4pm/eqdPdl25dK+tyAFD2ABqL/6RCgIuJh2z+R9BZJ31Oxy3xk2gtT7ZJkbTUQfbf9BkkLJH0xHSYozTv7575CXy4h1+NNLyHXyLQ5a6h+20eqCPKHRsTLpeE1tv/B9IHda/8j4smyhxdJOqds2sMqpv3JgFfYPH3ZdmdJ+nT5gCHw3Pem1voZ7M97Q2zvL+liSceUvwbKnvc1tq9VcTi917DGYdDmKb9k1smSflin7WbnMqQP+dL5WydIqvqtk4z12n/bY0qH+GyPlXSIpHujOAtzkYrz+GpOn7FG+r61pGtVnNNxdcW4wfbc9+cSctdJmuXi26JvlPQmSb9sUd0Dode+236LpP+SdFxErCkbXnX7b1nlA6OR/k8se3icpOXp/k2SjkrrYYyko7Tp0YXcNbLdy/Y+Kk6kv7Vs2FB47ntznaSPp2+Fvl3SM+kf0cH+vPfK9h6Svi/pzyPigbLhO9geVbqvou+Nvb+3+lsUw+Wm4nychZIeTH93SsOnS7q4rN1kSb+VtFXF9N2S7kpP5Lckvb7dfRro/qv4xsxdKr5FdZek08qm30vFh/YKSVdL2qbdfRrgvn9M0iuSlpbdDhysz72Kb349oGLPwJlp2NkqAookbZuexxXped2rbNoz03T3q/gvtO39GeC+/z9Jq8ue5+vS8Jrb/2C6NdD/f5Z0T+rnIklTy6b9RNomVkg6td19Gei+p8ddkr5WMd2gf+5V7GBYld7HelQc7psjaU4ab0nz0rq5S9L0IfS899b3iyU9VfaaX5KG75We82XpNXFmo8vkclMAAAAZ4zAoAABAxghrAAAAGSOsAQAAZIywBgAAkDHCGgAAQMYIawAGFduv2l5q+x7by2x/1vZWadx02+fVmXay7Y/UGT/J9jXp/im2z+9jbafYnlT2+GLb+/ZlHgBQiSsYABhsXoqIAyUpXbLlOyquiPDliFgiaUmdaSdL+kiaZhPpihkrtfHHmLfEKSp+H6/0K+Wf7Me8AEASe9YADGJRXBFgtqTT0y+lH2b7ekmyfWjaA7fU9q/TL4d/TdK70rD/nfaEXW37RyourjzZdvkviu9u+8e277f95TTfTdrY/pztLtsnqvjh42+n+W9n+ye2p6d2J9m+y/bdts8pm/55219Newlvsz2+2esNwOBCWAMwqEXEwyrey3apGPU5SZ9Oe+HeJeklSZ2SFkfEgRFxbmr3J5JOjogjqsz+YEkflXSgpD8rBa8adVyjYq/eR9P8XyqNS4dGz1FxAfMDJb3V9glp9A6SbouIA1RcI/AvGu89gOGAsAZgKHCVYT+T9HXbn5G0Y0SsrzHtLRGxrs64J1Pw+r6kd25hfW+V9JOIWJvq+Lakd6dxf5B0fbp/u4pDtQCwAWENwKBmey9Jr0paUz48Ir4m6ZOStpN0m+2pNWbxQp3ZV16PLySt16bvnds2Umadca/Exuv+vSrOJQZQgbAGYNCyPU7SBZLOj4oLHdueEhF3RcQ5Kg5PTpX0nKRRfVjEe23vZHs7SSeo2Fu3WtIutne2vY2kGWXta83/F5IOtT3W9ghJJ0n67z7UAWAY4z84AIPNdraXSnqdir1cl0v6epV2Z9g+XMXeqnsl3SjpNUnrbS+TdJmkp3pZ1v+k+e8t6Tvp26ayfbaKAPaIpPvK2l8m6QLbL6k4F06SFBGrbP+9pEUq9rLdEBE/7EOfAQxjrvhnFAAAABnhMCgAAEDGCGsAAAAZI6wBAABkjLAGAACQMcIaAABAxghrAAAAGSOsAQAAZOz/A+wfbYJdcp/nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse_error_kNN(X_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two graphs above, there are several observations we can make with regards to the MSE and the distribution of the errors. Firstly, looking at the top graph, we see that the MSE reaches a minimum when k = 2, and from there onwards, the MSE increases as k increases. The optimal k for kNN is therefore 2. \n",
    "Now if we look at the bottom graph, it appears that the errors are normally distributed about 0 (with mean 0) and with a standard deviation of approximately 1. As we increase the value of k, I don't seem to see any pattern with the distribution of the errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2\n",
    "Lastly, I obtain the in-sample MSE by retraining the model using the optimal penalty parameter on the\n",
    "entire training set, and compute its out-of-sample MSE (on the test set regression_test.csv).\n",
    "I firstly obtain the optimal k parameter over all folds. I use the same technique as I did in ridge regression, first finding the average MSE over all folds, find the minimum and then take the corresponding k term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal k is 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd4VGXawOHfM+m990ZCCSC92RBREXtXRCyr++1adi1rW1nLrmJXbFh2Bde1LoqKig0VVwREBBKaQGjpvfdkMuX9/jgDhhhIAplMyntfVyAzpz1nMjPPOW8VpRSapmmadjgmVwegaZqm9X46WWiapmkd0slC0zRN65BOFpqmaVqHdLLQNE3TOqSThaZpmtYhnSy0LhGRRBGpFxE3Jx9nkIgoEXF35nH6IxG5V0ReO8Jt3xCRR7o7pg6OeaKI7HG8ry7syWM7jn+TiJQ4jh/WwborReQPh1jWr9+zOln0ABHJFpEWEQlv8/xmx5trkONxvIh8JCLlIlIjIttE5FrHsv1vxPo2P5f35LkopXKVUv5KKdvR7OdwH7qBREQeFJF3unOfSqnHlFLd/to6EokSkSmtnhsiIqrV45Ui0iwiCa2emyEi2YfZ9TzgJcf76pNuiPNBEbG0+ZykHGJdD+BZYKbj+BVHe/z+SieLnpMFXLH/gYiMBnzarPM2kAckAWHANUBJm3WCHW/q/T/vOzHmAaevXRW6IN5KoKM7jwbggS7sMwnYfiTBHOb832/zOck8xHpRgPeRHn8g0cmi57yN8eW/3++At9qsMxl4QynVoJSyKqU2KaW+OpKDich1IrJTROpEJFNEbmiz/K8iUiQihSLyB8cV4xDHsnNEZJOI1IpInog82Gq7g261HVeSD4vIj45jfbP/DkpEvEXkHRGpEJFqEdkgIlEi8ihwEvCS46rvpU6czyWOO7RR7SybLiL5juKXcsd6V7Za3pnz+T8RyQX+53j+AxEpdtzhrRKRY1pt84aIvCIiXzni/1FEokXkeRGpEpEMERnfav1Yxx1jmYhkicitjufPBO4FLnfsZ4vj+SAR+bfj71MgIo+Io9hPRK51HO85EakEDpxLq+MduFtpdX6/E5Fcx+tzX0evt2PbABH5XkQWiIg4nn4TGCMiJx9m0wXAFfvfTx0cYx+QAnzmeA28HK/XMhGpFJG9IvLHNuf2oeN9VQtc25lzOcSxhwG7HA+rRWT/3/4Ex3u1xvH/CYfY3k1E5jte00zgnDbLr3V89uocf/cr29tPn6GU0j9O/gGygRkYb8wRgBu/3kEoYJBjvRXAj8BsILHNPgY51nXv5DHPAQYDApwMNAITHMvOBIqBYwBfjESmgCGO5dOB0RgXE2Mw7m4ubC8OYCWwDxiGcae0EnjCsewG4DPHMdyAiUBgq+3+cJj4DxwHuA7Yuz++dtadDlgxihO8HOfbAKR24XzeAvwAH8fzvwcCHPt7Htjc6nhvAOWO8/HGSDBZGBcDbhhX3t871jUBacDfAU+ML8ZM4AzH8geBd9qczyfAq454IoH1wA2OZdc6zvUWx2vj087rcWCfrc5vkePvMxYwAyMO8Vq+4Yg/zHHcR9pZdiuwxvHcEEC1Wmcl8AfH32J/DDOA7I4+H60e/wC84nhtxwFlwGmtzs0CXOh4bQ91/jUYd0HbgZs68z5zPA4FqoCrHa/vFY7HYW3ft8CNQAaQ4Njue359z/oBtfz6HowBjnH1d9HR/Og7i561/+7idIw3WUGb5ZcBqzFu4bPEqNOY3GadcsdV+v6fEe0dSCn1hVJqnzL8AHyDcTUPMAv4j1Jqu1KqEXiozbYrlVLblFJ2pdRWYDHGF/Ch/EcptVsp1QQswfiAg/GhDsP4krcppdKUUrWH2U97/gLcDUxXSu3tYN0HlFJmx/l+4TjPzp7Pg8q4o2tybPO6UqpOKWXG+PIZKyJBrdb/2HE+zcDHQLNS6i1l1OW8D+y/s5gMRCil5imlWpRRHLII44LgN0QkCjgL+IsjnlLguTbrFyqlXlTG3WdTB6/Jfg8ppZqUUluALRhJ41BiMb6wP1BK3d/O8leBRBE56zD7eBw4r/UdWWeIUdcxFbhHKdWslNoMvIbx5b3fT0qpTxx/z/bOfwnGRVkE8Efg7yJyRTvrteccYI9S6m3H67sY47N6XjvrzgKeV0rlKaUqMc65NTswSkR8lFJFSqk+XdSlk0XPehuYg3F12LYICqVUlVJqrlLqGIyy1M3AJ62KAADClVLBrX52tncgETlLRNY5buWrgbOB/RXssRh3Nvvltdn2WEfxQ5mI1GBcQR1UOd9GcavfGwH/Vuf7NfCeGMVdT4lRodgVdwMvK6XyO1ivSinV0OpxDsZ5dvZ8DrwGjuKFJ0Rkn6OoI9uxqPU2reuSmtp5vP81SAJiWyd4jKKnqEOcRxLgARS1Wv9VjDuM38TaBYf6G7XnHIy7kH+1t9CRQB92/Mgh1ikDXsKovO6KWKBSKVXX6rkcIK7V48Oev1Jqh1Kq0HGBshZ4Abi0C8fPafNc2+O3XjevzXr7Y2gALsd4rxWJyBciMryTMfRKOln0IKVUDkZxxdnA0g7WLQfmY7whQ7tyHBHxAj5ybB+llAoGvuTXD3YREN9qk4SD98B/gWVAglIqCONLo90vhQ7OwaKUekgpNRI4ATiXX+ttOjvc8UzgfhG5pIP1QkTEr9XjRKDQ8Xtnzqd1PHOACzCKT4IwiipoZ5vOyAOy2iT4AKXU2e0cd//6Zg6+KAh0XEC0F6szLAKWA1+2eU1b+w/Ga3PRYfbzNHAKRnFdZxUCoSIS0Oq5RA6+C+/q+Ss6/7crxEjYrbU9/n5FHPzZSTzooEp9rZQ6HaMIKgPjde2zdLLoef8HnNrmKhgAEXlSREaJiLvjw3ITsFd1vTmfJ0ZZexlgdRQXzGy1fAlwnYiMEBFfjPL01gIwru6axWgmOaeLx99/PqeIyGhH5WwtRrHU/ia3JRjl9x3ZjlHH8rKInN/Bug+JiKeInISRmD5wPN/V8wnA+MKuwKhveawTcR7KeqBWRO4RER/HXcuoVsWLJcAgETEBKKWKMIoMnxGRQBExicjgDiqUneFmjDq2z0Wkbas9lFJWjOK5ew61A6VUNfAM8NfOHlQplQesBR4Xo4HEGIzPzLud3YeIXCAiIWKYglHH8mknN/8SGCYicxyfw8uBkcDn7ay7BLhVjCbvIcDcVjFEicj5jmRrBur59b3fJ+lk0cMc9QgbD7HYF6P8uxqjEjQJaPsFWS0Htx+/o51j1GF8QJZgVM7Nwbiy3r/8K4wWK99jVBz/5Fhkdvz/J2CeiNRhJJIlXT5RQzTwIUai2IlRDr6/T8ELwKVitB5acLidOMrZzwUWHaacvBjjXAsxvlhuVEplHOH5vIVRpFAA7ADWdbD+4WK3YZR3j8O4qyzHKIPfX/+xP6FViEi64/drMBL+Dsc5fYhxddpjlFIKuB7jTudTEfFuZ7XFGFfXh/MCXf+SvALjbq4Q4/PwD6XUt13YfjbG+7oO42/5pFLqzc5s6LgwOxe4E+Ni4a/AuY47/bYWYRSzbgHSObi0wOTYRyFGRfvJGO/DPkuM94Q2kDkqyX8BvBxXjH2KiEzHaHkT39G6mqYdGX1nMUCJyEWOIpsQ4Engs76YKDRN6xk6WQxcN2DUaezDKCa4ybXhaJrWmzk1WYjImSKyS4xemHPbWT5NRNJFxCoil7ZZ9pSIbBejF3LrHqRaN1BKnamUClJKhSqlLnJUrPZJjn4UughK05zIacnC0QLmZYwORiMxuv+PbLNaLkafg/+22fYE4ESM3rajMDo29XRrEE3TNM3BmYOQTcFo9pkJICLvYbRd37F/BaVUtmOZvc22CqOrvydG+2gPfjug3kHCw8PVoEGDuil0TdO0gSEtLa1cKRXR0XrOTBZxHNy7MR84tjMbKqV+EpHvMZrlCcbwxb/pqSwi12M07yMxMZGNGw/VIlXTNE1rj4i07bHeLmfWWbRXx9CpdrpijFY5AqOXcRxwqohM+83OlFqolJqklJoUEdFhYtQ0TdOOkDOTRT4Hd4WP59fhFzpyEbBOKVWvlKoHvgKO6+b4NE3TtE5yZrLYAAwVkWQR8cToVbmsg232ywVOdnS398Co3G53wDxN0zTN+ZyWLBwdvG7G6A6/E1iilNouIvP2j/EjIpNFJB9jaO5XRWT/EL4fYrT/34bRlX6LUuozZ8WqaZqmHV6/Ge5j0qRJSldwa5qmdY2IpCmlJnW0nu7BrWmapnVIJwtN0zStQ87sZ6FpmqY5ia22FvOuXTRn7EI8PQm5fJZTj6eThaZpWi+m7HYsubk0Z+yieVcGZsf/1sJfh3PzGTtWJwtN07SBwlZfj3n3bpozfk0K5t17UE1NxgomE57JyfiOG4/X7CvwHp6KV2oq7pGRh99xN9DJQtM0rYfZGxowZ2XTkpVFS1Ymzbt3Y87YhSU//8A6psBAvFNTCb70UkdSGI7XkMGYvNubtND5dLLQNE1zAqUU1pISWjIzMWdmHUgM5swsrMXFv65oMuGZmIj3qFEEX3oJXqmpeKem4h4TQ2+amUEnC03TtKOgWlocdwmZmDMzacnMMhJEdjaqsfHAeiY/PzxTUvA7dgqeySl4piTjlZyMR1ISJk9PF55B5+hkoWmadgSUzUbNJ59StmAB1pJfZ1Bwj43BKzmF4IkT8UpJNhJDcjLukRG96k6hq3Sy0DRN6wKlFA2rVlE6/xnMe/bgPXo0kXfegdfQoXgmJWHy9XV1iE6hk4WmaVonNW37hdL582n8+Wc8EhOJe/45As44o0/fMXSWThaapmkdaMnLo+y556n98kvcQkKIuu8+Qi6fhfSBuobuopOFpmnaIVirqqj417+o/O9ixM2NsBtvIOwPf8DN39/VofU4nSw0TdPasDc3U/n221QsXIS9oYGgiy8i4pZb8IiKcnVoLqOThaZpmoOy2aj5dJnRwqm4GP/p0w9UXg90OllomjbgKaVoWLOG0qfnY969G+/Ro4l98kn8jp3i6tB6DZ0sNE0bsGz19TRt3kLFv1+j8ad1eCQkEPfcswSceeaAaOHUFTpZaJo2ICiLhebdu2neto2mLVtp2raVln2ZoBRuwcFE3XsvIbMvH1AtnLpCJwtN0/odpRSWggKatmyhees2mrZupXnHDpTZDIBbSAg+Y8YQePbZ+Iweg++E8Zj8/Fwcde+mk4WmaS6lrFZsdXVGsY+bG4gJMRm/H3jOZDpssZCtupqmbb/QtNWRHLZtw1ZZCYB4eeE9ciQhs2fjPWY0PmPH4hEXp4uZukgnC03TXKZxwwYK7rnnoIl8DknESBomEzh+9v9ur6s7sI7n4BT8p0/HZ8xovEePxnvYMMTDw7knMgDoZKFpWo9TFgtlr7xCxasL8UiIJ+revwECyo6y2X/9325H2W1gV2C3oex2+M1yO+7h4fiMHYP3qFEDssNcT3BqshCRM4EXADfgNaXUE22WTwOeB8YAs5VSH7Zalgi8BiQACjhbKZXtzHg1TXO+lrw8Cu66i+YtWwm6+GKi77tX1xf0AU5LFiLiBrwMnA7kAxtEZJlSaker1XKBa4G72tnFW8CjSqlvRcQfsDsrVk3TekbNsmUUPzQPTCbinnuWwLPOcnVIWic5885iCrBXKZUJICLvARcAB5LF/jsFETkoEYjISMBdKfWtY716J8apaZqT2erqKH5oHrWff47PpInEPfUUHrGxrg5L6wKTE/cdB+S1epzveK4zhgHVIrJURDaJyNOOO5WDiMj1IrJRRDaWlZV1Q8iapnW3xvRNZF14EbVffUXEbbeS9OabOlH0Qc5MFu21S1Od3NYdOAmjeGoykIJRXHXwzpRaqJSapJSaFBERcaRxaprmBMpqpeyll8m56ioQYdC77xB+002I22+u+7Q+wJnJIh+jcnq/eKCwC9tuUkplKqWswCfAhG6OT9M0J2nJLyDnmt9R/tJLBJ13LsmffIzPuHGuDks7Cs6ss9gADBWRZKAAmA3M6cK2ISISoZQqA04FNjonTE3TulPNF19Q/I8HAYh9+mmCzjvXtQFp3cJpdxaOO4Kbga+BncASpdR2EZknIucDiMhkEckHLgNeFZHtjm1tGEVQ34nINowirUXOilXTtKNnq2+g8J65FN55F15Dh5L8ycc6UfQjolRnqxF6t0mTJqmNG/XNh6a5QtOWLRTc/Vcs+fmE33QT4TfdiLjrPr99gYikKaUmdbSe/mtqmnbElN1OxaLXKFuwAI+oKJLefgvfiRNdHZbmBDpZaJp2RKxlZRTecw8Na38i8OyziH7wQdwCA10dluYkOllomtZlDWvXUvDXe7DX1xP98DyCL71Uj+Laz+lkoWlapymrlbKXX6biX6/iOTiFxNf/jfewYa4OS+sBOllomtYpluJiCu66i6aNaQRdcjHR992HydfX1WFpPUQnC03TOlS3ciVFc/+GvaWF2KeeJOj8810dktbDdLLQNO2QVEsLpc8+R+Ubb+A1fDhxzz2LV3Kyq8PSXEAnC03T2tWSn0/B7XfQvG0bIXOuIPKeezB5ebk6LM1FdLLQNO03apd/TdEDDwAQ98ILBJ4x08URaa6mk4WmaQfYzWZKnniC6sXv4T1mDHHPPoNnfLyrw9J6AZ0sNE0DwJyZRcEdd2DOyCD0uuuIvP0viKenq8PSegmdLDRNo2bZMooefAiTpyfx//onAdOnuzokrZfRyULTBjB7UxPFDz9CzdKlxnSn8+fjER3t6rC0XkgnC00boFqys8m/7S+Yd+0i7MYbiLj5Zj1SrHZI+p2haQNQ7TffUHTvfYibGwkLX8V/2jRXh6T1cjpZaNoAoiwWo5Pdf/6D9+jRxD//HB5xca4OS+sDdLLQtAHCUlJKwR130JSWZnSymzsXk27tpHWSThaaNgA0rPuZgjvvxN7YqOfF1o6IThaa1o8dmMnuhRfwHDSIpDffwGvIEFeHpfVBOlloWj9lq6mh8J651K9cacxkN+9h3Pz9XB2W1kfpZKFp/VDTL9spuO02LKWlRN1/PyFXztEz2WlHxeTMnYvImSKyS0T2isjcdpZPE5F0EbGKyKXtLA8UkQIRecmZcWpaf6GUour9JeRccQXKbmfQ228RetWVOlFoR81pdxYi4ga8DJwO5AMbRGSZUmpHq9VygWuBuw6xm4eBH5wVo6b1J/amJooffJCaT5fhd+KJxM5/GveQEFeHpfUTziyGmgLsVUplAojIe8AFwIFkoZTKdiyzt91YRCYCUcByYJIT49S0Ps+clUXBrbdh3ruX8JtvJvymGxE3N1eHpfUjzkwWcUBeq8f5wLGd2VBETMAzwNXAad0fmqb1H7XffEPR3+5FPDxIWLgQ/5OmujokrR9yZp1Fe4WkqpPb/gn4UimVd7iVROR6EdkoIhvLysq6HKCm9WVKKcoXLaLg1tvwHDKY5I+X6kShOY0z7yzygYRWj+OBwk5uezxwkoj8CfAHPEWkXil1UCW5UmohsBBg0qRJnU1EmtbnKauV4kceofq99wk8+2xiHn9MT3mqOZUzk8UGYKiIJAMFwGxgTmc2VEpduf93EbkWmNQ2UWjaQGVvaKDgjjup/+EHwv74ByJuvx0xObVho6Y5rxhKKWUFbga+BnYCS5RS20VknoicDyAik0UkH7gMeFVEtjsrHk3rD6xlZeRcfQ31q1cT/eA/iLzzTp0otB4hSvWP0ptJkyapjRs3ujoMTXMa89695F1/A9aqKuKefYaAU05xdUiaiyilyK/LJ600jfSSdPw8/Lhnyj1HtC8RSVNKddjiVPfg1rQ+oGH9evJvvgXx9CTprbfwGT3K1SFpPciu7Oyp2kNaSRrppemkl6RT1mQ06gn0DOTUxFOdHoNOFprWy9V89jmF996LZ2IiCa++ime8nn+iv7PYLGyv2H4gOWwq3URdSx0AUb5RTIqexMTIiUyImsDg4MGYxPlFkTpZaFovpZSiYuEiyp57Dt/Jk4l/6UXcgoJcHZbmBI2WRjaXbSa9JJ300nS2lW2j2dYMwKDAQcxMmsmEqAlMjJpIrF+sS4Zv0clC03ohZbVS/NA8qj/4gMBzzyXmsUf1REX9jNlm5tucb/l4z8eklaRhUzZMYmJ46HAuHXYpE6MmMj5yPGE+Ya4OFdDJQtN6HVt9AwV33E7DqtWE3XgDEbfdpgcC7Ed2Ve5i6Z6lfJ75ObUttcT7x/P7Ub9nYtRExkaMxd/T39UhtksnC03rRSwlpeTddCPmXbuJnvcQIbNmuTokrRs0WBr4Kusrlu5ZyrbybXiYPJiRNINLhl7C5OjJPVLncLR0stC0XqJ5927ybrgRe00NCf98Bf9p01wdknYUlFJsLd/K0j1L+SrrK5qsTQwJHsI9k+/h3JRzCfYOdnWIXaKThab1Ag3r1pF/8y2YfHxIeudtvEeOdHVI2hGqbq7m88zP+WjPR+yt3ouPuw9nJZ/FJUMvYXT46D5bpKiThaa5kLLbqf7wQ4offgSvQUkkvPoqHrGxrg5L6yK7srOheAMf7f6IFbkrsNgtjAkfw4PHP8iZyWfi59H3p7PVyULTXEDZ7dStWEH5iy9h3rMH3+OOI37BC7gFBro6NK0LKpsrWbpnKR/t/oj8+nwCPQOZlTqLi4ZcRGpoqqvD61Y6WWhaD1JKUf+//1H24kuYMzLwTE4mdv58As86U09W1If8Uv4LizMWszxrOS32FiZHT+bm8TczI2kGXm79c/RfnSw0rQcopahfuZLyF1+ieccOPJISiX3qSQLPOUcniT7CbDPzdfbXLN65mF8qfsHX3ZeLh17M7OGzGRw82NXhOZ1OFprmREopGtasoWzBizRv24ZHfDwxjz1G0PnnIe7649cXFNYXsmTXEpbuWUqVuYrkoGTuPfZezks5r9f2iXAG/W7VNCdQStGwdi3lL75E0+bNeMTGEvPIwwRdcAHi4eHq8LQOKKVYV7SO9zLeY2X+SgCmx0/nihFXcGz0sX22RdPR0MlC07pZw7qfKXvxRZrS0nCPjib6wQcJvvgiRA/X0evVt9SzbN8y3tv1Hlk1WYR4hfD7Ub9n1rBZxPjHuDo8l9LJQtO6SeOGDZS9+BKN69fjHhlJ1N8fIPjSS/WYTn3Avup9LM5YzGf7PqPR2sjo8NE8OvVRzhh0Rr+tsO4qnSw07Sg1bd1K2fPP07D2J9wiwom6916CL5+l58TuxSqbK9lUuon0knTSStLYXrEdT5MnZyafyezU2YyOGO3qEDvFZrVTuLuaFrOVweMjnXosnSw07ShUf/wJRQ88gFtQEJFz7yFk9mxM3t6uDktrZf+scuml6QcmDsquzQbA0+TJqPBR3DbhNi4eejGh3qGuDbYTLGYbudsryNxcRva2ClqarITF+elkoWm9Ueu5JvxOOJ64BQtw8x84LWN6M6vdyu6q3Wwq3URaSRqbSjdR3lQOQIBnABMiJ3DhkAuZEDWBkWEj+0QxU3O9hayt5WRuLiNvZyU2ix0vP3dSxoaTPC6ChJHOT3I6WWhaFymbjZJHH6Pqv/8l8NxziX3sUV157ULN1ma2lm09MKPc5tLNNFobAYjxi+HYmGOZEDmB8ZHje2xWue5QV9lM1pYyMjeXUbinBmVX+Id4MfLEWFLGhRM7NBiTW8+di04WmtYFdrOZwrvupu7bbwn9/e+JvOtOxNQ3vnz6m5KGEhZnLOaD3R9Q21KLIAwJGcJ5g887kBz6WgumyqIGMjeXkbW5jNIcYxrVkGhfJsxMJGV8BBGJAS5rtquThaZ1kq2mhrw//5mmtHSi/jaX0N/9ztUhDUg7Knbw9o63WZ61HDt2Tks8jQsGX8C4yHEEefW9aWcrCxvY9XMxmZvLqC4x7ogiBwVy3IUppIyLICS6dwxC6NRkISJnAi8AbsBrSqkn2iyfBjwPjAFmK6U+dDw/DvgnEAjYgEeVUu87M1ZNOxxLURF5119PS3YOcc8+Q+BZZ7k6pAHFruysyl/FWzveYkPxBnzdfZk9fDZzRswhISDB1eEdkbrKZtZ/lknGumJMIsQOC2bMKfEkj43AP6T31aM4LVmIiBvwMnA6kA9sEJFlSqkdrVbLBa4F7mqzeSNwjVJqj4jEAmki8rVSqtpZ8WraoTTv3k3eH6/H3tBAwqJF+B13rKtDGjAaLY18tu8z3t75Njm1OUT7RXPnxDu5eNjFBHr2zRF6zY0W0r/OYcv/8lFKMe60BCacmYSPf++u93LmncUUYK9SKhNARN4DLgAOJAulVLZjmb31hkqp3a1+LxSRUiAC0MlC61GNGzaQ9+ebMXl5kfTuO3in9q9hp3ur0sZS3st4jyW7l1BjruGYsGN4atpTzEiagYepbw6XYrPY2fZDPhu/ysbcaCV1SjRTzk8mMMzH1aF1ijOTRRyQ1+pxPtDlSzIRmQJ4AvvaWXY9cD1AYmLikUWpaYdQu/xrCu++G4/ERBIXvopHXJyrQ+r3MiozeHvH23yZ9SU2u41TE0/lmpHXMD5yfJ8dj0nZFbs3lPDzp5nUVTaTMDKU4y8aTERCgKtD6xJnJov2/rKqSzsQiQHeBn6nlLK3Xa6UWggsBJg0aVKX9q1ph1P5zruUPPooPuPGkfDPV3AL7lvzJfcldmVnTcEa3tr+Fj8X/4yPuw+zhs3iqhFXkRDYN+sj9svbUcnaj/dSnldPeII/p1w9joQRvb/jX3ucmSzygdZ/6XigsLMbi0gg8AVwv1JqXTfHpvUSym6n/KWXsBSX4BEdjXtMNB4xsXjEROMRHY3Jr2dbgiilKHv2OSoWLcJ/xmnEzZ+ve2Q7QaOlkXVF61hTsIY1BWsoaigi0jeS2yfeziVDL+mTrZpaK8ut46eP95K3s4qAMG9O//1Ihk6KQkx98+4InJssNgBDRSQZKABmA3M6s6GIeAIfA28ppT5wXoiaq1W89m/KX/knbuHh2CoqQB18g2gKDMQjJuY3icQ9Otp4Piqq2zrEKYuFovsfoObTTwmefTnRDzygJybqJkop9lXvO5Ac0krTsNqt+Lr7cmzMsdw24TZmDprZ4/URdpud7asLqSltIiDM+8BPYJg3Xr5dj6W2vImfl2Wye30JXn7uTL1sKKOmxeHm0ff74jhS1ANFAAAgAElEQVQtWSilrCJyM/A1RtPZ15VS20VkHrBRKbVMRCZjJIUQ4DwReUgpdQwwC5gGhInItY5dXquU2uyseLWe17hpE2UvvEDg2WcT+8x8sFqxlpZiKSrCUlSMpbgI64Hfi2navBlbTc3BOxHBLTzMkURijJ/YGNxjYoznYmNwCwnpsLzbVt9AwV/+QsOaNUT85TbCbrihz5aR9xYNloaD7h6KG4oBGBI8hKtHXM3UuKmMjxyPh5trKqzLcuv4/p0MynLrcPMwYbMcXNLt6eNuJI9QI3kYScTnQELx8nU/8B5prrew8atstv2Qj4gw4YwkJpyReEQJp7cSpfpHUf+kSZPUxo0bXR2G1km22lqyLrwITCaSP16KW0DnKvvsjY1YikuwFBViLS42EklRIdaiYkeSKUI1Nx+0jXh7H0gk7rGOhOJIJB4xMYiHB/m33EpzRgYx8x4i+JJLnHHK/Z5Sir3Vew8kh/TSdKx2K34efhwXcxxT46YyNW4q0X7RLo3T0mJjw+dZbF6Rh4+/B9NmDyNlfATmBiu1FU3UVTRTW9FMXUUzdRVN1FU2U1vejMVsO2g/Ht5uBIZ54x/iTdG+GizNVoYfH8OU85LxD+k7RZcikqaUmtTReroHt9bjlFIU3f8AltJSBv333U4nCgCTry9eKcl4pSQfct+26moshYXGXUlh0YEkYikqxPzDKqxlZb/ZTnx8SHjlZfxPPvmIz2ugUUqRV5fHjoodB+4gShpLABgaMpSrR17NSXEnMS5inMvuHtrKy6hk5bu7qC1rYuTUWI6/aDDefkZs3v4eePt7EJn02/4bSinMjVZHImlyJJJfk0p8aghTzksmLK7/Diapk4XW46rfX0LdN98Qefdd+IwZ0637FhHcQ0JwDwmBY45pdx17SwvWkhJHIinEWlKK/8nT8B4+vFtj6U/syn4gMez/2VmxkzqLMX6Rv4c/x8cez9S4qZwYeyJRflEujvhgzQ0WfvxoLxlriwiK9OHC28cTlxrS6e1FBG8/D7z9PIhI7FtNXrvLYZOFiFyllHrH8fuJSqkfWy27WSn1krMD1PqX5t27KXn8cfymTiX0uutcEoPJ0xPPhAQ8E/p2s0xn2Z8YtpdvNxJD5Q4yKjIOJAYPkwepIamclXwWI8NGMjJsJENChvTKznJKKfamlbL6/d00N1iZcGYSk88ehLunbrjQVYetsxCRdKXUhLa/t/fY1XSdRe9nb2oi67LLsNXUkPLJJ7iHhbk6JA0obypnfdH6A4lhZ8VO6i31gDE50LCQYQeSwjHhxzA4eHCvTAxt1VU2s+q93WRvLScyKYBTrh5OePzAvCs4nO6qs5BD/N7eY007rJLHn6BlXyaJ/35NJ4peIr0knZu/u5k6Sx2eJk9SQ1M5J+WcA8mhrySG1pRd8cuqAn76eB9KKU68dAhjTonv0bkf+qOOkoU6xO/tPda0Q6r96iuqlywh7Prr8TvhBFeHowGr8ldx58o7ifaLZuHMhaSGpva5xNBWZWED37+TQXFmDQkjQ5k+J5XA8L4x9lJv11GyGC4iWzHuIgY7fsfxOMWpkWn9Rkt+PkUP/B2fsWOJuOVmV4ejAZ/t+4wHfnyA1NBU/jnjn31i7unDsVnspC3PJm15Dh7ebsy4dgTDjo3WfWW6UUfJYkSPRKH1W8pioeDOO0GE2GeeQTz69pVrf/Duznd5Yv0TTImewoJTF+Dn0Tsm1zkSdpudvIwqfvxgD1XFjQydHMXUy4biG9i7h/vuiw6bLJRSOa0fi0gYRs/qXKVUmjMD0/qHsgULaN6ylbjnn8czXo/a6kpKKV7e/DKvbn2V0xJP48lpT+Ll1vsm2emIzWonP6OKfZtKydpcTnODBf9QL869eSxJowZWXVhJbTOL1+dityvumOnc4fM7ajr7OTBXKfWLYwTYdGAjRpHUQqXU806NTuvT6tf8SMWi1wi+/HICzzzD1eEMaDa7jcfXP877u97n4qEX88BxD+Bu6jvdrKwtNnJ3VJK5qYysreW0NFnx8HZj0OhwBk+IIOmYsAHTHFYpxbrMSt5Zl8PX24uxKcVZo6JRSjm12K2jd0uyUuoXx+/XAd8qpa4RkQDgR4wpUTXtN6zl5RTOnYvX0CFE/W2uq8MZ0Cw2C/euuZfl2cu5btR13D7h9j5Rlm8x28j5pYJ9m0rJ2VaBxWzDy9edlHHhDB4fSfyIENw9BkaCAKhrtvDxpgLe/imHPaX1BPt68PupyVx5bCJJYc4vSuwoWVha/X4asAhAKVXXdnY7TdtP2e0U3jMXe309cf95XQ/x7UKNlkZuX3k7awvXcsfEO7hulGs6QnZWS5OV7G3l7EsvI3d7BVaLHZ8AD4ZOiWLw+AjiUkNwG2BNYHcV1/H2umw+Ti+gocXGmPggnr50DOeNjcW7B5NlR8kiT0RuwZibYgKwHEBEfABdU6m1q/L112n48UeiH3oIr6FDXR3OgFVjruFP3/2JX8p/Yd4J87ho6EWuDqldzQ0WsraUs29TKXk7K7FbFX5Bnow4MZbB4yOIGRqMqQ/PA3EkWqx2vtlRzFs/5bA+qxJPdxPnj43l6uOSGJvgmom4OkoW/wfMA2YAlyul9s+BfRzwH2cGpvVNTVu2UPr8CwSceSbBsy5zdTh9wrs738Vqt3Ja4mnEB8R3yz5LGkq4ccWN5NTm8OzJz3Ja0mndst/uVFfZzOYVuexYU4i1xU5AqDejp8czeHwk0cmBfXqioCNVXNPMf9fnsnh9LmV1ZhJDfbn37OFcNjGBED/XtvDSQ5Rr3cZWW0vWRReDUiR/8jFugb8dvVM72BeZXzB39a91OiNCRzAjaQYzEmeQEnxkXZlyanO4/pvrqTZX8+KpLzIlZkp3hdstKgrr2fRNLnvWO0aonRLF6OnxRCYF9Im6lO6mlOKnfRW8vS6Hb3aUYFeKU1Ijufr4JE4eGuH0u6puGe5DRJYdbrlS6vyuBqb1T0opiv7xDyzFxQx69x2dKDohpzaHeT/NY0LkBB4+8WG+z/ueFTkreHHTi7y46UVSglKYkTSD05NOJzUktVNfpDsrdnLjihtRSvH6Ga9zTHj7I++6QnFmDWnLc8jeWo67p4lR0+MYNyORgNCBWafVYrWzbEshC1ftY3dJPSG+HvzhpGSuOjaJhFBfV4f3Gx0NJFgG5AGLgZ9pMx6UUuoHp0bXBfrOwrWqPviA4gf+TsQddxB+/R9dHU6vZ7aZuerLqyhuKOaD8z44aEKg0sZSvsv9jhU5K9hYshG7shPvH2/ccSTNYHT4aEzy20reDcUbuPV/t+Lv6c/C0xeSHNT+nB89SSlFzi8VbPoml8I91Xj5uTPmlATGTI/H239gVnvWNVtYvD6X19dkU1zbzPDoAP5wUgrnjonpWoV1cw1krYbMleDhAzMfPqJ4Ontn0VGycANOB64AxgBfAIuVUtuPKCon0snCdcx79pB12Sx8J4wn4bXXENPAaq1yJB77+TEWZyzm5dNeZlr8tEOuV9lcycq8lXyb8y3ritZhtVuJ9I3ktMTTOD3pdCZETsDN5Mb3ud9z1w93ERcQx8LTF7p8Njq7zc7etFLSv86loqAe/xAvxs1IZOTUWDy8Bk5z19ZKa5t5/cds3l2XQ53ZygmDw7jh5MFMGxreueI3awvkbzCSQ+b3UJAGyg4efjDiPLj41SOKq1uSRZsdemEkjaeBeUqpF48oMifRycI1bPX15FxxBdbKKlI++Rj3iAhXh9TrrchZwe0rb+d3I3/HXZPv6vR2tS21/JD3AytyVvBj4Y+YbWZCvUOZFDWJ73K/Y0ToCF6Z8Qoh3p2f1Ke7WVpsZKwtYtO3udRVNBMS48eEmYkMnRyFm/vAvIjYW1rPolWZfLypAKvdzlmjY7hhWgpj4jto1aQUlO78NTlk/wiWBhATxE2ElOmQcgrETwb3I6/87rZk4UgS52AkikHAMuB1pVTBEUfnBDpZ9DxlsZB3w400/PwziYsW6tFkOyG/Lp9Zn81iUNAg3jzzzSOebrTR0siagjWsyFnB6oLVjI8cz9MnP+2ycZ6aGyz88kMBW7/Po6nOQnRKIBPOSGLQ6PAB2aoJYGN2Jf/6IZMVO0vw9jAxa1ICf5iaQmLYYeojaot+TQ6ZK6HeaARA2JBfk8OgqeDTfc1nu6uC+01gFPAV8FCr3tzaAGdUaD9Iw9q1xDz6qE4UnWCxWfjrqr8C8NS0p45qXmpfD19mDprJzEEznT7Mw6GYm6wU7Koid0clu38uxmK2kTQqjAlnJBEzJGhAtmyy2xUrdpbw6qpM0nKqCPH14LbThnLN8UmE+bczDpfdDtmrYNdyI0GUZRjP+4ZDyslGckiZDsGun9Wxo34WVwMNwDDg1lZ/fAGUUko3eRmgyl95hZqlSwn/858JvuRiV4fTJyzYtIBt5dt45uRnuq0/BdBjX8o2q52SrBrydlaRt7OS0uxalAJ3LzeSx4Qz4YzEATsTndlq4+P0AhauziSzrIGEUB/mXXAMl01MwKe9MauqcmDzf42fmlxw94akE2DclUZyiBoFvazur6NRZ48qWhE5E3gBcANeU0o90Wb5NIzxpcYAs5VSH7Za9jvgfsfDR5RSbx5NLFr3qV76MeUvvkTQhRcSfvOfXR1On7AqfxVvbH+Dy1MvZ+agma4Op1OUUlQUNJCfUUnezioK91RhbbEjJiFqUAATzxpEwogQopKDBmx9RLPFxls/ZbNodRZldWZGxQXy4hXjOWtUNO5thyWxNMHOz2DT25C1ChAYfAqc/iCknm20aOrFnDbspKMl1csYranygQ0iskwptaPVarnAtcBdbbYNBf4BTMKYkS/NsW2Vs+LVOqf+xx8p+vvf8TvheGLmPTQgixq6qrihmPvW3EdqSCp3T777iPdjbrJSU9pITVmT8X9pEzVlTVSXNSECfkFe+AZ54hfoiW+QF35Bxv++QZ7GskDPDr/U6yqbDySH/IxKmuqM4eFCon0ZcUIs8cNDiEsNwcun74xY6wwWm50lG/NY8N0eSmrNnDQ0nOcvH8cJg8MO/kwoBYXpsOkd2PYRmGsgOAlOuQ/GXtEripc6y5l/8SnAXqVUJoCIvAdcABxIFkqpbMeytoMSnoExwm2lY/m3wJkY/T00F2nOyKDg1tvwGjyYuAULEE89wUxHrHYr96y6B7PNzPyT53c4f0Rzg8VIBmWOZFBq/F5d2kRzveWgdf2CvQiO9GHQaGMOh8aaFhqqzZTl1NFU10J7bVe8/TwcyaNVQgn0oqa0kbyMKqpLGgHwCfQkYUQo8cNDSRgRgn/IwOw415bdrvh8WxHPfrOL7IpGJiaFsGD2eI5NaTOPRn0ZbH3fSBJlO8HdB0ZeAOOvgqQTe10RU2c4M1nEYXTo2y8fOPYotv3NzDkicj1wPUBiYuKRRdnHWUpKyb/lFvymTCb81lsxOekL3FJcTN4NN2Ly9yfh1X/h5u/vlOP0N//a8i/SS9N5/KTHGRQ06DfLy/Pr2bG6gNLcOmpKm2huODgh+Id4ERTpS8r4CIIifAiO9CUowofACB88DjN/g91mp6neciCBNNSYaaxtoaGmhcYaMw01LVQVV9FY24LdpnD3ciNuaDDHnBRLwohQQmP99F1jK0opVu4q4+mvd7GjqJbh0QH8+3eTOHV45K+vk80Ke1cYxUy7l4PdCnGT4NznYdTF4B3k2pM4Ss5MFu290zo7EFWntlVKLQQWgtF0tvOh9Q/2hgbybrqRlj17ad66lfof1xI3/2m8Bg/u1uPY6urIu/4G7A0NJL37Lh7Rru3w1VesK1rHwq0LuWjIRZybcu6B5202O1mby9m2Mp/CPdW4eZiIGRzE4ImRBEX4HEgKgeHeRzyhj8nNhF+QF35BXkQkHrrSWdkV5kZjIqGBWu/QkY3ZlTy1fBfrsytJDPXl+cvHcf7YWEy2ZqjKgtpC2PMtbFlsNHX1DYdjbzTuIiL7z8zUzkwW+UDrArl4oLAL205vs+3Kbomqn1A2GwV33oU5YxcJ/3wFZbNTdN99ZF18CVFz7yF49uxuuTJULS3k33or5sxMEhe+infqsG6Ivv8rbypn7qq5pASlMHeKMVBgQ42ZHWsK2b6qgIaaFgLDvTnh4iGMODEGbz/XDH0hJhmww24ckt0GDeVkZe3hszVplBRkc4ZXLY8NtpDiVY/ppyL4uhCaq3/dRtxg2BlGghg6E46iWXRv5cxksQEYKiLJQAEwG5jTyW2/Bh4Tkf1dUWcCf+v+EPsmpRQljz1O/cqVRP/j7/iffDIAPss+pfBv91L80Dzqf1hFzKOP4B525HMSK6UoeuDvNP60jpjHH9d9KTrJruz8bfXfaLA0sOj0RdTmWVjz/Xb2pZditykSR4Yy/cp4EkeFDbh5Gnodqxk2vAY5a6GuCOqKUXXFiLKRDNwK4AFKmZDaKAiIhpBkSDweAmMgINZ4Lno0+Ee6+GScy2nJQillFZGbMb743TB6fW8XkXnARqXUMhGZDHwMhADnichDSqljlFKVIvIwRsIBY3iRSmfF2tdUvfUWVe++S+i11xI063JKc2oJjvLFMyKChIWvUvXOu5TOn0/m+RcQ+/hj+E879NhDh1P+4kvUfPop4bfeQvBFF3bzWfRf/972bzbkp3Fn8ENs+lc1Zbl5eHq7MerkOEafHE9wVO8bUXRA2rMCvvorVO6DsKG0+Mfyi/tY1lknUyahjBo+nDOOHYd/RCLiFwFuA7sFmJ7Poo+pW7GCnFvvoPmUy6mZchFZW8sxN1jxD/Hi5DmpDBodDkDz7t0U3nU35t27CbnqKiLvurNL05tWf/ghRfc/QNAlFxPzyCO6srOTfszYwJtLPmVU+VTcWjwJjfVj9PR4hk2JwtN7YH/Z9BqVWfD1vbDrSwgbQv0pj/BSXjJvrM3CalNcMSWRW04dQmTgwGgB1u0DCfZ2/T1ZtDRb2fPlZnYsXkVF6Ehs4omnjzuDxoQRNzSEzd/lUVXUwLApUUydNRQff0/sZjNlzz5L5Ztv4TV0CLHz5+OdmtrhsepXryHvxhvxO/54Ev75CuLR/8pfu5OyK/IyKkn/Lov87TUoUSSPDWP8qYOIHRqsE21v0dIIa56FHxeAyZ2S8bfxYuMMPtpSRrPVxgVjY7n99GEkhblmfC1X0cmiH2iqbyF7azmZm8rI21GJzabwtDYw+LgEhhyfYExe72jBYrPYSVueTdryHDx93Dlp1lCGTo5CRKhf8yOFf5uLvbqGiDvvIPSaaw45jHjzjh3kXHU1HklJJL39Nm7+h//g2O2KvJ2VNNW1/NqEzfHlKAIIiPEPvy5us1yEqORA/IIO3wehNyrLreO7N3dSUVCP1auZbRGr+dOcOUxIGe3q0LT9lIIdn8LX90FtPnnx5/Bw82y+yXfD28PEBWPjuG7qIIZHD8zRi3Sy6KPqq5rJ3FxO5uZSCndXoxT4h3gSnvcT4fnrGbPwMXyGDT3k9hWF9Xz/dgYlWbUkjQ7j5CtSCQj1xlpVRdH9D1D/3Xf4nXACMY8/jkfUwRVylsJCsi+fDe7uDHrvvd8sb81qsbFrXTGbvs2lprTpqM/bZBKSx4ZzzLQ44lNDev1IpTabnfTlOWz8IhufAA9sU4pZUPsI9xz3V+aM6Gw7Ds3pSjPgq7shaxWlvkO513w1KxqGkBTmy9XHJXHZxASCfAf2nbNOFn1IbXkTe9NKydxcRklWLWAMr5AyPoKU0aE0PXInjes3kPjaIvyOO67D/dntim3f57Pu032ISTj+wsGMmhYHAtVLPqDkiScweXkR88jDBMyYARjzZ+dceSWW4hKS3n0H72HtN5E1N1r4ZVUBW/6XT1NtCxGJAYyfmUhkUoDRY9jxdmr9vtr/vDL+Ofh5FFaLncz0MnauLaK5wUJQpA/HnBTHiONjemWzzsrCBr57cwelOXUMmxJF2Awr//fDtZwcfzLPTX9OFzv1Bs01qJVPoH5eSJP48GTLpfzXdirTh8dw1XFJTOuBua37Cp0s+oiSrFo+fiYdm9VOZFKAkSDGRRAS7Wc0Xb3/fmo+WkrM4493uUVSbXkTK9/NIG9nFTFDgjjlquGERPthzsyi8O67ad6+neDLLiPyrjvJv/U2GtPTjXkp2klI9VVmtnyXy/bVhVjMNhJGhjJhZiJxqSHd9uVotdjYl17G9tUFFO2twc3dxJCJkRwzLY7olECXfwkru2LL//JY90kmHl5ujL0kirXey1myewl+7n4sOW8JQV59u5dun2e307TxHdSKB/FuqWSx9VRe85jDGVNGceWxib1ybmtX08miD2iqa2HJYxsQk3DBX8YTFHHwqJPl/3qVsuefJ/xPNxFx661HdAylFLvWFbPmgz1YW+xMOmcQ42cmYrJZKXvxJSpeew3x9kY1NRH71JMEnX/+QdtXFjWw6Zscdq8vQdkVQyZFMX5mIhEJzh2KuqKgnu2rCsj4uRhLs42wOH9GTYtl2JRoPF0wiF1NWRP/e2snhXuqCRnuwebhX/FN6ZfYlZ3pCdO5bcJtDA7u3p7zWtdkb12Nafk9JDZuJ80+lHdDb+bEk2ZwTlfnth5gdLLo5ew2O8sWbKE4s4ZL7p74myEZaj7/gsK77iLw3HOJffqpo76qbqgxs/r9PexLLyUs3p9Trx5OZFIgDT+vp+SRhwm68CLC/u/3B9Yv2ltN+je5ZG8tx93DxIgTYxk3I4HA8J4dRrml2cqeDSX8sqqA8rx6PLzcGDYlimOmxTk9YYGRbHesKWTNB3uwYWPniO9Z6fMpAV4BXDL0Ei5Pvbxb56YYkIq2Gn0dxM2YMtTkhhITLXbBbBPMNjDbodkKZpvQZAOzFZqsimYbNLVYCfnlTU5tXE4FgayI/zPHnPlHxiSEuvrM+gSdLHq5tUv3sumbXE69ZgQjTog5aFljWhq5116H99gxJL7+ercODpi5uYwfFu+iqbaFcTMSmXxe8oEB6ZRdkb2tnE3f5FK0rwYvP3fGTI9n9Cnx+Pi7doRZpRSlOXX8sqqAvRtKsFrsRCUHMmpaHEMmRh7xGEqH01Bt5qs3tlCSUU9xcCYrUt4iOiqMOSPmcE7yOfh66CKNo5K3HvN3j+OV/b+j3pUVN3YkXEHCRfMICT3yUQsGIp0serF96aUsX/gLx0yLY/qcg/s9tOTkkH35bNyCgkh6bzHuISGH2MuRMzdaWLt0HzvWFBIY4cP0K1Kprzaz6dtcqooaCAj1ZtzpCYw4IRYPr953+97cYGHXumK2ry6gqrgRL193kkaHETM4mNghwYRE+x5Vayq73c6K/20gY1kVygrrEpcROdmDOSPnMCV6isvrTvq8nJ9QPzyJZH5PFQH823YuDUmn4udpwscdfNzA28OEtxv4uIO3m8LbDbzcwNsdPN2M5zxN4OWm8DQpvBMmIOFDXH1mfZJOFr1UVXEDHzy+kdBYPy66YwJuHr/2d7BWVZEz+wpstbUMem8xnklJTo0lf1cV37+TQW2Z0fQ1LN6fCTMTGTwxEre2s3z1QkopCvdUs2NNIXkZVTTVtgDg5edOzOBgYoYEETskmIjEgE6NqGq2mfl8+3K2LS0longwZYG5BJ5Rz6wpF+qipu6Q/SP88ARkraLGFMTL5rPZGTeLBy+bwuAIPeS9q+hk0Qu1NFv58ImNNDdYmHXv5IMmlLGbzeT+/v9o3raNxDfewHfC+B6JydJiI2NtEYERPiSODO2zV81KKWrKmijaW03R3hqK9tUcmMjH3cNEVHIgMUOMBBKdHHRQJXlJQwnv73qftWu3MnHXuXhb/Qg4sZlLLzsVf6+B1Zu32ykF2ath5ZOQs4ZGzzAWNJ/NR3I6t541jiunJOomrC7W2WShB6vpIUop/vfWTqpLGjn/L+MPShTKbqfo3vtoSksj7rlneyxRAHh4ujF6et+/ahYRgiN9CY40pv8EaKxtoWhfNUV7aijcW03aV9koZfQcD08IIColgAyvTbxf/iajck/mpLI5+ESZOP+PEwmPd37leb+mFGSuhB+egty1WH2jeMPvep6uOIETh8fz6YWjiA3u3XNOawfTyaKHbP42j33pZRx/8WDiU3+th7CWlVHx2r+p/eILIu64g8CzznJhlP2Lb6Ang8dHMni80RO9pdlKSVYthXur2b0jny2rK3GzRXEJfwWBiWclMfmcZD0J0NFQCvZ9ZySJvJ9RAbGsTLmbm3eNwsvbj6dmj+T8sbF99g52INPJogfk76rip4/3kjI+gpFDbFR/9BGNG9NoTE/DkpMLQPBllxH2xz+4ONL+zdPbncAUN16repOl0UuJTY7jL4lziWsaTHRKEJFJA3NsoG6hlDFb3A9PQsFGCIwn7/iHuXH7SLbvMHPBuFj+fu5Iwvz73vhfmkEnCydSVisVG7az/N1S/FQjSe88QtZzRQC4BQfjM3EiIZfPxnfiBLzHjBmYV1uNlcZIoB5+MPxsiB5zYCDC7qSU4vPMz5m/cT415hquO+Y6bhx7o27+erTsNtj1FayeD4WbICgR81nP8kzJRBatLCA6UHj92kmcOjzK1ZFqR0kni25kb2qiacsWGtPSaEpLo37LL6Sl3oDVN4ZJBf8l+ITJ+EyciO/EiXimpAzM5NDazs/g89uNhKHsRkuZwDhIPQtSz4ZBU8H96K9Es2uyeWTdI/xc/DNjIsaw8PSFpIZ2PFS7dhj1pZD+FqS9ATV5EJwE57/Ij74zmLssg7zKgv9v777Do6rSB45/TyaF9E5IAxISAgECAtJBiiAIC4oiYBeEdV119beKKIossmJdFcWCgIKLiIqLiCAitiBFaoBQkkBCEhJI723K+f1xhxhiQmIykwlwPs8zz5R75s6by3DfOefccw53DejAnLFRuLdpffN7KX+eShbNJE0mct9/n+KffqIi/hgYDCAETlFRnLnuHxSVBXL91FCiRqyzdaitR1kebH4Cjn6h1STu2gBuAZC4VQuvXFkAACAASURBVPuVeugTbalLR3eIGAVdxkPE9eDy50bkVhorWXFkBcuPLKeNrg3PDniWWzvfip1QfRJNIiWk7tb+bY59BSY9hA+HsYspDL2eRVsS+Hz/QcL9XFk3ewD9w9XguCuJunS2mfJWr+b8C4tx7tULl379cOnbB+devUiML2X7quNcM7o9g25Rg4WqXahNlBfAdXNgyGN/XNxeXw7Jv8CJbyDhWyg5r00F0WHQ77UOn7BLfsyezD0s2r2IlKIUxoWNY861c/Bz9rPe3yUl6Mugoggqi6GyCCoKzfdF9dybt1cWQxsv8OsMfhHavW8k+ISDQytYra2yGA5/BntXQFY8OHnCNXdA3xmk2QUTm5jD698nkFdaxV+HhfPIqEg1F9NlRI2zaAGVyckk3zwZ1wEDCHn3nepmpezUYta/sp924R5MfKQXdpfBADerq12buOldaNedkqoSlh1ZRoWhgjDPMDp6dCTMM4wAF23hJkwmrS385Gat1pEVr+3Pv6uWOLqMh6DeYF7MKbc8l1f3vcqm05sIdQ/lmf7PMCh4kPX+rpIs+PQOyDgAJkMDhQU4uYOTB7TxqHHvDmW5kJMIRWdrFLcDr/a/Jw+/C7fO4Opvlb6di2Qd1xJE3KdQVYxsF0NO17v50WEou9Iq+C05j7MF2oDO7sEevDg5hu7Batbdy41KFlYmjUbO3HEnlcnJhH+9EYe22uWZFaV6Pl+8F5NRMuWpa3HxsO2cSq1CPbWJY7nHeOLnJ0gvScfZ3plSfWn1W5ztnasTR0dP7T7MI4wORkmbUz9qySPlV5BGcG2LafhT/M/Tk//s/w9lhjLu63Yfs2Nm08beir/Mi8/Bqr9AYTr0mwXO3uYE4FlHQvAAR7fqpFavyhLITdJuOQlaAslJ1J4baiwy5eRZI3lEan0G7u3APVC7d2ziYEJDFZzYpCWJMzsw6ZxIDhjDl7qxfJYZQHaJNkrez82J/mE+9A/3oV+YD53buqvBdZcpNSjPyvI++ojyQ4cIeuWV6kRhMkm2rYynJL+Smx/vrRJFXX0T7bojpWTt8U94dd+reLfxZuUNK+ndtjfZ5dkkFyaTUphCcpF2fyjrEJuTN1fvUiAIdA0kLCiMsMh/0LG8jLZpe/nwwGIOtmlD77a9eW7gc4R7hVv3byvK0BJF8Tm4c73WRGYJTm4Q1Eu71WQyQVF6jeSRqCWT0z9D3No69uNhTh41Ekjte7d2vzdzFZ7FtO9DjPs+wqE8m2z7QNaIO1lVOoT80x4EerZhcIQP/cN96RfmQ7ifq7pA4yqjkkUTVCYlkf3mEtxHX4/HhPHVr+/9JpnU+Dyuuz2KdmFXeXW8Zm1ixLzq2kRhZSHP7XyO7anbGRYyjEWDF+HdRhuk2NalLW1d2tI/sP9Fuyo3lJNalEpyYTLJRcnVCeVA1gHKDeUgwNPZjYVZ55nk7YWdh3Xn1KIgTUsUpTlw55fQvn/D72kuO3OTlFd7rdO/pspiLXkVZ2rJq/Z96i7t3lj1h93qHb0otvfGs+wMQkp+MfXiY+O9nHEZQN8wf+aF+9I/zIcQb2eVHK5yVk0WQoixwJuADlgupXyx1nYnYDXQB8gFpkopU4QQDsByoLc5xtVSysXWjLWxpMFAxtynsHN1pd2CBdX/gVIO57DvmxS6DGxHt6FBNo7ShkpztTWPj66/qDYBEJcdx5yf55BVlsXjfR/n7ui7G3UCcrZ3Json6g+Xu5qkiayyLNKK04j0jMDr1yXamI3yXJi83Dqdw/lnYNUEKC+EuzdASIO1d+tzcgf/KO1WB5NJkppbSkJKKulpp8nNPENZ7llcKrMJMOQTIPLJc76G9PBpdO7SnRfDfGnn2Qo61pVWxWrJQgihA5YCo4F0YK8QYqOU8liNYjOBfCllhBBiGvASMBWYAjhJKXsIIVyAY0KItVLKFEvHKaW2BrSdTmBnJxo8eeUuX07F0aMEv/E69r7apYEFWWVs+/AYfqFuXDc96ur9BVZPbcIkTayOX82bB94kwDWA1eNW08O/R7M/zk7Y0c61He1c22kvXP8cuLWFb+fCf2+B6Z9o/QeWkncaVk3UfsnfvQGCe1tu3xZSZTCRmFVMfEYRxzKKiM8o5HhmMSWVWue7vZ0dkQExRHcZQmiQB1FBHnQN8sBDjYVQGmDNmkU/IElKeRpACPEpMAmomSwmAQvMj78A3hbamVYCrkIIe8AZqAKKrBFkRYmelU/sqH5uZye0xKETCJ3ATmeH7sJzox5Dujv2I17g6EFf7A7vx04nKMopRwgY99ceVlmEp9WrWZsI7HlRbSK/Ip95O+YRezaW0R1Gs2DQAjwcrTitxoC/gYsfbHgAPhwPd36htc83V+4p+GgCGCrgno3a39kKpOaWEZuUTVxaAfEZRSScL0Zv1C5acXHU0TXQg8m9g+kW5EG3IE8iA9xwsr8Kv6NKs1kzWQQDaTWepwO1G3ery0gpDUKIQsAXLXFMAjIBF+AxKWVe7Q8QQswGZgO0b9++SUHaO+oYeHMnTEaJyWgy38vq50aT+XGVgeLYXzEaDDh3iULa2VeX8/Bzpt+EsBZfcrRVSN0Dn92ldWaPeAaGPFo9bmLfuX08GfukljD6z2Nq1NSWqXXFTNEG8K27C1aMgbv+B77NWB87O0HrozDp4Z6vqxOhLRRV6NmZlMuOpGxiE3M4k6tNw+7r6kh0kAczh4SbE4MHHX1d1RVKisVYM1nU9S2tfZ1ufWX6AUYgCPAGYoUQ31+opVQXlHIZsAy0S2ebEqSDk47eNzTcIZq95C1y9rxDyNK3cR/VCtqpW4P9q+Cbf4JXqNbRaz6JGk1Glh9Zzjtx7xDqHsrS8Uvp4tOlZWOLGKWd2D+ZAitvgDu++OMVRo2RdVxregK49xto29WycTbAYDQRl15IbKKWHA6lFWA0SVwddQzs5MuMwWEMjfQjTF2dpFiZNZNFOhBa43kIkFFPmXRzk5MnkAfcDnwrpdQDWUKIX4G+wGlsoPxoPDnvv4/npIm4jxrV8BuudEY9fPsU7P0AOo2EW1dqYwyAnPIc5sbOZU/mHm4Mu5H5A+fj6mCjBYRC+sCMrfDxZPhoPExbo01P0VjnjsLqSWBnryUe/87WivQiqbll/JKYTWxiNjtP5VJcYUAIiAnx4sHhnRga6c817b1wUIM9lRZkzWSxF4gUQoQBZ4FpaEmgpo3APcAu4FbgBymlFEKkAiOFEP9Fa4YaALxhxVjrZaqqIvOpudj7+hLw9NO2CKF1Kc2Bz+6BMztg0CNw/QKw09rAd2XsYm7sXMr0ZSwctJCbIm6y/a9dv0iYuVXr8F4zBSYvg243N/y+zMNaorBvA/dual4zVgOKK/TsPJVbXXu40LQU7OXM+B6BDI30Z3CEL14uV/m4HcWmrJYszH0QDwFb0S6dXSmljBdCLAT2SSk3AiuAj4UQSWg1imnmty8FPgSOojVVfSilPGytWC8l5623qUxMInTZ++g8r/KxE5mH4dPboTQbJn8AMbcBYDAZeOfQOyw/spxwz3BWjFlBhHcrmg/LIwju2wxrp8Pn92kJr9+s+stnHITVN2mXpN6zUZujyQqMJsknv6XyyrcnKKowqKYlpVVT031cQvmhQ6Tcfgeek28maNEii+77snN0PWz4u9ZxPG0NBGlLv6YVp/HMjmc4kHWAyZGTmdtvLs72rbSjX18OX8zQpgoZNgdGPP3H+ZXS92nNVs6ecM8m8LbOAL+jZwuZt+EocWkFDAz35ZFRkfTp4I2jWqVPaWFquo9mMlVUkPHU09i3CyBg7lxbh2M7JiP8sEgb7BY6AKZ+DG5tkVLy2cnPeG3/a+iEjsVDFzMhfIKto700B2e47WPY9A/45WUozYLx/6luRiN1j9Zc5eqn9VF4hV56f01QXKHnte8SWL0rBR9XR96Y2otJvdQyo0rrp5JFPbLfeJOq5GTar1yBzs3N1uHYRkUhrL8fEr+DPvfCuFfA3pHMkkzm75zP7szdDAoaxL8G/ev3gXGtnc4eJr4Nrm21BFhmHu2dcUDr03AL0BKFZ7BFP1ZKyTdHMln49TGySyq5o397nhjTBU8XNRhOuTyoZFGHsn37yFu1Cq/p03Ad1IgJ4vQVrWPdAUvKSdTa+POTtV/f185ESsmGxP/x8t6XMUojzw54limdp1x+v4qFuHi094fjIPsEeIbA3RvBI9CiH5eSU8r8jfH8kpBNtyAPlt3dl16hXhb9DEWxNpUsajGVlZHx9DwcQkIIePzxugsZKrUVw05th6Qf4PwRiJ4Ek5ZqnaKXu4TvYP1M0DlqJ8+Og8kqy+Jfu/7FL+m/0DegL88Pfp4Q9xBbR9o8NUd7+0ZoNQq3thbbfaXByHs/nWbpT0k46ux47i/R3DWgA/bqklflMqSSRS1Zr/0HfWoqHT5ejZ2reXyAlNq8QEnbtQSRHAv6Uu36+9AB0Oc+bT3irBNa569fpG3/iKaSUmua2f48tOsB0z5Beoaw+fQ3vLDnBaqMVcztN5fpXaZfOUuTxkzR5nhya2vRRP9rUg7PbjjK6ZxSxscEMn9CNAEeV1jtU7mqqGRRQ+nu3eSvWYP33Xfh0iMKjm8y1x62Q8EZrZB3GPSaDp1GQdjQ308w3Sdrl2UuGwE3vwddW3lnb21VpfDVQxD/JXS/BSa+Ta6xnEU//R/fp35PT/+eLBq8iI6eHW0dqeVZcAxFVnEF//7mOF8dyqCDrwurZ/RjWGd/i+1fUWxFXTprZiwqIvkv4xGyirA7vLA7v09bJtPRDcKGaSOVI0Zd+pr7wnRtPqKMAzD0n9rMq3aXwaRtBana+IlzR7W2/MGPsi31e57f9Twl+hIevuZh7o6+G93l8LfYiNEkWbPnDK9sPUml3sQDwzvx4PBOai1qpdVTl842VvE52DqPrI9j0WfZ0WFUDnZEa6OTI0ZBSD+wb+TIWc8QuG8LbJkDsa9BxiG4Zbk2NsFaCs/Cb8t+X9zmws1QqU3LYaw0P6+qtb3GY2nUlum8/TMKO/Tn37FPsiV5C9G+0fx78L9b1wC7VuhIeiHzNhzhcHohgyN8eX5Sd8L9r9Ir6JQrlkoWbTwp2bWbgpM6fCYOxmX+YnBrRrOBQxuYuERrB9/8BCy7Dqb+1/JTWhdlav0L+z8CadJGKeuctE5pe0ftXucIDl5g76TNBFvXdp2jFnO3yfxcls6Cr26ioKKAv/f6OzN7zMTBTl3aWZ+ckkpe3XqSdfvS8HV14s1pvZjYU42ZUK5MV32yMJbrydznj2OEG/7PLwUnJ8vsuM+9ENBDm757xRiY8IbW19Fcxefh1zdg30qtmazXHTDscW25zabusqqYl357ia9OfUVn7868e/27LT9L7GVEbzSxetcZ3vg+gfIqIzMGh/HIqEg8nVViVa5cV32ykJWVOHWJwv+hh7CzVKK4IKQPzP4ZvrhPuzzz7H644YXGN2vVVJqjJYnflmtNRz2na0nCJ6zJ4RlMBjYkbeDdQ++SW5HLrB6z+FvPv+GgUye9+vySkM3CTcdIyiphWGd/5k/oSkTbK+ByaUVpgOrgbglGA2xfADvfgtD+MGVV4wd+leXBziWwZxkYyqHHbXDdnGZdwWOSJr5L+Y63D73NmaIzxPjHMPfauRZZ6vRKdSa3lOc3Hef74+fp4OvCs+OjGdW1rWpyUi57qoO7NdHZw5hF2uR7Xz2s9WNMWQUdBtb/nvJ82LUUdr+rXdba/Ra47slmrakgpST2bCxvHXyLE3kniPCKYMmIJQwPHa5OevUorTTw9o9JrIhNxl4neHJsF2YM6aiWJlWuOld9ssivyOedQ+8wJWoKnb2tvLhN91vAvyusuwNWTYAbFmtTZdc8UVcUagli11KoLILom2D43Gav0Hbg/AHePPAmB7IOEOwWzAtDXuDGsBvV5bD1MJkkGw6d5cUtJ8gqrmTyNcE8Oa6LGlinXLWu+mRhJ+z4+vTX5Ffm8+p1r1r/AwOiYdaP8L8HYMsTWj/GhNe1y1f3vKc1VVUUQpcJMPypZq/3fCLvBEsOLCH2bCx+zn480/8ZJkdOVv0SlxCXVsCCr+M5mFpATIgn793Vh97tvW0dlqLY1FWfLDydPJneZTorjqzgdM/ThHtZZ6Gbizh7wbRPIPZV+PEFbbGd0iyt6anzOK0m0ZT1oms4U3SGpQeXsiVlCx6OHjza+1Fu73p7611rohXILq7kla0n+Hx/Or6uTrx8awy39g7Bzk410SmK6uAG8iryGLt+LKPaj2Lx0MUWjqwBCd/Bhr9p/RkjnoLgPs3a3bnSc7wX9x4bkjbgqHPkzq53cm/3e/Fw9LBQwJZToTey5WgmGw5m4OnswJAIPwZH+hHs1bIJrcpgYtXOFJZsT6Rcb2TGkDAeHhmBextV+1KufKqD+0/waePD1KiprD62mgd6PkAHD+usjlanzmPgiaQ/rtj2J+VX5LPiyArWnliLCRNTo6YyK2YWfs5+FgrUcpKySlj7WyrrD6RTUKYn1MeZCr2JjXEZAIT5uTI4wpchEX4MDPez+JoPJZUGjqQXEpdeQFxaAfvO5JNdXMnwKH+enRBNJzX6WlH+QNUszHLKcxi7fizjwsbx/ODnLRiZdemNej6M/5CVR1dSbihnQvgEHuz1IMFull28p7kqDUa+PXqOT/aksic5D3s7wQ3d2nF7//YMDPdFCEjMKmFHYg6/JuWw+3QupVVG7AT0CPZkcIQfQyL86N3B+0/Nt1RlMHHyXDGHzInhcHoBiVklXPjad/B1ISbEi8nXBDOii+WmJ1eUy0VjaxYqWdTw4m8vsu7EOjZN3tTqTrZ1OV1wmrmxczmed5yRoSN5pPcjdPKy3AyqlpCcU8ra31L5Yn86eaVVhPo4M71fe6b0CcXfvf5BkHqjibi0AnYkacnjYGoBBpPEyd6OfmE+1ckjOtCjuk9BSklKbhlxaQUcSisgLr2A+IwiqgwmAHxdHekZ6kXPEC96hnrSM8QLb9cmDJBUlCuIShZNcL70POO+HMdNETcxf+B8C0VmeSZpYu2Jtby+/3Vc7F14buBzjOowytZhVasymPjumFaL2HkqF52dYHTXAG7v354hEX5N6jAuqTTwW3IuOxJz+TUph5PniwHwdnFgQLgvJZUGDqcXUliuB8DZQUePEE96hnhWJ4gQb2c1nkRRamkVfRZCiLHAm4AOWC6lfLHWdidgNdAHyAWmSilTzNtigPcBD8AEXCulrLBmvAGuAUyOnMz6xPXMjpndKteVPl96nvk757MzYydDg4eycPDCVtMvcSa3lLW/pfHF/jRySqoI9nLm8TGdua1vKG2bOT7BzcmekV0CGNklAICsogp2nsplh7nJyqONAzf2CKRXqJYcIvzd1Ip0imJBVqtZCCF0QAIwGkgH9gLTpZTHapR5EIiRUj4ghJgG3CylnCqEsAcOAHdJKeOEEL5AgZTSWN/nWWq6j4ySDMZ/OZ4pUVN4uv/Tzd6fJW1N2crCXQvRm/Q83vdxm69/bTJJUnJLiUsv4MsDZ4lNzEFnJxjZpS2392/PsEh/dOqyU0Vp1VpDzaIfkCSlPG0O6FNgEnCsRplJwALz4y+At4V29hsDHJZSxgFIKXOtGOdFgtyCmBgxkfUJ65nVYxb+LrZf5ay4qpjFexbz9emv6eHXgxeGvNDiK9aVVxk5ca6IY5lFHMso4nhmESfOFVNWpeXvIM82PHZ9Z6ZeG0o7TzXKWVGuNNZMFsFAWo3n6UD/+spIKQ1CiELAF+gMSCHEVsAf+FRK+XLtDxBCzAZmA7Rv3/Qpumu7v/v9fJX0FR/Ff8QT1z5hsf02xd5ze5m3Yx5ZZVk82PNB7o+53+prTGQVV3AsQ0sMxzOLOZZRSHJOKSZzJdS9jT3RgR5MvTaUroEeRAd60DXQQ9UiFOUKZs1kUdeZo3abV31l7IEhwLVAGbDdXFXaflFBKZcBy0Brhmp2xGahHqHcGHYjn538jBndZ+Dr7GupXTdalbGKtw++zUfxH9Heoz2rx60mxj/G4p9TaTDy44lsDqUVVNcackoqq7eHeDsTHejBhJggooO0xKA6ihXl6mPNZJEOhNZ4HgJk1FMm3dxP4QnkmV//WUqZAyCE2Az0BrbTQmbFzGLT6U2sPraax/o8ZrXPKa00sDEug2AvZwZ28sVBZ0dCfgJPxT5FQn4Ct3W+jX/2/ScuDi4W/dwT54pYtzeNDQfPkl+mx0En6BzgzvAof6IDPYgO8qBrOw+LD4hTFOXyZM1ksReIFEKEAWeBacDttcpsBO4BdgG3Aj9IKS80P80RQrgAVcB1wOtWjPUPwjzDGNtxLJ+e+JT7ut2HVxsvi+6/ymBi7W+pvPVDIjklVQB4uuiIijxEov4zPJ08WDpqKcNChlnsM4sr9Gw6nMmne9OISyvAQScY060dU/uGMiDcF0d7dfWQoih1s1qyMPdBPARsRbt0dqWUMl4IsRDYJ6XcCKwAPhZCJKHVKKaZ35svhPgPWsKRwGYp5TfWirU+s2JmsSVlC/89/l8euuYhi+zTZJJ8fTiD175LIDWvjP5hPrxzRxTJ+eksPbqI41XH0RdHYyqYymZ7LyjLrq5xNIWUkv1n8lm3N41NhzMp1xvpHODGsxOiufmaYHzUoDRFURpBDcprwGM/PsbuzN1svXVrsybjk1Lyc0I2L397kmOZRXQN9GDO2CiGd/ZnS/IWFu1ehFEa+WefOXgaBrHl6Dm2HTtPaZURbxcHbujWjht7BDY6ceSUVPLlgXTW7U3jVHYpro46JvYK4ra+ofQK9VJ9DoqiAGoEt8WcyDvBlK+n8Pdef+eBng80aR8HU/N56dsT7D6dR6iPM4+PieIvMUHY2QnWHF/Di7+9SC//Xrww9AVC3X/v5qnQG/klIZtvjmTyfSMSh9Ek+SUhm3V70/j++HkMJkmfDt5MvTaU8T0CcXVS80YqinIxlSws6OHtD3Mw+yBbb9mKq4Nro9+XlFXCq1tP8m38OfzcHHl4ZCTT+7Wv7hv4POFzFu5ayMjQkbw6/NVLXhJboTfyc0I2m+tIHNd3DSAuvYAv9qeTWViBr6sjk3sHM/XaUCLaujf771cU5cqlkoUFHc05yvRvpvNo70eZ2WNmg+UzC8t58/tEPtuXhrODjtnDOjFzaBhuNX7Zf5X0Fc/8+gxDg4fyxog3cNQ1vu+grsRhJ2BYZ3+m9g1lVNcA1VmtKEqjtIYR3JcFk0mSU1qJl7NjvSfY7n7dGRw0mFXxq5jeZXq9l7EWlFXx7k+n+GhnCiYpuWdQRx4aEYGv28Wzq24+vZn5O+czIHAAr494/U8lCoA2Djpu6NaOG7q1o0JvZF9KPuH+rgS18KJBiqJcPa76ZFFYrqffv7XhG66OOrxcHPFycdBuzr8/DhJ/4dfKX/l37IfcFD4dbxcHPM3bDUbJhzuTee+nUxRXGri5VzCPje5MqM8fk8q2M9t4esfT9G7bmyUjl+Ckq3+a7sZo46BjSGTrmEhQUZQr11WfLBzt7Xj+pu4UlFZRUK6noExPQZn2+HhhEYVlegrK9RhN4Ny+ExuS17BmWzDI3/sXHHQCvVEysktbnrghiq6BdV819VPaT8z5eQ49/HqwdNRStR62oiiXjas+Wbg62XPXgEsvoyqlpLjSQGyqB3N3PciMsVn08pqgJZUyPSWVBkZ1aUv/8PqnBdlxdgf/99P/0cWnC+9c/47FR2QriqJY01WfLBpDCKGtlxA5hM9O9ebnrHXMHXJvo/sa9mTu4dEfH6WTVyfeG/0e7o7qCiVFUS4v6pKZP0EIwV97/pWssiw2JG1o1Hv2n9/Pwz88TKh7KMtGL8PTydPKUSqKolieShZ/0sDAgcT4xbDy6Er0Jv0ly8Zlx/Hg9w8S4BLAB2M+wLuNdwtFqSiKYlkqWfxJF2oXZ0vOsunUpnrLxefG87dtf8PX2ZflY5a3mqVPFUVRmkIliyYYGjyUrj5d+eDIBxhMhj9sP5l3kr9u+yvuju6sGLOCANcAG0SpKIpiOSpZNMGF2kVacRpbkrdctO1UwSlmb5uNk86J5TcsJ9At0EZRKoqiWI5KFk00InQEkd6RfHDkA4wmbR3qlMIU7v/ufuyEHSvGrLhoUkBFUZTLmUoWTWQn7JgdM5vkwmS2pW4jrTiNmd/NxCRNLB+znI6eHW0doqIoisWocRbNMLr9aMI8w1h6cCl6k55KYyUrxqygk1cnW4emKIpiUapm0Qw6Ox2zY2aTUpRCUWUR749+nyifKFuHpSiKYnGqZtFMYzuOJaUwhRGhI+jm283W4SiKoliFShbNZG9nb7H1uRVFUVor1QylKIqiNEglC0VRFKVBKlkoiqIoDbJqshBCjBVCnBRCJAkh5tax3UkIsc68fY8QomOt7e2FECVCiMetGaeiKIpyaVZLFkIIHbAUGAdEA9OFENG1is0E8qWUEcDrwEu1tr8ObEFRFEWxKWvWLPoBSVLK01LKKuBTYFKtMpOAVebHXwCjhBACQAhxE3AaiLdijIqiKEojWDNZBANpNZ6nm1+rs4yU0gAUAr5CCFfgSeBfl/oAIcRsIcQ+IcS+7OxsiwWuKIqiXMyayULU8ZpsZJl/Aa9LKUsu9QFSymVSyr5Syr7+/v5NDFNRFEVpiDUH5aUDNaddDQEy6imTLoSwBzyBPKA/cKsQ4mXACzAJISqklG/X92H79+/PEUKcseQfYGF+QI6tg7gEFV/zqPiaR8XXPM2Jr0NjClkzWewFIoUQYcBZYBpwe60yG4F7gF3ArcAPUkoJDL1QQAixACi5VKIAkFK26qqFEGKflLKvreOoj4qveVR8zaPia56WiM9qyUJKaRBCPARsBXTASillvBBiIbBPSrkRWAF8LIRIQqtRTLNWPIqiKErTWXVuKCnlZmBzrdfm13hcAUxpYB8LrBKcoiiK0mhqBHfLWWbrABqg4mseFV/zqPiax+rxCa2LQFEUTWAX+wAABTBJREFURVHqp2oWiqIoSoNUslAURVEapJKFhQghQoUQPwohjgsh4oUQ/6ijzHAhRKEQ4pD5Nr+ufVk5zhQhxBHz5++rY7sQQiwxT+54WAjRuwVji6pxbA4JIYqEEI/WKtOix1AIsVIIkSWEOFrjNR8hxDYhRKL53rue995jLpMohLinBeN7RQhxwvzv9z8hhFc9773kd8GK8S0QQpyt8W94Yz3vveREpFaMb12N2FKEEIfqeW9LHL86zys2+Q5KKdXNAjcgEOhtfuwOJADRtcoMBzbZOM4UwO8S229Em7xRAAOAPTaKUwecAzrY8hgCw4DewNEar70MzDU/ngu8VMf7fNDmNvMBvM2PvVsovjGAvfnxS3XF15jvghXjWwA83oh//1NAOOAIxNX+/2St+Gptfw2Yb8PjV+d5xRbfQVWzsBApZaaU8oD5cTFwnD/OhXU5mASslprdgJcQItAGcYwCTkkpbToqX0r5C9oYoJpqToC5CripjrfeAGyTUuZJKfOBbcDYlohPSvmd1OZaA9iNNnuCTdRz/BqjMRORNtul4jNPanobsNbSn9tYlzivtPh3UCULKzCvy3ENsKeOzQOFEHFCiC1CiG4tGphGAt8JIfYLIWbXsb0xE0C2hGnU/5/U1scwQEqZCdp/ZqBtHWVay3GcQf3T/Df0XbCmh8zNZCvraUJpDcdvKHBeSplYz/YWPX61zist/h1UycLChBBuwHrgUSllUa3NB9CaVXoCbwEbWjo+YLCUsjfaOiN/F0IMq7W9MRNAWpUQwhGYCHxex+bWcAwbozUcx3mAAVhTT5GGvgvW8i7QCegFZKI19dRm8+MHTOfStYoWO34NnFfqfVsdrzX5GKpkYUFCCAe0f9A1Usova2+XUhZJ80y6Uhvd7iCE8GvJGKWUGeb7LOB/aNX9mhozAaS1jQMOSCnP197QGo4hcP5C05z5PquOMjY9jubOzAnAHdLcgF1bI74LViGlPC+lNEopTcAH9XyurY+fPTAZWFdfmZY6fvWcV1r8O6iShYWY2zdXAMellP+pp0w7czmEEP3Qjn9uC8boKoRwv/AYrSP0aK1iG4G7zVdFDQAKL1R3W1C9v+hsfQzNLkyAifn+qzrKbAXGCCG8zc0sY8yvWZ0QYizaejATpZRl9ZRpzHfBWvHV7AO7uZ7PrZ6I1FzTnIZ23FvK9cAJKWV6XRtb6vhd4rzS8t9Ba/bkX003YAhaFe8wcMh8uxF4AHjAXOYhtJX/4tA6Hge1cIzh5s+OM8cxz/x6zRgF2nK4p4AjQN8WjtEF7eTvWeM1mx1DtKSVCejRfqnNBHyB7UCi+d7HXLYvsLzGe2cASebbfS0YXxJaW/WF7+F75rJBwOZLfRdaKL6Pzd+tw2gnvcDa8Zmf34h29c+plozP/PpHF75zNcra4vjVd15p8e+gmu5DURRFaZBqhlIURVEapJKFoiiK0iCVLBRFUZQGqWShKIqiNEglC0VRFKVBKlkoihUJITrWnNFUUS5XKlkoiqIoDVLJQlFaiBAiXAhxUAhxra1jUZQ/SyULRWkBQogotPl97pNS7rV1PIryZ9nbOgBFuQr4o83dc4uUMt7WwShKU6iahaJYXyHaXE2DbR2IojSVqlkoivVVoa1ktlUIUSKl/MTWASnKn6WShaK0ACllqRBiArBNCFEqpaxrSmlFabXUrLOKoihKg1SfhaIoitIglSwURVGUBqlkoSiKojRIJQtFURSlQSpZKIqiKA1SyUJRFEVpkEoWiqIoSoP+H2/jK8VUxZjMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "folds = cross_val_evaluate(X_train , 5)\n",
    "\n",
    "def runFold_mse(data):\n",
    "    \n",
    "    X_train_subset, y_train_subset, X_val, y_val = data \n",
    "    empty_mse = np.zeros(20)\n",
    "    \n",
    "    for k in range(1,21):\n",
    "            y_pred_kNN = reg_predict(X_val, X_train_subset, y_train_subset, k)\n",
    "            mse_kNN = mse(y_val, y_pred_kNN) # calculating mse\n",
    "            empty_mse[k-1] = mse_kNN # appending mse values     \n",
    "    plt.plot(range(1,21), empty_mse)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"MSE against k parameter in kNN for 5 folds\")\n",
    "    return empty_mse\n",
    "results = map(runFold_mse, folds) # applying runFold_mse over the 5 folds\n",
    "m = np.array(list(results))\n",
    "msemean = np.mean(m, axis=0)\n",
    "optimal_k = msemean.argmin() + 1 # add 1 for index  \n",
    "print(\"The optimal k is\", optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample MSE: 3.970810643564356\n",
      "Out-of-sample MSE: 18.544779411764708\n"
     ]
    }
   ],
   "source": [
    "# MSE in-sample\n",
    "y_pred_train_kNN = reg_predict(X_train, X_train, y_train_r, optimal_k)\n",
    "MSE_in_sample_kNN = mse(y_train_r, y_pred_train_kNN)\n",
    "print(\"In-sample MSE:\", MSE_in_sample_kNN)\n",
    "\n",
    "# MSE out-of-sample\n",
    "y_pred_test_kNN = reg_predict(X_test, X_train, y_train_r, optimal_k)\n",
    "MSE_out_of_sample_kNN = mse(y_test_r, y_pred_test_kNN)\n",
    "print(\"Out-of-sample MSE:\", MSE_out_of_sample_kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN seems to perform the best out of the 3 models because it has both the lowest in-sample and out-of-sample MSEs. However, there is quite a significant difference between in-sample and out-of-sample MSE: there is a difference of roughly 14 in kNN whereas only 2 in linear and ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Logistic Regression\n",
    "#### 2.1.1\n",
    "In this task, we train a logistic regression classifier on the training data with gradient descent for 5000\n",
    "iterations. We use a grid-search with 5-fold cross validation to find the optimal set of hyperparameters (learning rate, decision threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data as pandas data frames\n",
    "classification_train = pd.read_csv('classification_train.csv', header=None) \n",
    "classification_test = pd.read_csv('classification_test.csv', header=None) \n",
    "\n",
    "X_train_c = np.array(classification_train.iloc[:,:-1]) \n",
    "X_train_c_T = np.array(classification_train.iloc[:,:-1].T) \n",
    "y_train_c = np.array(classification_train.iloc[:,-1])\n",
    "X_test_c = np.array(classification_test.iloc[:,:-1])\n",
    "y_test_c = np.array(classification_test.iloc[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the logistic function\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log(X, beta, beta_0):\n",
    "  y_log = logistic(beta.T @ X + beta_0) \n",
    "  return y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function initialises the beta terms to zeros\n",
    "def initialise(d):\n",
    "  \"\"\"    \n",
    "  Argument:\n",
    "  d: size of the beta vector (or number of parameters)\n",
    "  \n",
    "  Returns:\n",
    "  beta: initialised vector of shape (d, 1)\n",
    "  beta_0: initialised scalar (corresponds to the offset)\n",
    "  \"\"\"\n",
    "  \n",
    "  beta = np.zeros(shape=(d, 1), dtype=np.float32)\n",
    "  beta_0 = 0\n",
    "  \n",
    "  assert(beta.shape==(d, 1))\n",
    "  assert(isinstance(beta_0, float) or isinstance(beta_0, int))\n",
    "  \n",
    "  return beta, beta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(X, y, beta, beta_0):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: data of size (d, n)\n",
    "    y: true label vector of size (1, n)\n",
    "    beta: parameters, a numpy array of size (d, 1)\n",
    "    beta_0: offset, a scalar\n",
    "  \n",
    "    Returns:\n",
    "    cost: negative log-likelihood cost for logistic regression\n",
    "    dbeta: gradient of the loss with respect to beta\n",
    "    dbeta_0: gradient of the loss with respect to beta_0\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    y_log = predict_log(X, beta, beta_0) #.T\n",
    "    y_log.shape\n",
    "    y.shape\n",
    "\n",
    "  # cost function\n",
    "    cost = (-1) * np.mean(np.multiply(y, np.log(y_log)) + np.multiply(1-y, np.log(1 - y_log)), axis=1)\n",
    "\n",
    "  # derivatives\n",
    "    dbeta = (1/n) * X @ np.transpose(y_log - y)  # dbeta = (1/n) * X @ np.transpose(y_log - y)\n",
    "    dbeta_0 = np.mean(y_log - y)\n",
    "    \n",
    "    assert(dbeta.shape==beta.shape)\n",
    "    assert(dbeta_0.dtype==float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape==())\n",
    "  \n",
    "  # store gradients in a dictionary\n",
    "    grads = {\"dbeta\": dbeta, \"dbeta_0\": dbeta_0}\n",
    "  \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent with 5000 itration\n",
    "def optimise(X, y, beta, beta_0, learning_rate, num_iterations=5000, print_cost=False):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  X: data of size (d, n)\n",
    "  y: true label vector of size (1, n)\n",
    "  beta: parameters, a numpy array of size (d, 1)\n",
    "  beta_0: offset, a scalar\n",
    "  num_iterations: number of iterations gradient descent shall update the parameters\n",
    "  learning_rate: step size in updating procedure\n",
    "  print_cost: whether to print the cost every 100 iterations or not\n",
    "\n",
    "  Returns:\n",
    "  params: dictionary containing the parameters beta and offset beta_0\n",
    "  grads: dictionary containing the gradients\n",
    "  costs: list of all the costs computed during the optimisation (can be used to plot the learning curve).\n",
    "  \"\"\"\n",
    "  costs = []\n",
    "    \n",
    "  for i in range(num_iterations):\n",
    "\n",
    "      # calculate cost and gradients\n",
    "      grads, cost = propagate(X, y, beta, beta_0)\n",
    "      \n",
    "      # retrieve derivatives from grads\n",
    "      dbeta = grads[\"dbeta\"]\n",
    "      dbeta_0 = grads[\"dbeta_0\"]\n",
    "      \n",
    "      # updating procedure\n",
    "      beta = beta - learning_rate * dbeta \n",
    "      beta_0 = beta_0 - learning_rate * dbeta_0\n",
    "      \n",
    "      # record the costs\n",
    "      if i % 100 == 0:\n",
    "          costs.append(cost)\n",
    "      \n",
    "      # print the cost every 100 iterations\n",
    "      if print_cost and i % 100 == 0:\n",
    "          print (\"cost after iteration %i: %f\" %(i, cost))\n",
    "  \n",
    "  # save parameters and gradients in dictionary\n",
    "  params = {\"beta\": beta, \"beta_0\": beta_0}\n",
    "  grads = {\"dbeta\": dbeta, \"dbeta_0\": dbeta_0}\n",
    "  \n",
    "  return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, decision_threshold, beta, beta_0):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  X_test: test data of size (d, n)\n",
    "  beta: parameters, a numpy array of size (d, 1)\n",
    "  beta_0: offset, a scalar\n",
    "\n",
    "  Returns:\n",
    "  y_pred: vector containing all binary predictions (0/1) for the examples in X_test\n",
    "  \"\"\"\n",
    "  n = X_test.shape[1]\n",
    "  y_pred = np.zeros((1,n))\n",
    "  beta = beta.reshape(X_test.shape[0], 1)\n",
    "  \n",
    "  # compute vector y_log predicting the probabilities\n",
    "  y_log = predict_log(X_test, beta, beta_0)\n",
    "  \n",
    "  for i in range(y_log.shape[1]):\n",
    "      \n",
    "      # convert probabilities y_log to actual predictions y_pred\n",
    "      if y_log[0, i] > decision_threshold:\n",
    "          y_pred[0, i] = 1 \n",
    "      else:\n",
    "          y_pred[0, i] = 0\n",
    "  \n",
    "  assert(y_pred.shape==(1, n))\n",
    "  \n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test, learning_rate, decision_threshold, num_iterations=5000, print_cost=False):\n",
    "  # initialize parameters with zeros\n",
    "  beta, beta_0 = initialise(X_train.shape[0])\n",
    "\n",
    "  # gradient descent\n",
    "  parameters, grads, costs = optimise(X_train, y_train, beta, beta_0, learning_rate, num_iterations, print_cost=False)\n",
    "\n",
    "  # retrieve parameters beta and beta_0 from dictionary \"parameters\"\n",
    "  beta = parameters[\"beta\"]\n",
    "  beta_0 = parameters[\"beta_0\"]\n",
    "\n",
    "  # predict test and train set examples\n",
    "  y_pred_test = predict(X_test, decision_threshold, beta, beta_0)\n",
    "  y_pred_train = predict(X_train, decision_threshold, beta, beta_0)\n",
    "\n",
    "  train_accuracy = 100 - np.mean(np.abs(y_pred_train - y_train)) * 100\n",
    "  test_accuracy = 100 - np.mean(np.abs(y_pred_test - y_test)) * 100\n",
    "\n",
    "  # print train/test Errors\n",
    "  #print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100))\n",
    "  #print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))\n",
    "\n",
    "  # saving all information\n",
    "  d = {\"costs\": costs, \"y_pred_test\": y_pred_test, \"y_pred_train\": y_pred_train, \"beta\": beta, \"beta_0\": beta_0, \"learning_rate\": learning_rate, \"num_iterations\": num_iterations, \"train_accuracy\": train_accuracy, \"test_accuracy\": test_accuracy}\n",
    "  \n",
    "  return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use a grid search which is a tool to find the optimal hyperparameters. This methods works by considering different combinations of parameters and chooses the one that will return the lowest error. The two hyperparameter we are optimizing are the learning rate and the decision threshold. I chose learning rates and decision thresholds that range between 0 and 1. Based on what I have seen in the coding tasks and in literature, these seem like resonable ranges. We will therefore have a 10 x 10 grid of possible hyperparameter values and we will look for the pair that has the smallest mean squared error. This will correspond to the optimal hyperparameters. I show this example for one of the folds below and then I do it over the 5 folds in the cell after. These code chuncks do take a bit of time to compute (especially the second one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning_rate choices     \n",
    "learning_rates = np.linspace(0,1,11)\n",
    "      \n",
    "# decision threshold choices     \n",
    "decision_thresholds  = np.linspace(0,1,11)\n",
    "\n",
    "accuracies = np.zeros((11,11))\n",
    "folds = cross_val_evaluate(classification_train , 5)\n",
    "X_train_subset, y_train_subset, X_val, y_val = folds[4]\n",
    "\n",
    "for i in range(len(learning_rates)):         \n",
    "    for j in range(len(decision_thresholds)): \n",
    "        d = model(X_train_subset.T, y_train_subset, X_val.T, y_val, learning_rate=learning_rates[i], decision_threshold=decision_thresholds[j], num_iterations=5000, print_cost=False)\n",
    "        accuracies[i,j] = d[\"test_accuracy\"]\n",
    "\n",
    "# plot heatmap\n",
    "import seaborn as sns\n",
    "fix, ax = plt.subplots(figsize=(9, 6))\n",
    "ax = sns.heatmap(accuracies, vmin=0, vmax=100, annot=True)\n",
    "ax.set_title('Hyperparameter Gridsearch', fontsize=16)\n",
    "ax.set_ylabel('Decision Threshold', fontsize=12)\n",
    "ax.set_xlabel('Learning Rate', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal learning rate is 0.1\n",
      "The optimal decision threshold is 0.5\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = []\n",
    "for i in learning_rates:\n",
    "    for j in decision_thresholds:\n",
    "        hyperparameters.append((i,j)) \n",
    "\n",
    "folds = cross_val_evaluate(classification_train , 5)\n",
    "\n",
    "def optimalhyperparameters(data):\n",
    "    \n",
    "    X_train_subset, y_train_subset, X_val, y_val = data \n",
    "    accuracies = np.zeros(len(hyperparameters))   \n",
    "    \n",
    "    for i in range(len(hyperparameters)):\n",
    "        d = model(X_train_subset.T, y_train_subset, X_val.T, y_val, learning_rate=hyperparameters[i][0], decision_threshold=hyperparameters[i][1], num_iterations=5000, print_cost=False)\n",
    "        accuracies[i] = d[\"test_accuracy\"]\n",
    "    return accuracies\n",
    "results = map(optimalhyperparameters, folds)\n",
    "matrix_hyperparameters = np.array(list(results))\n",
    "mean_hyperparameters = np.mean(matrix_hyperparameters, axis=0)\n",
    "optimal_hyperparameters = mean_hyperparameters.argmax() \n",
    "print(\"The optimal learning rate is\", hyperparameters[optimal_hyperparameters][0])\n",
    "print(\"The optimal decision threshold is\", hyperparameters[optimal_hyperparameters][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2\n",
    "Compare the performance of your optimal model on the training data and on the test data by\n",
    "their mean accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 74.25\n",
      "test accuracy: 74.0\n"
     ]
    }
   ],
   "source": [
    "X_train_c = np.array(classification_train.iloc[:,:-1]) \n",
    "y_train_c = np.array(classification_train.iloc[:,-1])\n",
    "X_test_c = np.array(classification_test.iloc[:,:-1])\n",
    "y_test_c = np.array(classification_test.iloc[:,-1])\n",
    "\n",
    "d = model(X_train_c.T, y_train_c, X_test_c.T, y_test_c, num_iterations=5000, learning_rate=0.1, decision_threshold=0.5, print_cost=False)\n",
    "      \n",
    "print(\"train accuracy:\", d[\"train_accuracy\"])\n",
    "print(\"test accuracy:\", d[\"test_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the test accuracy is slighly lower than the train accuracy, this is because we have trained the model with the training data hence the optimal hyperparameters are more adapted to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random forest\n",
    "#### 2.2.1\n",
    "Train a random forest classifier on the training data. You should use the same 5-fold cross\n",
    "validation subsets to explore and optimise over suitable ranges of the following hyperparameters: (i)\n",
    "number of decision trees; (ii) depth of trees, (iii) maximum number of descriptors (features) randomly\n",
    "chosen at each split. Use cross-entropy as your information criterion for the splits.\n",
    "\n",
    "\n",
    "The cross entropy equation I used is from the book \"Elements of Statistical Learning\" given by:\n",
    "$$-\\sum_{k=1}^K\\hat{p}_{mk}\\log{\\hat{p}_{mk}}$$\n",
    "\n",
    "where $\\hat{p}_{mk}$ is the proportion of class k observations in node m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c = np.array(classification_train.iloc[:,:-1]) \n",
    "y_train_c = np.array(classification_train.iloc[:,-1])\n",
    "X_test_c = np.array(classification_test.iloc[:,:-1])\n",
    "y_test_c = np.array(classification_test.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy as the information criterion for the splits\n",
    "def cross_entropy(y, sample_weights=None):\n",
    "  \"\"\" \n",
    "  Calculate the cross-entropy for labels.\n",
    "  Arguments:\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Returns:\n",
    "      (float): the cross-entropy for y.\n",
    "  \"\"\"\n",
    "  if sample_weights is None:\n",
    "      sample_weights = np.ones(y.shape[0]) / y.shape[0]\n",
    "  \n",
    "  cross_entropy = 0\n",
    "  num = y.shape[0]  # number of labels\n",
    "  label_counts = {}  # caculate different labels in y，and store in label_counts\n",
    "  for i in range(num):\n",
    "      if y[i] not in label_counts.keys():\n",
    "          label_counts[y[i]] = 0\n",
    "      label_counts[y[i]] += sample_weights[i]\n",
    "  \n",
    "  for key in label_counts:\n",
    "      prob = float(label_counts[key]) / float(np.sum(sample_weights))\n",
    "      cross_entropy -= prob ** np.log2(prob)\n",
    "\n",
    "  return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, column, value, sample_weights=None):\n",
    "  Return the split of data whose column-th feature equals value.\n",
    "  Arguments:\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      column: the column of the feature for splitting.\n",
    "      value: the value of the column-th feature for splitting.\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Returns:\n",
    "      (np.array): the subset of X whose column-th feature equals value.\n",
    "      (np.array): the subset of y whose column-th feature equals value.\n",
    "      (np.array): the subset of sample weights whose column-th feature equals value.\n",
    "  \"\"\" \n",
    "  ret = []\n",
    "  featVec = X[:, column] # take one of the features of X\n",
    "  X = X[:,[i for i in range(X.shape[1]) if i!=column]] # 2 list comprehensions : = all samples != means not the same\n",
    "    # take all the colums expect the one we have defined in the featVec (feature vector)\n",
    "  \n",
    "  for i in range(len(featVec)):\n",
    "      if featVec[i] > value:\n",
    "          ret.append(i) \n",
    "      elif featVec[i] < value:\n",
    "        ret.append(i)\n",
    "  \n",
    "  sub_X = X[ret,:]\n",
    "  sub_y = y[ret]\n",
    "  sub_sample_weights = sample_weights[ret]\n",
    "\n",
    "  return sub_X, sub_y, sub_sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_purification(X, y, column, sample_weights=None):\n",
    "  \"\"\"\n",
    "  Calculate the resulted gini impurity given a vector of features.\n",
    "  Arguments:\n",
    "      X: training features, of shape (N, D). N is the number of samples we have and D is the dimensionality \n",
    "      y: vector of training labels, of shape (N,).\n",
    "      column: the column of the feature for calculating. 0 <= column < D\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Returns:\n",
    "      (float): the resulted gini impurity after splitting by this feature.\n",
    "  \"\"\"\n",
    "  if sample_weights is None:\n",
    "      sample_weights = np.ones(y.shape[0]) / y.shape[0] # all samples have the same weight (every sample is as influencial as the other)\n",
    "  \n",
    "  new_impurity = 0\n",
    "  old_cost = cross_entropy(y, sample_weights) # call previous function gini_impurity taking into account the uniform sample weights\n",
    "  \n",
    "  unique_vals = np.unique(X[:, column]) # take all the unique values of all samples of X (:) but only take it from the column \"column\" (only take the feature of X that we want to consider at the moment), np.unique means you only take the unique values from that\n",
    "  new_cost = 0.0\n",
    "  # split the values of i-th feature and calculate the cost \n",
    "  for value in unique_vals: # iterate over the unique values HERE PROBABLY HAVE TO CHANGE VALUE\n",
    "      sub_X, sub_y, sub_sample_weights = split_dataset(X, y, column, value, sample_weights) \n",
    "      prob = np.sum(sub_sample_weights) / float(np.sum(sample_weights)) \n",
    "      new_cost += prob * cross_entropy(sub_y, sub_sample_weights) # if some samples have a bigger weight, you want this to be reflected in the new cost which is why you multiply by prob\n",
    "  \n",
    "  new_impurity = old_cost - new_cost # information gain\n",
    "\n",
    "  return new_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.540920518035385"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate for feature in the 4th column\n",
    "cross_purification(X_train_c, y_train_c, column=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def choose_best_feature(X, y, sample_weights=None):\n",
    "  \"\"\"\n",
    "  Choose the best feature to split according to criterion.\n",
    "  Args:\n",
    "      X: training features, of shape (N, D).\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Returns:\n",
    "      (int): the column for the best feature\n",
    "  \"\"\"\n",
    "  if sample_weights is None:\n",
    "      sample_weights = np.ones(y.shape[0]) / y.shape[0]\n",
    "\n",
    "  best_feature_idx = 0\n",
    "  n_features = X.shape[1]    \n",
    "  \n",
    "  # use C4.5 algorirhm\n",
    "  best_gain_cost = 0.0\n",
    "  for i in range(n_features):\n",
    "      info_gain_cost = cross_purification(X, y, i, sample_weights)         \n",
    "      if info_gain_cost > best_gain_cost:\n",
    "          best_gain_cost = info_gain_cost\n",
    "          best_feature_idx = i                \n",
    "\n",
    "  return best_feature_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate which feature is best\n",
    "choose_best_feature(X_train_c, y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(y, sample_weights=None):\n",
    "  \"\"\"\n",
    "  Return the label which appears the most in y.\n",
    "  Args:\n",
    "      y: vector of training labels, of shape (N,).\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Returns:\n",
    "      (int): the majority label\n",
    "  \"\"\"\n",
    "  if sample_weights is None:\n",
    "      sample_weights = np.ones(y.shape[0]) / y.shape[0]\n",
    "  \n",
    "  majority_label = y[0]\n",
    "\n",
    "  dict_num = {}\n",
    "  for i in range(y.shape[0]):\n",
    "      if y[i] not in dict_num.keys():\n",
    "          dict_num[y[i]] = sample_weights[i]\n",
    "      else:\n",
    "          dict_num[y[i]] += sample_weights[i]\n",
    "  \n",
    "  majority_label = max(dict_num, key=dict_num.get)\n",
    "  # end answer\n",
    "  return majority_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate it\n",
    "majority_vote(y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS FUNCTION - probably have to chnage something here for continuous data\n",
    "def build_tree(X, y, feature_names, depth, sample_weights=None, max_depth=10, min_samples_leaf=2):\n",
    "  \"\"\"Build the decision tree according to the data.\n",
    "  Args:\n",
    "      X: (np.array) training features, of shape (N, D).\n",
    "      y: (np.array) vector of training labels, of shape (N,).\n",
    "      feature_names (list): record the name of features in X in the original dataset.\n",
    "      depth (int): current depth for this node.\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  Returns:\n",
    "      (dict): a dict denoting the decision tree. \n",
    "      Example:\n",
    "          The first best feature name is 'title', and it has 5 different values: 0,1,2,3,4. For 'title' == 4, the next best feature name is 'pclass', we continue split the remain data. If it comes to the leaf, we use the majority_label by calling majority_vote.\n",
    "          mytree = {\n",
    "              'titile': {\n",
    "                  0: subtree0,\n",
    "                  1: subtree1,\n",
    "                  2: subtree2,\n",
    "                  3: subtree3,\n",
    "                  4: {\n",
    "                      'pclass': {\n",
    "                          1: majority_vote([1, 1, 1, 1]) # which is 1, majority_label\n",
    "                          2: majority_vote([1, 0, 1, 1]) # which is 1\n",
    "                          3: majority_vote([0, 0, 0]) # which is 0\n",
    "                      }\n",
    "                  }\n",
    "              }\n",
    "          }\n",
    "  \"\"\"\n",
    "  mytree = dict()\n",
    "\n",
    "  # include a clause for the cases where (i) no feature, (ii) all lables are the same, (iii) depth exceed, or (iv) X is too small\n",
    "  if len(feature_names)==0 or len(np.unique(y))==1 or depth>=max_depth or len(X)<=min_samples_leaf: \n",
    "      return majority_vote(y, sample_weights)\n",
    "  \n",
    "  else:\n",
    "    best_feature_idx = choose_best_feature(X, y, sample_weights)  # find the index of the best feature\n",
    "    best_feature_name = feature_names[best_feature_idx]\n",
    "    feature_names = feature_names[:]\n",
    "    feature_names.remove(best_feature_name)\n",
    "    \n",
    "    mytree = {best_feature_name:{}} # define a dictionary inside a dictionary\n",
    "    unique_vals = np.unique(X[:, best_feature_idx])\n",
    "    for value in unique_vals:\n",
    "        sub_X, sub_y, sub_sample_weights = split_dataset(X, y, best_feature_idx, value, sample_weights)  ## <-- SOLUTION\n",
    "        mytree[best_feature_name][value] = build_tree(sub_X, sub_y, feature_names, depth+1, sub_sample_weights)  ## <-- SOLUTION (hint: use depth+1 as the depth)\n",
    "\n",
    "    return mytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c = classification_train.iloc[:,:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, sample_weights=None):\n",
    "  \"\"\"\n",
    "  Build the decision tree according to the training data.\n",
    "  Args:\n",
    "      X: (pd.Dataframe) training features, of shape (N, D). Each X[i] is a training sample.\n",
    "      y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n",
    "      an integer in the range 0 <= y[i] <= C. Here C = 1.\n",
    "      sample_weights: weights for each samples, of shape (N,).\n",
    "  \"\"\"\n",
    "  if sample_weights is None:\n",
    "      # if the sample weights is not provided, we assume the samples have uniform weights\n",
    "      sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "  else:\n",
    "      sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n",
    "\n",
    "  feature_names = X.columns.tolist()\n",
    "  X = np.array(X)\n",
    "  y = np.array(y)\n",
    "  tree = build_tree(X, y, feature_names, depth=1, sample_weights=sample_weights)\n",
    "  return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-501-7bcb5705356f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the decision tree with training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-500-a24eb0af698d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, y, sample_weights)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-9c4c14646ee6>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(X, y, feature_names, depth, sample_weights, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmytree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION (hint: use depth+1 as the depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-9c4c14646ee6>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(X, y, feature_names, depth, sample_weights, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmytree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION (hint: use depth+1 as the depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-9c4c14646ee6>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(X, y, feature_names, depth, sample_weights, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmytree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION (hint: use depth+1 as the depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-9c4c14646ee6>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(X, y, feature_names, depth, sample_weights, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmytree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION (hint: use depth+1 as the depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-9c4c14646ee6>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(X, y, feature_names, depth, sample_weights, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmytree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION (hint: use depth+1 as the depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-9c4c14646ee6>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(X, y, feature_names, depth, sample_weights, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmytree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION (hint: use depth+1 as the depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-9c4c14646ee6>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(X, y, feature_names, depth, sample_weights, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmytree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION (hint: use depth+1 as the depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-9c4c14646ee6>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(X, y, feature_names, depth, sample_weights, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmytree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION (hint: use depth+1 as the depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-9c4c14646ee6>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(X, y, feature_names, depth, sample_weights, max_depth, min_samples_leaf)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0munique_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_feature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mmytree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## <-- SOLUTION (hint: use depth+1 as the depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-487-23b8be7473ab>\u001b[0m in \u001b[0;36msplit_dataset\u001b[0;34m(X, y, column, value, sample_weights)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0msub_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0msub_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0msub_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msub_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the decision tree with training data\n",
    "tree = train(X_train_c, y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x):\n",
    "  \"\"\"\n",
    "  Classify a single sample with the fitted decision tree.\n",
    "  Args:\n",
    "      x: ((pd.Dataframe) a single sample features, of shape (D,).\n",
    "  Returns:\n",
    "      (int): predicted testing sample label.\n",
    "  \"\"\"\n",
    "  feature_name = list(tree.keys())[0] # first element\n",
    "  second_dict = tree[feature_name]            \n",
    "  key = x.loc[feature_name]\n",
    "  if key not in second_dict:\n",
    "      key = np.random.choice(list(second_dict.keys()))\n",
    "  value_of_key = second_dict[key]\n",
    "  if isinstance(value_of_key, dict):\n",
    "      label = classify(value_of_key, x)\n",
    "  else:\n",
    "      label=value_of_key\n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree,X):\n",
    "  \"\"\"\n",
    "  Predict classification results for X.\n",
    "  Args:\n",
    "      X: (pd.Dataframe) testing sample features, of shape (N, D).\n",
    "  Returns:\n",
    "      (np.array): predicted testing sample labels, of shape (N,).\n",
    "  \"\"\"\n",
    "  if len(X.shape)==1:\n",
    "      return classify(tree, X)\n",
    "  else:\n",
    "      results=[]\n",
    "      for i in range(X.shape[0]):\n",
    "          results.append(classify(tree, X.iloc[i, :]))\n",
    "      return np.array(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the decision tree algorithm, we can move onto building the random forest algorithm. I begin with the bootstrapping step where I sample with replacement to introduce some randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the bootstrapped data frame\n",
    "def bootstrapping(training_dataset, B): # B is the number of sample\n",
    "    bootstrap_indeces = np.random.randint(low = 0, high = len(training_dataset), size = B)\n",
    "    bootstrap_df = training_dataset.iloc[bootstrap_indeces]\n",
    "    return bootstrap_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.067899</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.071641</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.534280</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.107131</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.240618</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.063772</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.111643</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.114614</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.063387</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.027512</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.087763</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.028942</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.064488</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.244085</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.182569</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.362716</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.302245</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.064488</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.029713</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.195554</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.150105</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.288489</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.643117</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.112854</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>0.029412</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.043854</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.138935</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.134643</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.148949</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.029412</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.033564</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.030098</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.774073</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.057280</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.183119</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.150820</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.036261</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.068945</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.073529</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.037746</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.036261</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.073787</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.030098</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.088235</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.103555</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.138054</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.093540</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.111313</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.072466</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.027017</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.069440</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.182569</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.138495</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.275008</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.109387</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.137284</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.124408</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.231374</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.246836</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.377958</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1         2     3     4         5         6         7   \\\n",
       "288  0.117647  0.50  0.067899  1.00  0.50  0.333333  0.000000  0.000000   \n",
       "771  0.294118  0.50  0.071641  0.00  0.75  0.666667  0.000000  0.666667   \n",
       "632  0.647059  0.50  0.534280  0.00  0.25  0.000000  0.333333  0.666667   \n",
       "624  0.294118  1.00  0.107131  1.00  0.75  1.000000  1.000000  0.666667   \n",
       "768  0.161765  0.50  0.240618  0.25  0.50  0.666667  0.333333  0.333333   \n",
       "454  0.117647  1.00  0.063772  0.00  1.00  1.000000  0.666667  0.000000   \n",
       "702  0.117647  0.50  0.111643  1.00  0.50  1.000000  1.000000  1.000000   \n",
       "365  0.382353  0.50  0.114614  0.50  1.00  1.000000  0.333333  0.666667   \n",
       "741  0.117647  1.00  0.063387  0.50  0.75  0.666667  1.000000  0.666667   \n",
       "302  0.205882  0.50  0.027512  0.00  0.00  1.000000  0.000000  0.000000   \n",
       "735  0.602941  0.50  0.087763  0.00  0.50  1.000000  1.000000  1.000000   \n",
       "138  0.117647  0.50  0.028942  0.00  0.50  1.000000  0.333333  0.000000   \n",
       "17   0.294118  0.50  0.142236  0.50  1.00  0.666667  1.000000  0.333333   \n",
       "564  0.073529  0.50  0.064488  0.00  0.25  0.666667  0.333333  1.000000   \n",
       "587  0.470588  0.50  0.244085  0.00  0.50  0.333333  0.333333  1.000000   \n",
       "179  0.161765  0.50  0.182569  0.00  1.00  1.000000  0.333333  0.666667   \n",
       "723  0.294118  1.00  0.362716  1.00  0.50  0.333333  1.000000  0.333333   \n",
       "208  0.294118  1.00  0.302245  0.00  0.25  0.333333  1.000000  1.000000   \n",
       "564  0.073529  0.50  0.064488  0.00  0.25  0.666667  0.333333  1.000000   \n",
       "726  0.073529  0.50  0.029713  0.50  0.50  1.000000  0.666667  0.000000   \n",
       "222  0.470588  0.00  0.195554  0.00  0.50  1.000000  0.000000  0.666667   \n",
       "187  0.294118  0.75  0.150105  1.00  0.50  1.000000  1.000000  0.000000   \n",
       "543  0.470588  0.50  0.288489  0.00  1.00  0.333333  1.000000  1.000000   \n",
       "614  0.294118  1.00  0.643117  0.00  0.50  0.333333  0.666667  0.666667   \n",
       "418  0.073529  0.50  0.112854  0.25  0.25  0.333333  1.000000  0.333333   \n",
       "635  0.029412  1.00  0.043854  0.00  0.50  0.333333  1.000000  0.333333   \n",
       "22   0.205882  1.00  0.138935  0.00  0.75  0.333333  0.333333  0.333333   \n",
       "688  0.073529  0.50  0.134643  0.00  0.50  0.000000  0.333333  0.000000   \n",
       "77   0.294118  1.00  0.148949  0.00  1.00  1.000000  1.000000  0.333333   \n",
       "311  0.470588  1.00  0.512600  0.00  0.50  0.333333  0.333333  0.666667   \n",
       "..        ...   ...       ...   ...   ...       ...       ...       ...   \n",
       "470  0.029412  1.00  0.033564  0.00  1.00  0.000000  1.000000  1.000000   \n",
       "350  0.117647  1.00  0.030098  1.00  1.00  1.000000  0.666667  0.333333   \n",
       "519  0.470588  0.50  0.774073  0.00  1.00  1.000000  0.333333  1.000000   \n",
       "61   0.117647  1.00  0.057280  0.00  0.50  1.000000  0.333333  0.333333   \n",
       "609  0.647059  1.00  0.183119  1.00  1.00  1.000000  0.000000  0.000000   \n",
       "727  0.382353  0.50  0.150820  1.00  1.00  0.333333  1.000000  0.666667   \n",
       "626  0.470588  0.50  0.036261  0.50  1.00  1.000000  1.000000  0.333333   \n",
       "89   0.000000  1.00  0.068945  0.00  0.75  0.333333  0.000000  0.000000   \n",
       "186  0.073529  1.00  0.037746  0.50  1.00  1.000000  0.333333  0.666667   \n",
       "626  0.470588  0.50  0.036261  0.50  1.00  1.000000  1.000000  0.333333   \n",
       "657  0.250000  0.75  0.073787  0.25  0.75  1.000000  0.666667  0.000000   \n",
       "350  0.117647  1.00  0.030098  1.00  1.00  1.000000  0.666667  0.333333   \n",
       "214  0.058824  1.00  0.026466  0.00  1.00  1.000000  1.000000  0.000000   \n",
       "141  0.088235  1.00  0.103555  1.00  0.25  0.333333  0.666667  0.000000   \n",
       "2    0.117647  0.00  0.138054  0.00  1.00  0.333333  1.000000  0.333333   \n",
       "66   0.205882  0.50  0.093540  0.00  0.75  1.000000  0.000000  0.666667   \n",
       "322  0.470588  0.50  0.111313  0.00  0.75  0.666667  0.000000  0.666667   \n",
       "239  0.117647  0.50  0.072466  0.00  0.50  0.000000  0.000000  0.666667   \n",
       "634  0.117647  0.50  0.027017  0.25  0.00  1.000000  0.666667  0.333333   \n",
       "117  0.161765  0.75  0.069440  0.75  0.50  0.666667  0.666667  0.333333   \n",
       "179  0.161765  0.50  0.182569  0.00  1.00  1.000000  0.333333  0.666667   \n",
       "578  0.250000  0.50  0.138495  0.25  1.00  1.000000  0.333333  0.666667   \n",
       "403  0.250000  0.50  0.275008  1.00  0.50  0.000000  0.666667  0.666667   \n",
       "477  0.117647  0.50  0.055684  0.00  0.50  0.666667  0.333333  0.666667   \n",
       "245  0.205882  1.00  0.109387  0.00  0.50  0.333333  0.000000  0.666667   \n",
       "413  0.250000  1.00  0.137284  0.75  0.75  0.666667  0.333333  0.666667   \n",
       "127  0.161765  0.50  0.124408  0.00  0.00  0.000000  1.000000  0.666667   \n",
       "238  0.470588  0.75  0.231374  0.00  0.50  0.333333  0.333333  0.000000   \n",
       "102  0.294118  1.00  0.246836  0.00  0.25  0.333333  1.000000  0.666667   \n",
       "548  0.647059  0.00  0.377958  0.00  0.50  0.666667  1.000000  1.000000   \n",
       "\n",
       "           8         9    10   11  \n",
       "288  0.107143  0.666667  1.0  0.0  \n",
       "771  0.232143  0.666667  0.0  1.0  \n",
       "632  0.125000  0.666667  1.0  0.0  \n",
       "624  0.428571  0.666667  1.0  1.0  \n",
       "768  0.375000  1.000000  1.0  0.0  \n",
       "454  0.625000  0.666667  0.0  1.0  \n",
       "702  0.321429  0.666667  1.0  1.0  \n",
       "365  0.196429  1.000000  0.0  1.0  \n",
       "741  0.321429  0.666667  1.0  1.0  \n",
       "302  0.142857  0.000000  0.0  0.0  \n",
       "735  0.071429  0.666667  1.0  0.0  \n",
       "138  0.160714  0.666667  0.0  1.0  \n",
       "17   0.607143  0.666667  0.0  1.0  \n",
       "564  0.142857  1.000000  1.0  0.0  \n",
       "587  0.232143  1.000000  1.0  1.0  \n",
       "179  0.625000  1.000000  1.0  1.0  \n",
       "723  0.482143  1.000000  1.0  1.0  \n",
       "208  0.089286  0.666667  1.0  1.0  \n",
       "564  0.142857  1.000000  1.0  0.0  \n",
       "726  0.839286  0.333333  0.0  1.0  \n",
       "222  0.410714  0.666667  1.0  0.0  \n",
       "187  0.232143  0.666667  1.0  1.0  \n",
       "543  0.410714  0.666667  0.0  1.0  \n",
       "614  0.357143  1.000000  1.0  0.0  \n",
       "418  0.053571  0.666667  0.0  1.0  \n",
       "635  0.553571  0.333333  0.0  1.0  \n",
       "22   0.214286  0.666667  0.0  0.0  \n",
       "688  0.232143  0.666667  0.0  1.0  \n",
       "77   0.785714  0.666667  1.0  1.0  \n",
       "311  0.214286  0.666667  0.0  1.0  \n",
       "..        ...       ...  ...  ...  \n",
       "470  0.357143  0.666667  1.0  1.0  \n",
       "350  0.250000  0.333333  0.0  0.0  \n",
       "519  0.678571  1.000000  1.0  0.0  \n",
       "61   0.285714  0.666667  0.0  1.0  \n",
       "609  0.500000  0.666667  1.0  1.0  \n",
       "727  0.107143  0.666667  0.0  1.0  \n",
       "626  0.303571  0.666667  0.0  1.0  \n",
       "89   0.410714  0.333333  0.0  1.0  \n",
       "186  0.589286  0.666667  1.0  1.0  \n",
       "626  0.303571  0.666667  0.0  1.0  \n",
       "657  0.267857  1.000000  0.0  1.0  \n",
       "350  0.250000  0.333333  0.0  0.0  \n",
       "214  0.500000  0.333333  0.0  1.0  \n",
       "141  0.142857  0.666667  0.0  1.0  \n",
       "2    0.267857  0.666667  0.0  1.0  \n",
       "66   0.267857  0.666667  1.0  1.0  \n",
       "322  0.232143  0.666667  0.0  1.0  \n",
       "239  0.053571  0.666667  1.0  1.0  \n",
       "634  0.053571  0.666667  0.0  0.0  \n",
       "117  0.750000  0.666667  0.0  0.0  \n",
       "179  0.625000  1.000000  1.0  1.0  \n",
       "578  0.750000  0.333333  0.0  0.0  \n",
       "403  0.125000  0.666667  0.0  1.0  \n",
       "477  0.107143  0.666667  0.0  1.0  \n",
       "245  0.107143  0.666667  0.0  1.0  \n",
       "413  0.232143  0.666667  1.0  1.0  \n",
       "127  0.071429  0.666667  0.0  1.0  \n",
       "238  0.196429  1.000000  1.0  0.0  \n",
       "102  0.107143  0.333333  0.0  0.0  \n",
       "548  0.607143  0.666667  0.0  0.0  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with number of sample equal to B = X_train_c.shape[0]\n",
    "bootstrapping(classification_train, X_train_c.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2\n",
    "Compare the performance of your optimal model on the training data and on the test data using\n",
    "different measures computed from the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_tn_fp_fn(y_true, y_pred):\n",
    "    tp = sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = sum((y_true == -1) & (y_pred == -1))\n",
    "    fp = sum((y_true == 1) & (y_pred == -1))\n",
    "    fn = sum((y_true == -1) & (y_pred == 1))\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def precision(tp, fp):\n",
    "    return (tp * 100)/float(tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    return (tp * 100)/float(tp + fn)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    tp, tn, fp, fn = tp_tn_fp_fn(y_true, y_pred)\n",
    "    precisions = precision(tp, fp)/100\n",
    "    recalls = recall(tp, fn)/100\n",
    "    f1_score = (2 * precisions * recalls)/(precisions + recalls)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Support vector machines (SVMs)\n",
    "#### 2.3.1\n",
    "This task will deal with two hard margin SVM classifiers : (i) Implement the standard linear SVM\n",
    "with hard margin on the training data; (ii) implement a hard margin kernel SVM with radial basis function\n",
    "(RBF) kernel, and demonstrate that you have used a grid-search with 5-fold cross validation (same folds\n",
    "as above) to find the RBF kernel with the optimal hyperparameter with respect to the F1-score.\n",
    "Compare the results of the linear SVM and the optimal kernel SVM.\n",
    "\n",
    "\n",
    "SVM enables us to solve binary classification problems. Here it will serve us to predict the decision of the banker as to whether the applicant is granted a credit or not.\n",
    "There are 11 predictors in this problem.\n",
    "\n",
    "Our goal will be to find a hyperplane that will optimally seperate samples into 2 classes - class \"1\" and class \"-1\" (I replaced the 0s in the classification data to -1s). To achieve this, we need to find parameters W and b, which parametise the hyperplane.\n",
    "\n",
    "In this section, we use the soft margin SVM to approximate the hard margin in the limit because with large enough regularization strength $C$, soft margin approximates hard margin SVM. This is because, with a large C, more importance is given to avoiding misclassification which is what hard-margin classification aims to do. I chose $C=10000$. The objective function we have to minimize in soft-margin SVMs is:\n",
    "$$\n",
    "\\mathcal L (\\boldsymbol w) = \\frac{1}{2} \\| \\boldsymbol w \\|^2 + C \\sum_{i=1}^n \\max \\bigg( 0, 1-y_i (\\boldsymbol w \\cdot x_i + b) \\bigg) \\, .\n",
    "$$\n",
    "where $\\boldsymbol w$ is the vector of weights, $\\lambda$ the regularisation parameter, and $b$ the intercept which is included in our `X` as an additional column of $1$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data as pandas data frames\n",
    "classification_train = pd.read_csv('classification_train.csv', header=None) # 800 x 12 matrix\n",
    "classification_test = pd.read_csv('classification_test.csv', header=None) # 200 x 12 matrix\n",
    "\n",
    "# split data\n",
    "X_train_c = classification_train.iloc[:,:-1]\n",
    "y_train_c = classification_train.iloc[:,-1]\n",
    "X_test_c = classification_test.iloc[:,:-1]\n",
    "y_test_c = classification_test.iloc[:,-1]\n",
    "\n",
    "# replace the 0s with -1s (output variable)\n",
    "y_train_c = np.where(y_train_c == 0, -1, y_train_c) \n",
    "y_test_c = np.where(y_test_c == 0, -1, y_test_c)\n",
    "\n",
    "# standardize\n",
    "X_train_c = standardise(X_train_c)\n",
    "X_test_c = standardise(X_test_c)\n",
    "\n",
    "# insert 1 in every row for intercept b\n",
    "X_train_c = np.c_[np.ones((X_train_c.shape[0], 1)), X_train_c]\n",
    "X_test_c = np.c_[np.ones((X_test_c.shape[0], 1)), X_test_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W is the vector normal to the hyperplane\n",
    "def compute_cost(W, X, y, regul_strength=10000):\n",
    "  n = X.shape[0]\n",
    "  distances = 1 - y * (np.dot(X, W))\n",
    "  distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "  hinge = regul_strength * (np.sum(distances) / n) \n",
    "\n",
    "  # calculate cost\n",
    "  cost = 1 / 2 * np.dot(W, W) + hinge\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient of cost, in the SVM algorithm, we minimize the cost function.\n",
    "def calculate_cost_gradient(W, X_batch, y_batch, regul_strength=10000):\n",
    "  # if only one example is passed\n",
    "  if type(y_batch) == np.float64:\n",
    "      y_batch = np.asarray([y_batch])\n",
    "      X_batch = np.asarray([X_batch])  # gives multidimensional array\n",
    "\n",
    "  distance = 1 - (y_batch * np.dot(X_batch, W))\n",
    "  dw = np.zeros(len(W))\n",
    "\n",
    "  for ind, d in enumerate(distance):\n",
    "      if max(0, d)==0:\n",
    "          di = W\n",
    "      else:\n",
    "          di = W - (regul_strength * y_batch[ind] * X_batch[ind])\n",
    "      dw += di\n",
    "\n",
    "  dw = dw/len(y_batch)  # average\n",
    "  return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradience descent function to update the weights iteratively with a given learning rate alpha\n",
    "def sgd(X, y, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=10000, print_outcome=False):\n",
    "  # initialise zero weights\n",
    "  weights = np.zeros(X.shape[1])\n",
    "  nth = 0\n",
    "  # initialise starting cost as infinity\n",
    "  prev_cost = np.inf\n",
    "  \n",
    "  # stochastic gradient descent\n",
    "  for iteration in range(1, max_iterations):\n",
    "      # shuffle to prevent repeating update cycles\n",
    "      np.random.shuffle([X, y])\n",
    "      for ind, x in enumerate(X):\n",
    "          ascent = calculate_cost_gradient(weights, x, y[ind], regul_strength)\n",
    "          weights = weights - (learning_rate * ascent)\n",
    "\n",
    "      # convergence check on 2^n'th iteration\n",
    "      if iteration==2**nth or iteration==max_iterations-1:\n",
    "          # compute cost\n",
    "          cost = compute_cost(weights, X, y, regul_strength)\n",
    "          if print_outcome:\n",
    "            print(\"Iteration is: {}, Cost is: {}\".format(iteration, cost))\n",
    "          # stop criterion\n",
    "          if abs(prev_cost - cost) < stop_criterion * prev_cost:\n",
    "              return weights\n",
    "          \n",
    "          prev_cost = cost\n",
    "          nth += 1\n",
    "  \n",
    "  return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is: 1, Cost is: 217148.62032416498\n",
      "Iteration is: 2, Cost is: 174825.06865086342\n",
      "Iteration is: 4, Cost is: 274786.27423515136\n",
      "Iteration is: 8, Cost is: 204602.20641614526\n",
      "Iteration is: 16, Cost is: 281249.53158207884\n",
      "Iteration is: 32, Cost is: 216660.52076973073\n",
      "Iteration is: 64, Cost is: 222937.9713573361\n",
      "Iteration is: 128, Cost is: 179689.6563925721\n",
      "Iteration is: 256, Cost is: 203074.43778767652\n",
      "Iteration is: 512, Cost is: 218470.9355421231\n",
      "Iteration is: 1024, Cost is: 163204.94569968566\n",
      "Iteration is: 1999, Cost is: 216096.4831536729\n",
      "Training finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 30.64408161,  -4.15787275,  67.99800951,  -2.45255382,\n",
       "        18.98589257, -19.01662827, -14.38889498,   7.51232364,\n",
       "         0.41285016, -16.77793641,  22.63421632, -32.60308648])"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model, takes a bit of time to compute\n",
    "W = sgd(X_train_c, y_train_c, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-3, regul_strength=10000, print_outcome=True)\n",
    "print(\"Training finished.\")\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate the mean accuracies in both train and test set\n",
    "def score(W, X, y):\n",
    "  y_preds = np.array([])\n",
    "  for i in range(X.shape[0]):\n",
    "    y_pred = np.sign(np.dot(X[i], W))\n",
    "    y_preds = np.append(y_preds, y_pred)\n",
    "  \n",
    "  return y_preds, np.float(sum(y_preds==y)) / float(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.6175\n",
      "Accuracy on test set: 0.645\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on train set: {}\".format(score(W, X_train_c, y_train_c)[1]))\n",
    "print(\"Accuracy on test set: {}\".format(score(W, X_test_c, y_test_c)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement a hard margin kernel SVM with radial basis function (RBF) kernel to tacked the problem of linear inseparability of the data. Here is the function for the radial basis kernel I use:\n",
    "$$k(X,W) = \\exp^\\frac{-\\lvert\\lvert x-w \\lvert\\lvert^2}{\\sigma}$$\n",
    "I replace some of the dot products from the linear SVM by the kernel function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kernel function (here radial basis function)\n",
    "def rbf(X, W, alpha): # for a matrix X, this will return a vector\n",
    "    norm = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        norm[i] = np.dot((X[i] - W),(X[i] - W))\n",
    "    k = np.exp(-1 / alpha * norm)\n",
    "    return k\n",
    "\n",
    "def rbf2(X, W, alpha): # for a vector X, this will return a number\n",
    "    norm = (X - W).T @ (X - W)\n",
    "    k = np.exp(-1 / alpha * norm)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel cost function\n",
    "def compute_cost_kernel(W, X, y, alpha, regul_strength=10000):\n",
    "  n = X.shape[0]\n",
    "  distances = 1 - y * (rbf(X, W, alpha))\n",
    "  distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "  hinge = regul_strength * (np.sum(distances) / n) \n",
    "\n",
    "  # calculate cost\n",
    "  cost = 1 / 2 * np.dot(W, W) + hinge\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient of cost\n",
    "def calculate_cost_gradient_kernel(W, X_batch, y_batch, alpha, regul_strength=10000):\n",
    "  # if only one example is passed\n",
    "  if type(y_batch) == np.float64:\n",
    "      y_batch = np.asarray([y_batch])\n",
    "      X_batch = np.asarray([X_batch]) \n",
    "\n",
    "  distance = 1 - (y_batch * rbf(X_batch, W, alpha))\n",
    "  dw = np.zeros(len(W))\n",
    "\n",
    "  for ind, d in enumerate(distance):\n",
    "      if max(0, d)==0:\n",
    "          di = W\n",
    "      else:\n",
    "          di = W - 2/alpha*(regul_strength * rbf2(X_batch[ind], W, alpha)* y_batch[ind] * (X_batch[ind] - W)) # new gradient\n",
    "      dw += di\n",
    "\n",
    "  dw = dw/len(y_batch)  # average\n",
    "  return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradience descent function to update the weights iteratively\n",
    "def sgd_kernel(X, y, alpha, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-3, regul_strength=10000, print_outcome=False):\n",
    "  # initialise zero weights\n",
    "  weights = np.zeros(X.shape[1])\n",
    "  nth = 0\n",
    "  # initialise starting cost as infinity\n",
    "  prev_cost = np.inf\n",
    "  \n",
    "  # stochastic gradient descent\n",
    "  for iteration in range(1, max_iterations):\n",
    "      # shuffle to prevent repeating update cycles\n",
    "      np.random.shuffle([X, y])\n",
    "      for ind, x in enumerate(X):\n",
    "          ascent = calculate_cost_gradient_kernel(weights, x, y[ind], alpha, regul_strength)\n",
    "          weights = weights - (learning_rate * ascent)\n",
    "\n",
    "      # convergence check on 2^n'th iteration\n",
    "      if iteration==2**nth or iteration==max_iterations-1:\n",
    "          # compute cost\n",
    "          cost = compute_cost_kernel(weights, X, y, alpha, regul_strength)\n",
    "          if print_outcome:\n",
    "            print(\"Iteration is: {}, Cost is: {}\".format(iteration, cost))\n",
    "          # stop criterion\n",
    "          if abs(prev_cost - cost) < stop_criterion * prev_cost:\n",
    "              return weights\n",
    "          \n",
    "          prev_cost = cost\n",
    "          nth += 1\n",
    "  \n",
    "  return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "W_kernel = sgd_kernel(X_train_c, y_train_c, alpha=i, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=10000, print_outcome=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_kernel(W, X, y):\n",
    "  y_preds = np.array([])\n",
    "  for i in range(X.shape[0]):\n",
    "    y_pred = np.sign(np.dot(X[i], W)) # do not replace dot product here as it does not apply to kernel\n",
    "    y_preds = np.append(y_preds, y_pred)\n",
    "  \n",
    "  return y_preds, np.float(sum(y_preds==y)) / float(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "       -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
       "        1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "        1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "        1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_kernel = score_kernel(W_kernel, X_test_c, y_test_c)[0]\n",
    "y_pred_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING FOR 1 FOLD - THIS WORKS!\n",
    "alphas = np.linspace(0, 3, 10**2)\n",
    "alphas = alphas[1:]\n",
    "\n",
    "def optimal_alpha(data, n_folds):\n",
    "    \n",
    "    X_train_subset, y_train_subset, X_val, y_val = cross_val_evaluate(data, n_folds)[1]\n",
    "    f1_scores = []\n",
    "   \n",
    "    for i in alphas:\n",
    "        W_kernel = sgd_kernel(X_train_subset, y_train_subset, alpha=i, max_iterations=2000, stop_criterion=0.05, learning_rate=1e-5, regul_strength=10000, print_outcome=False)\n",
    "        y_preds = score_kernel(W_kernel, X_train_subset, y_train_subset)[0]\n",
    "        f1 = f1_score(y_train_subset, y_preds)\n",
    "        f1_scores.append(f1)\n",
    "    return y_preds, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now use a grid-search with 5-fold cross validation to find the RBF kernel with the optimal hyperparameter with respect to the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal hyperparameter alpha is 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "classification_train = np.where(classification_train == 0, -1, classification_train)\n",
    "folds = cross_val_evaluate(classification_train, 5)\n",
    "alphas = np.linspace(0, 3, 10**2)\n",
    "alphas = alphas[1:]\n",
    "\n",
    "def optimal_alpha(data):\n",
    "    \n",
    "    X_train_subset, y_train_subset, X_val, y_val = data \n",
    "    f1_scores = []\n",
    "   \n",
    "    for i in alphas:\n",
    "        W_kernel = sgd_kernel(X_train_subset, y_train_subset, alpha=i, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=10000, print_outcome=False)\n",
    "        y_pred = score_kernel(W_kernel, X_train_subset, y_train_subset)[0]\n",
    "        f1 = f1_score(y_train_subset, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "    return f1_scores\n",
    "\n",
    "results = map(optimal_alpha, folds) # use map to apply optimal_alpha function to each fold\n",
    "matrix_f1_scores = np.array(list(results))\n",
    "mean_f1_scores = np.mean(matrix_f1_scores, axis=0)\n",
    "optimal_alpha_loc = mean_f1_scores.argmax() \n",
    "optimal_alpha =  alphas[optimal_alpha_loc]\n",
    "print(\"The optimal hyperparameter alpha is\", optimal_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the results of the linear SVM and the optimal kernel SVM, I evaluate the f1-score and the mean accuracy of both on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1-score for linear SVM is 0.7320754716981132\n",
      "Accuracy on test set with linear SVM: 0.645\n",
      "The F1-score for kernel SVM is 0.7936507936507936\n",
      "Accuracy on test set with kernel SVM: 0.675\n"
     ]
    }
   ],
   "source": [
    "# linear SVM model\n",
    "y_pred_linear = score(W, X_test_c, y_test_c)[0]\n",
    "f1_linear = f1_score(y_test_c, y_pred_linear)\n",
    "print(\"The F1-score for linear SVM is\", f1_linear)\n",
    "print(\"Accuracy on test set with linear SVM: {}\".format(score(W, X_test_c, y_test_c)[1]))\n",
    "\n",
    "# train the kernel SVM model using optimal hyperparameter\n",
    "W_kernel = sgd_kernel(X_train_c, y_train_c, alpha=optimal_alpha, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=10000, print_outcome=False)\n",
    "y_pred_kernel = score_kernel(W_kernel, X_test_c, y_test_c)[0]\n",
    "f1_kernel = f1_score(y_test_c, y_pred_kernel)\n",
    "print(\"The F1-score for kernel SVM is\", f1_kernel)\n",
    "print(\"Accuracy on test set with kernel SVM: {}\".format(score_kernel(W_kernel, X_test_c, y_test_c)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the F1-scores and the accuracy above, the kernel SVM appears to perform better than linear SVM. This makes sence because kernel functions are meant to help when the data is not linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2\n",
    "Evaluate the performance of the RBF SVM on the test data over the range of the\n",
    "hyperparameter of the kernel and represent the results using a receiver operating characteristic (ROC)\n",
    "curve . Use this ROC curve to evaluate the quality of the optimal kernel SVM obtained in 2.3.1 through\n",
    "cross-validation on the training set.\n",
    "\n",
    "The ROC curve will allow us to interpret how good or bad a binary classifier works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-565-573408723745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tp' is not defined"
     ]
    }
   ],
   "source": [
    "tpr = tp/(tp + fn)\n",
    "fpr = fp/(fp + tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
